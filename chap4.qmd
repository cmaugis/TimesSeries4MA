::: {.content-hidden unless-format="html"}
$$
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\C}{\text{Cov}}
\newcommand{\V}{\mathrm{Var}}
\newcommand{\EL}{\text{EL}}
\newcommand{\H}{\mathcal H}
$$
:::

```{r}
#| echo: false
#| message: false
#| warning: false

library(caschrono) 
library(ggfortify) 
library(gridExtra)
library(fpp3) 
library(tidyverse) 
library(latex2exp) 
library(ggplot2)
library(bayesforecast)
library(astsa)
library(forecast)
```

# Les modèles ARMA {#sec-chap4}

Dans ce chapitre, nous allons nous intéresser aux modèles ARMA qui sont très couramment utilisés dans l'étude des séries temporelles. Ce sont des modèles paramétriques linéaires de séries temporelles proposés par Box et Jenkins. Leurs écriture et analyse utilisent abondamment les opérateurs retard $B$ et avance $F$. Aussi, nous allons débuter ce chapitre en étudiant quelques propriétés sur des polynômes ou séries "en $B$" que nous exploiterons ensuite dans le cadre des processus ARMA.

## Polynomes et séries en B

### Définitions

Soit $(X_t)_{t\in \Z}$ un processus stationnaire du second ordre. Rappelons que l'opérateur retard $B$ est défini par $BX_t=X_{t-1}$ et l'opérateur avance $F$ par $FX_t=X_{t+1}$. Par composition, on a alors que $B^kX_t=X_{t-k}$ et $F^kX_t=X_{t+k}$. Puis par extension, un **polynôme en** $B$ (ou $F$) est défini par 
$$
\left(\sum_{i=1}^p a_iB^i\right)X_t=\sum_{i=1}^p a_iX_{t-i}. 
$$

On a déjà utilisé ce type d'opérateur pour définir les moyennes mobiles (voir @sec-chap1). Nous avons également considéré la notion de série en $B$ quand nous avons introduit la notion de filtrage linéaire (voir @sec-chap3). Rappelons que nous avons vu que si $(X_t)_{t\in \Z}$ est un processus stationnaire, alors le processus $(Y_t)_{t\in \Z}$ défini par $$Y_t=\sum_{i\in \Z}a_iX_{t-i}$$ est également stationnaire si $\sum_{i\in \Z}|a_i|<+\infty$.

::: {#def-serieB .definition}
Soit $(a_i)_{i\in \Z}$ une famille absolument sommable de réels $\left(\sum_{i\in \Z}|a_i|<+\infty\right)$. <br> On appelle [**série en** $B$]{style="color:blue;"} de coefficients $(a_i)_{i\in \Z}$ l'opérateur $$
P(B)=\sum_{i\in \Z}a_iB^i 
$$ sur les processus stationnaires qui transforme le processus stationnaire $(X_t)_{t\in \Z}$ en un processus stationnaire $(Y_t)_{t\in \Z}$ défini par , pour tout $t\in \Z$, par 
$$
Y_t=P(B)X_t=\left(\sum_{i\in \Z}a_iB^i\right)X_t=\sum_{i\in \Z}a_iX_{t-i},\ \forall t\in\Z 
$$
:::

La manipulation des séries en $B$ se fait comme celle des séries réelles, en particulier pour les combinaisons linéaires et composées de séries.

::: {#prp-serieB .propositon}
On a les résultats suivants :

-   La **combinaison linéaire** de deux séries en $B$ est une série en $B$.
-   La **composée** de deux séries en $B$ est une série en $B$.

:::

::: {.callout-warning collapse="true" icon="false"}
### Preuve 

Soient $P(B)=\sum_{i\in \Z}a_iB^i$ et $Q(B)=\sum_{i\in \Z}b_iB^i$ deux séries en $B$, donc telles que $\sum_{i\in \Z}|a_i|<+\infty$ et $\sum_{i\in \Z}|b_i|<+\infty$. Considérons l'opérateur $\lambda P(B)+Q(B)$ et appliquons le à un processus stationnaire $(X_t)_{t\in \Z}$. On obtient, pour tout $t\in \Z$ : 
\begin{eqnarray*} 
\left(\lambda P(B)+Q(B)\right)X_t
&=&\lambda
P(B)X_t+Q(B)X_t=\sum_{i\in \Z}\lambda a_iX_{t-i}+\sum_{i\in \Z}b_iX_{t-i}\\
&=&\lim_{n\to +\infty}\sum_{i=-n}^{i=+n}\lambda a_iX_{t-i}+\lim_{n\to
+\infty}\sum_{i=-n}^{i=+n}b_iX_{t-i}\\ 
&=&\lim_{n\to
+\infty}\sum_{i=-n}^{i=+n}\left(\lambda a_iX_{t-i}+b_iX_{t-i}\right)=\sum_{i\in
\Z}(\lambda a_i+b_i)X_{t-i}, 
\end{eqnarray*} 
où la dernière égalité est justifiée par l'absolue convergence de la série $\sum(\lambda a_i+b_i)$ garantie par l'inégalité 
$$ 
\sum_{i\in \Z} |\lambda a_i+b_i|\leq \lambda\sum_{i\in \Z}|
a_i|+\sum_{i\in \Z}|b_i|<+\infty. 
$$ 
On a bien alors 
$$
\lambda\sum_{i\in \Z}
a_iB^i+\sum_{i\in \Z} b_iB^i=\sum_{i\in \Z}(\lambda a_i+b_i)B^i. 
$$ 
Considérons maintenant la composition des deux séries $P(B)$ et $Q(B)$. On a 
$$
P(B)\circ Q(B)X_t=P(B)Y_t, 
$$ 
où 
$$ 
Y_t=\sum_{j\in \Z} b_jX_{t-j}. 
$$ 
On a donc 
\begin{eqnarray*} 
P(B)\circ Q(B)X_t&=&\sum_{i\in \Z} a_iY_{t-i}=\lim_{n\to
+\infty}\sum_{i=-n}^na_iY_{t-i}\\ &=&\lim_{n\to
+\infty}\sum_{i=-n}^na_i\left(\lim_{m\to +\infty}\sum_{j=-m}^mb_j
X_{t-i-j}\right)\\ &=&\lim_{n\to +\infty}\lim_{m\to
+\infty}\sum_{i=-n}^n\sum_{j=-m}^ma_ib_j X_{t-i-j}\\ &=&\lim_{n\to
+\infty}\lim_{m\to
+\infty}\sum_{k=-n-m}^{n+m}\left(\sum_{i=\max(-n,k-m)}^{\min(n,k+m)}a_ib_{k-i}\right)X_{t-k},
\end{eqnarray*} où la dernière égalité est obtenue en faisant un changement d'indice de $j$ à $k=i+j$. On sait que les séries $\sum a_i$ et $\sum b_j$ étant absolument sommables, la série produit de Cauchy (appelé aussi parfois convoluée) $\sum c_k$ de ces deux suites, où $$
c_k=\sum_{i\in \Z}a_ib_{k-i}, 
$$ pour $k\in \Z$, l'est aussi. On a donc : $$ 
P(B)\circ Q(B)X_t=\sum_{k\in \Z}c_kB^k, 
$$ ce qui montre que la composition des deux séries en $B$ est bien une série en $B$.
:::

### Inversion de $I-\lambda B$ 

Dans l'étude des modèles ARMA, on va avoir besoin de savoir quand et comment on peut inverser des polynômes en $B$. On va donc ici commencer par le polynôme de degré 1 en $B$ : $I-\lambda B$.

L'opérateur $P(B)=I-\lambda B$ associe à tout processus stationnaire $(X_t)_{t\in \Z}$ un nouveau processus stationnaire $(Y_t)_{t\in \Z}$ tel que 
$$ 
Y_t=(I-\lambda B)X_t=X_t-\lambda X_{t-1}. 
$$
Se donnant un processus stationnaire $(Y_t)_{t\in \Z}$, le problème de l'inversion du polynôme $I-\lambda B$ revient à déterminer s'il existe un processus stationnaire $(X_t)_{t\in \Z}$ vérifiant : 
$$ 
Y_t=(I-\lambda B)X_t=X_t-\lambda X_{t-1} 
$$ et de donner son expression en fonction du processus $(Y_t)_{t\in \Z}$.

On va montrer dans la suite que ce polynôme $P(B)=I-\lambda B$ est inversible seulement si $|\lambda|\neq 1$ et on obtiendra dans ce cas l'expression de son inverse.

#### 1er Cas : $|\lambda|<1$

- **Une solution :**

Soit la suite $(a_i)_{i\in \Z}$ définie par $a_i=0$ pour $i\in\mathbb Z \setminus \mathbb N$ et $a_i=\lambda^i$ pour $i\in \N$. On a alors 
$$ 
\sum_{i\in\Z}|a_i|=\sum_{i\in \N}|\lambda|^i<+\infty. 
$$

La série en $B$ de coefficients $(a_i)_{i\in \Z}$ est donc bien définie et on peut écrire 
\begin{eqnarray*}
(I-\lambda B)\left(\sum_{i\in \Z}a_iB^i\right)
&=&(I-\lambda B)\left(\sum_{i\in\N}\lambda^iB^i\right)\\
&=&\sum_{i\in \N}\lambda^iB^i-\sum_{i\in \N}\lambda^{i+1}B^{i+1}=I.
\end{eqnarray*}

Ainsi, pour un processus stationnaire $(Y_t)_{t\in \Z}$ donné, le processus $(X_t)_{t\in \Z}$ défini par $X_t=\underset{i=0}{\stackrel{+\infty}{\sum}}\lambda^iY_{t-i}$ est solution de l'équation 
$$ 
Y_t=(I-\lambda B)X_t. 
$$

- **Non unicité :**

$(X_t)_{t\in\Z}$ avec $X_t=\sum_{i=0}^{+\infty}\lambda^iY_{t-i}$ n'est pas l'unique solution. En effet, soit $(X^*_t)_{t\in \Z}$ tel que $(I-\lambda B)X_t^*=0$ et définissons le processus $(\tilde X_t)_{t\in \Z}$ par $\tilde X_t=X_t+X_t^*$. Alors on a que $Y_t=(I-\lambda B)\tilde X_t$.

Un processus $(X^*_t)_{t\in \Z}$ vérifiant $(I-\lambda B)X_t^* = X_t^*-\lambda X_{t-1}^*=0$ est de la forme $X^*_t=\lambda^t A$, pour tout $t\in\Z$, où $A$ est une v.a.r. de $L^2$. Ainsi, étant donné un processus $(Y_t)_{t\in \Z}$, les solutions de l'équation $Y_t=(I-\lambda B)X_t,\ \forall t\in\Z$ sont de la forme 
$$
\tilde X_t=X_t+X_t^*=\sum_{i=0}^{+\infty}\lambda^iY_{t-i}+\lambda^t A,\ t\in\Z
$$

Mais le processus $(X^*_t)_{t\in \Z}$ n'est pas stationnaire !

- **Unique solution stationnaire :**

Etant donné le processus $(Y_t)_{t\in \Z}$, **la solution stationnaire** de l'équation $Y_t=(I-\lambda B)X_t$ est le processus $(X_t)_{t\in \Z}$ défini par 
$$ 
X_t=\sum_{i=0}^{+\infty}\lambda^iY_{t-i}. 
$$

::: {.proposition #prp-lambdainf1}
L'opérateur $(I-\lambda B)$ est inversible dans la classe des processus stationnaires d'inverse $\underset{i=0}{\stackrel{+\infty}{\sum}}\lambda^iB^i$ quand $|\lambda|<1$.
:::

#### 2ème Cas : $|\lambda|>1$

On peut écrire 
$$ 
I-\lambda B=-\lambda B\left(I-\frac{1}{\lambda}F\right)
$$
Comme $|\frac 1 \lambda|<1$, par le même raisonnement que précédent, le polynôme $\left(I-\frac{1}{\lambda}F\right)$ est inversible dans la classe des processus stationnaires d'inverse $\sum_{i=0}^{+\infty}\lambda^{-i}F^i$. Le processus $-\lambda B$ est inversible d'inverse $(-1/\lambda)F$.

::: {.proposition #prp-lambdasup1}
Quand $|\lambda|>1$, l'e processus'opérateur $I-\lambda B$ est inversible dans la classe des processus stationnaires d'inverse 
$$
\left(\frac{-1}{\lambda}F\right)\left(\sum_{i=0}^{+\infty}\frac{1}{\lambda^i}F^i\right)
=-\sum_{i=0}^{+\infty}\frac{1}{\lambda^{i+1}}F^{i+1}=-\sum_{j=1}^{+\infty}\frac{1}{\lambda^j}F^j
$$


Etant donné un processus stationnaire $(Y_t)_{t\in \Z}$, l'unique solution stationnaire de l'équation 
$Y_t=(I-\lambda B)X_t$ est donc $(X_t)_{t\in \Z}$ défini par 
$$ 
X_t=-\sum_{j=1}^{+\infty}\frac{1}{\lambda^j}Y_{t+j}. 
$$
:::

::: {.callout-note icon="false"}
### Remarque 
Dans le cas $|\lambda|>1$, l'expression de la solution $(X_t)_{t\in \Z}$ est fonction du futur du processus $(Y_t)_{t\in \Z}$ et non en fonction de son passé comme dans le cas $|\lambda|<1$.
:::

#### 3ème Cas : $|\lambda|=1$

::: {.proposition #prp-lambda1}
Si $|\lambda|=1$, l'opérateur $I-\lambda B$ n'est pas inversible dans la classe des processus stationnaires.
:::

::: {.callout-warning collapse="true" icon="false"}
### Preuve
-   Soit $(X_t)_{t\in \Z}$ le processus constant $X_t=m,\ \forall t\in\Z$. On a alors $(I- B)X_t= m-m=0$ ce qui prouve que l'opérateur n'est pas injectif et donc pas bijectif.

-   On peut aussi montrer que $(I-B)$ n'est pas surjectif :

si le processus constant $(Y_t)_{t\in\Z}$ égal à $m\neq 0$ avait un antécédent $(X_t)_{t\in \Z}$ stationnaire, ce dernier serait tel que 
$X_t-X_{t-1}=m,\ \forall t\in\Z$. On arrive alors à une contradiction car $\E [X_t-X_{t-1}]=0\neq m$.
:::

### Inverse d'un polynôme en $B$

On considère maintenant un polynôme en $B$ de la forme 
$$
    \Phi(B)=I-\varphi_1B-\varphi_2B^2-\cdots-\varphi_pB^p. 
$$
et on répondre à la question suivante : $\Phi(B)$ est-il inversible ?
Se donnant un processus $(Y_t)_{t\in \Z}$, existe-t-il un unique processus stationnaire $(X_t)_{t\in \Z}$ tel que 
$$ 
\Phi(B)X_t=Y_t,\ \forall t\in\Z. 
$$
Dans les complexes, le polynôme $\Phi(z)$ a $p$ racines non nécessairement distinctes. Comme il est non constant, ce polynome est scindé. On peut donc alors utiliser les résultats du polynôme $I-\lambda B$. On a alors la discussion de cas suivante : 

-   Si au moins une des racines est de module 1, alors il n'existe pas de processus stationnaire $(X_t)_{t\in \Z}$ solution de l'équation $\Phi(B)X_t=Y_t$.
-   Si toutes les racines sont de module différent de 1, alors il existe une série en $B$, notée $\Psi(B)=\sum \psi_iB^i$, telle que :
    -   $\Phi(B)\Psi(B)=I$
    -   $(X_t)_{t\in \Z}$, solution de $\Phi(B)X_t=Y_t$, est stationnaire.
-   Si toutes les racines sont à l'extérieur du disque unité, alors l'inverse est une série en puissances positives de $B$ uniquement $$
    \Psi(B)=\sum_{i\in \N} \psi_iB^i 
    $$
-   Si toutes les racines sont à l'intérieur du disque unité, alors l'inverse est une série en puissances strictement positives de $F$ uniquement 
$$ 
\Psi(B)=\sum_{i\in \N^*} \psi_iF^i 
$$

#### Comment déterminer l'inverse de $\Phi(B)$ ?

Si l'inverse du polynôme $\Phi(B)$ existe, on peut le déterminer par l'une de ces méthodes

-   *Identification* :

Par exemple quand toutes les racines sont à l'extérieur du disque unité, on écrit 
$$
\left(1-\varphi_1z-\varphi_2z^2-\cdots-\varphi_pz^p\right)  \left(\sum_{i\in\N}\psi_iz^i\right)=1 
$$ 
dont on tire des équations donnant les expressions des coefficients $\psi_i$ en fonction des $\varphi_i$.

-   *Décomposition en éléments simples de* $\Phi(z)$ : On écrit \begin{eqnarray*} \frac{1}{\Phi(z)}&=&\frac{1}{\prod_{i=1}^p (1-\lambda_iz)}\\
    &=&\sum_{i=1}^p\frac{a_i}{1-\lambda_iz}\\ &=&\sum_{i=1}^pa_i \sum_{j\in
    \Z}\lambda_i^jz^j\\ &=&\sum_{j\in \Z}\left(\sum_{i=1}^pa_i
    \lambda_i^j\right)z^j=\Psi(z). 
    \end{eqnarray*}

## Processus AR {#sec-chap4AR}

### Définition

Dans cette partie, on se restreint à des **processus centrés** sans perte de généralités

Dans de nombreuses situations pratiques la valeur en un instant $t$ d'une série temporelle peut s'écrire comme la somme d'une combinaison linéaire des valeurs précédentes de la série et d'un terme de bruit. Un tel modèle est connu sous le nom d'un **processus AR (AutoRegressive)**.

::: {#def-ARp .definition}
## AR(p)
<br>
On dit qu'un processus stationnaire $(X_t)_{t\in \Z}$ admet une représentation [**auto-régressive d'ordre**]{style="color:blue;"} $p$ (noté AR(p)) s'il vérifie l'équation récurrente : 
$$
X_t-\varphi_1X_{t-1}-\varphi_2X_{t-2}-\cdots-\varphi_pX_{t-p}=\varepsilon_t, 
$$ 
où $(\varepsilon_t)_{t\in \Z}\sim\text{WN}(0,\sigma^2)$ et $(\varphi_1,\ldots,\varphi_p)\in\mathbb R^p$ avec $\varphi_p\neq0$.

**Réécriture :** <br>
Un processus AR(p) vérifie l'équation : $\Phi(B)X_t=\varepsilon_t,\ \forall t\in\Z$ avec le polynome en $B$ 
$$ 
\Phi(B)=I-\varphi_1 B-\varphi_2 B^2-\cdots-\varphi_p B^p.
$$

Cette représentation est dite [**canonique**]{style="color:blue;"} si le bruit blanc $(\varepsilon_t)_{t\in \Z}$ est tel que $\varepsilon_t\perp\mathcal H_{-\infty}^{t-1}(X)$ pour tout $t\in\Z$.
:::

::: {.callout-note icon="false"}
### Remarque 
Comme un processus AR doit être stationnaire par déf., les racines du polynôme $\Phi(z)$ doivent toutes être de module différent de 1 !

Soit un processus AR(1) vérifiant 
$$
X_t-\varphi X_{t-1}=\varepsilon_t \textrm{ avec }\varphi=\pm 1. 
$$ 
Par itération, on a $X_t=\varphi^tX_0+\sum_{j=0}^{t-1}\varphi^j\varepsilon_{t-j}$. 
$$
\V(X_t)+\V(\varphi^tX_0)-2\varphi^t\C(X_t,X_0)=\sum_{j=0}^{t-1}\varphi^{2j}\V(\varepsilon_{t-j}).
$$ 
S'il existe un processus stationnaire $(X_t)_{t\in \Z}$ vérifiant cette équation, on doit donc avoir <!--$2\gamma_X(0)-2\varphi^t\gamma_X(t)=t\sigma^2$
                            et donc --> $2- 2 \varphi^t\rho_X(t)=\frac{t\sigma^2}{\gamma_X(0)}.$ On aboutit à une contradiction en faisant tendre $t$ vers $+\infty$. <!--puisque le terme
                            de gauche reste borné et le terme de droite tend vers $+\infty$. Il n'existe
                            donc pas de processus stationnaire vérifiant cette équation.-->
:::

### Un exemple instructif : le processus AR(1)

On considère $(X_t)_{t\in \Z}$ un processus AR(1), vérifiant donc l'équation de récurrence 
$$ 
X_t-\varphi X_{t-1}=\varepsilon_t,\ \forall t\in\Z 
$$ où $(\varepsilon_t)_{t\in \Z}$ est un bruit blanc $\text{WN}(0,\sigma^2)$ et $\varphi\neq 0$.

D'après l'étude précédente sur l'inversibilité de $I-\varphi B$, trois cas se présentent suivant les valeurs du paramètre $\varphi$.

#### Cas $|\varphi|=1$

On a vu précédemment que dans ce cas, il n'existe pas de processus stationnaire $(X_t)_{t\in \Z}$ vérifiant l'équation 
$(I-\varphi B)X_t=\varepsilon_t$. Il n'existe donc pas de processus AR(1) de paramètre 1 ou -1.

#### Cas $|\varphi|<1$

- **Ecriture de $X_t$ en fonction du bruit blanc :**

Pour $|\varphi|<1$, l'inversibilité de $I-\varphi B$ est assurée dans la classe des processus stationnaires et 
$$ 
(I-\varphi B)^{-1}=\sum_{i=0}^{+\infty}\varphi^iB^i. 
$$

Ainsi le processus AR(1) pour $|\varphi|<1$ s'écrit, en fonction du bruit blanc $(\varepsilon_t)_{t\in \Z}$, sous la forme : 
$$
X_t=\sum_{i=0}^{+\infty}\varphi^i\varepsilon_{t-i},\ \forall t\in\Z. 
$$

De cette écriture, on en déduit que $X_t\in\mathcal H_{-\infty}^t(\varepsilon)$ et donc $\mathcal H_{-\infty}^t(X)\subset \mathcal H_{-\infty}^t(\varepsilon)$ pour tout $t$ dans $\Z$. De plus,    par propriété des bruits blancs $\varepsilon_t\perp \mathcal H_{-\infty}^{t-1}(\varepsilon)$. Ainsi $\varepsilon_t \perp \mathcal H_{-\infty}^{t-1}(X)$, donc cette représentation est **canonique**.  


- **Prévision linéaire optimale :**

Ayant observé $X_1,\ldots,X_n$, on prédit la valeur suivante de la série $X_{n+1}$ par 
$$ 
\hat X_n(1)=\EL(X_{n+1}|\mathcal
H_{-\infty}^n(X))=\EL(\varphi X_n +\varepsilon_{n+1}|\mathcal
H_{-\infty}^n(X))=\varphi X_n 
$$ 
car $\varepsilon_{n+1}\perp\mathcal H_{-\infty}^n(X)$.

- **Processus des innovations :** 

Dans ce cas, 
$$ 
X_{t}-\hat X_{t-1}(1)=X_{t}-\varphi X_{t-1} = \varepsilon_{t},\ \forall t\in\Z
$$ 
C'est donc le bruit blanc $(\varepsilon_t)_{t\in \Z}$ dans la représentation AR(1). On verra que ce résultat reste valable pour tout modèle AR de représentation canonique.

#### Cas $|\varphi|>1$

- **Ecriture (non canonique) de $X_t$ selon le bruit blanc :**

D'après l'étude précédente, le polynôme $I-\varphi B$ est inversible d'inverse 
$$ 
    (I-\varphi B)^{-1}=-\sum_{i=1}^{+\infty}\frac{1}{\varphi^i}F^i \textrm{  d'où  }
    X_t=-\sum_{i=1}^{+\infty}\frac{1}{\varphi^i}\varepsilon_{t+i}. 
$$

Cette représentation du processus AR(1) **n'est pas canonique** car

```{=tex}
\begin{eqnarray*}
\C(\varepsilon_t,X_{t-1})&=&\C\left(\varepsilon_t,-\sum_{i=1}^{+\infty}\frac{1}{\varphi^i}\varepsilon_{t-1+i}\right)\\
&=& -\frac{1}{\varphi}\V(\varepsilon_t)= -\frac{\sigma^2}{\varphi}\neq 0.
\end{eqnarray*}
```

- **Prévision linéaire optimale :**

Ayant observé $X_1,\ldots,X_n$, on prédit la valeur suivante de la série $X_{n+1}$ par 
\begin{eqnarray*} 
\hat X_n(1)&=&\EL(X_{n+1}|\mathcal
H_{-\infty}^n(X))\\ &=&\EL(\varphi X_n +\varepsilon_{n+1}|\mathcal
H_{-\infty}^n(X))\\ &=&\varphi X_n+\EL(\varepsilon_{n+1}|\mathcal
H_{-\infty}^n(X)) 
\end{eqnarray*} 
le dernier terme n'ayant aucune raison d'être nul, puisque la représentation n'est pas canonique.

- **Processus des innovations :** 
\begin{eqnarray*} 
X_{t+1}-\hat X_t(1)
&=&\varphi X_t+\varepsilon_{t+1}-\varphi X_t-\EL(\varepsilon_{t+1}|\mathcal H_{-\infty}^t(X))\\ &=&\varepsilon_{t+1}-\EL(\varepsilon_{t+1}|\mathcal H_{-\infty}^t(X)) 
\end{eqnarray*} 
donc non confondus avec le bruit blanc utilisé dans la représentation (non canonique) du processus AR(1).

- **Vers une représentation canonique :**

On va ici montrer que le processus AR(1) pour $|\varphi|>1$ possède un autre représentation, qui est canonique. Pour cela, on utilise la formule reliant les densités spectrales de deux processus stationnaires dont l'un est le filtrage linéaire d'un autre : partant de $(I-\varphi B)X_t=\varepsilon_t$, on a 
$$ 
f_X(\omega)|1-\varphi e^{-i\omega}|^2=f_\varepsilon(\omega)=\frac{\sigma^2}{2\pi} \Longleftrightarrow
f_X(\omega)=\frac{\sigma^2}{2\pi}\frac{1}{|1-\varphi e^{-i\omega}|^2}. 
$$

Soit le processus $(\eta_t)_{t\in \Z}$ défini par $\eta_t=X_t-\frac{1}{\varphi}X_{t-1},\ \forall t \in \Z$. Alors \begin{eqnarray*}
    f_\eta(\omega)&=&\textcolor{magenta}{f_X(\omega)} \left|1-\frac{1}{\varphi}
    e^{-i\omega}\right|^2
    =\textcolor{magenta}{\frac{\sigma^2}{2\pi}}\frac{|1-\frac{1}{\varphi}
    e^{-i\omega}|^2}{\textcolor{magenta}{|1-\varphi e^{-i\omega}|^2}}\\
    &=&\frac{\sigma^2}{2\pi\varphi^2}\frac{|1-\frac{1}{\varphi}
    e^{-i\omega}|^2}{|\frac{1}{\varphi}- e^{-i\omega}|^2} =
    \frac{\sigma^2}{2\pi\varphi^2} \frac{|1-\frac{1}{\varphi}
    e^{-i\omega}|^2}{|\frac{1}{\varphi}e^{i\omega}-1|^2}
    =\frac{\sigma^2}{2\pi\varphi^2}. 
    \end{eqnarray*}

On a donc une nouvelle représentation AR(1) pour le processus $(X_t)_{t\in \Z}$ : 
$$
(I-\frac{1}{\varphi} B)X_t=\eta_t \textrm{ où } (\eta_t)_{t\in \Z}\sim
\text{WN}(0,\sigma^2/\varphi^2) 
$$ et la représentation est canonique car $|1/\varphi|<1$. On a aussi que le processus $(\eta_t)_{t\in \Z}$ correspond au processus des innovations.

Cette étude du cas particulier du processus AR(1) nous a donc permis de voir que la représentation AR(1) n'est pas unique. En choisissant celle  avec le coefficient $\varphi$ ou $1/\varphi$ de module inférieur à 1, on a la représentation canonique. Ce résultat se généralise aux processus AR d'ordre quelconque.

### Propriétés

La proposition suivante généralise les résultats vus sur la surjectivité et l'inversibilité du polynôme en $I-\lambda B$ et décrit le processus AR en fonction du bruit blanc.

::: {#prp-existAR .proposition}
Soit $(\varepsilon_t)_{t\in \Z}$ un bruit blanc et $\Phi(B)$ un polynôme en $B$.

-   Il existe une infinité de processus du second ordre $(X_t)_{t\in \Z}$ vérifiant : 
$$
\Phi(B)X_t=\varepsilon_t, \text{ pour tout }t\in \Z. 
$$
-   Si les racines du polynôme $\Phi(B)$ sont **toutes de module différent de 1**, il existe une **unique solution stationnaire** $(X_t)_{t\in \Z}$. Elle s'écrit alors sous la forme d'un filtrage linéaire du bruit blanc $(\varepsilon_t)_{t\in \Z}$ : 
$$ 
X_t=\Psi(B)\varepsilon_t=\sum_{i\in\Z}\psi_i\varepsilon_{t-i},\ \forall t\in \Z. 
$$
-   Si les racines du polynômes sont **toutes à l'extérieur du disque unité**, alors l'écriture moyenne mobile du processus AR(p) ne considère que les valeurs passées du bruit blanc : 
$$
X_t=\Psi(B)\varepsilon_t=\sum_{i\in\textcolor{red}{\N}}\psi_i\varepsilon_{t-i},\ \forall t\in \Z.
$$
:::

::: {.callout-warning collapse="true" icon="false"}
### Preuve 

Pour s'en convaincre, il suffit de factoriser le polynôme $\Phi(B)$ de degré $p$ en fonction de ses $p$ racines et d'appliquer $p$ fois le résultat d'inversibilité vu pour le polynôme $I-\lambda B$. 
:::

Comme vu dans le cas du processus AR(1), il n'existe pas qu'une seule représentation d'un processus AR($p$). En revanche une seule de ces représentations est canonique.

::: {#prp-ARcano .proposition}
Soit $(\varepsilon_t)_{t\in \Z}$ un bruit blanc et $\Phi(B)$ un polynôme en $B$ de degré $p$ exactement dont toutes les racines $(z_i = 1/ \lambda_i)_{i=1,\ldots,p}$ (non néc. distinctes) sont de module différent de 1.

Soit $(X_t)_{t\in \Z}$ l'unique processus stationnaire AR(p) vérifiant 
$$
\Phi(B)X_t=\varepsilon_t, \text{ pour tout }t\in \Z. 
$$

-   Le processus $(X_t)_{t\in \Z}$ satisfait plusieurs représentations AR(p) différentes : soit $\tilde \Phi(B)$ un nouveau polynôme en $B$ ayant pour racines celles de $\Phi(B)$ ou leurs inverses, le processus $(X_t)_{t\in \Z}$ admet aussi la représentation : $$ \tilde \Phi(B)X_t=\eta_t,
    \text{ pour tout }t\in \Z, \textrm{ où }(\eta_t)_{t\in \Z} \textrm{un bruit blanc}. 
    $$
-   Toutes les représentations ont même ordre.
-   Une seule représentation est canonique : 
    $$ 
    \tilde \Phi(B)X_t=\eta_t,\ \forall t\in \Z, 
    $$ où 
    $$ 
    \tilde\Phi(B)=\prod_{i:|z_i|>1}\left(I-\lambda_iB\right)\prod_{i:|z_i|<1}\left(I-\frac{1}{\lambda_i}B\right).
    $$
:::

::: {.callout-warning collapse="true" icon="false"}
### Preuve 

On montre seulement le dernier point. 
<!--
Le second est en effet évident et le premier point s'obtiendrait de manière tout à fait identique à ce qui suit en faisant un changement de racines qui ne retient pas uniquement celles à l'extérieur du disque unité. -->
La démonstration est assez proche de ce que l'on a fait pour le processus AR(1) et utilise donc les densités spectrales. De l'équation 
$$
\Phi(B)X_t=\varepsilon_t, \text{ pour tout }t\in \Z, 
$$ on tire 
$$
f_X(\omega)\left| \Phi(e^{-i\omega}) \right|^2=f_\varepsilon(\omega). 
$$ 
L'équation 
$$ 
\tilde \Phi(B)X_t=\eta_t, \text{ pour tout }t\in \Z, 
$$ 
donne quant à elle 
$$ 
f_\eta(\omega)=f_X(\omega)\left| \tilde\Phi(e^{-i\omega})
\right|^2. 
$$ 
En supposant toutes les racines de $\Phi(z)$ réelles, on peut refaire le même type de raisonnement que celui fait pour le AR(1) et obtenir \begin{eqnarray*} f_\eta(\omega)&=&\frac{\left| \tilde\Phi(e^{-i\omega})
\right|^2}{\left| \Phi(e^{-i\omega})
\right|^2}f_\varepsilon(\omega)=\frac{\sigma^2}{2\pi}\frac{\left|
\prod_{j:|z_j|>1}\left(1-\lambda_je^{-i\omega}\right)\prod_{j:|z_j|<1}\left(1-\frac{1}{\lambda_j}e^{-i\omega}\right)
\right|^2}{\left|    \prod_{j=1}^p\left(1-\lambda_je^{-i\omega}\right)
\right|^2}\\ &=&\frac{\sigma^2}{2\pi}\frac{\left|
\prod_{j:|z_j|<1}\left(1-\frac{1}{\lambda_j}e^{-i\omega}\right)
\right|^2}{\left|    \prod_{j:|z_j|<1}\left(1-\lambda_je^{-i\omega}\right)
\right|^2}=\frac{\sigma^2}{2\pi}\frac{1}{\prod_{j:|z_j|<1} |\lambda_j|^2}.
\end{eqnarray*} Que se passe-t-il si une racine $z=1/\lambda$ n'est pas réelle ? Le polynôme $\Phi(z)$ étant à coefficient réel, son conjugué $\bar z$ est également racine. On peut alors écrire en ne considérant dans le ratio précédent que les termes correspondants à ces deux racines conjuguées : \begin{eqnarray*} \frac{\left|
\left(1-\frac{1}{\lambda}e^{-i\omega}\right)   \left(1-\frac{1}{\bar
\lambda}e^{-i\omega}\right)  \right|^2}{\left|    \left(1-\lambda
e^{-i\omega}\right)\left(1-\bar \lambda e^{-i\omega}\right)
\right|^2}&=&\frac{\left| \left( -\frac{1}{\lambda } e^{-i\omega} \right)
\left(-\frac{1}{\bar \lambda}e^{-i\omega}\right)  \left(1-\lambda
e^{i\omega}\right) \left(1-\bar \lambda e^{i\omega}\right)  \right|^2}{\left|
\left(1-\lambda e^{-i\omega}\right)\left(1-\bar \lambda e^{-i\omega}\right)
\right|^2}\\ &=&\frac{1}{|\lambda|^4}\text{ (puisque }\overline{1-\lambda
e^{i\omega}}=1-\bar \lambda e^{-i\omega}) \end{eqnarray*} et on obtient le même résultat que dans le cas de racines réelles. On a donc prouvé que l'on avait, avec ce nouveau polynôme $\tilde \Phi(B)$ obtenu en remplaçant racines à l'intérieur du disque unité par leur inverse, une nouvelle représentation AR($p$) du processus $(X_t)_{t\in \Z}$ puisque $(\eta_t)_{t\in \Z}$ est un bruit blanc. Il reste à prouver que la représentation obtenue est canonique. Le raisonnement suit de près ce que l'on a fait pour le cas particulier du processus $AR(1)$. D'une part on montre l'égalité des histoires des processus $(X_t)_{t\in \Z}$ et $(\eta_t)_{t\in \Z}$, i.e. $$ \H_{-\infty}^t(X)=
\H_{-\infty}^t(\eta). $$ On dit dans ce cas que le processus est \textbf{régulier}. C'est bien le cas ici puisque l'équation $$ \tilde \Phi(B)X_t=\eta_t, \text{
pour tout }t\in \Z., $$ où $\tilde \Phi(B)$ est un polynôme avec puissance positives de $B$, assure la première inclusion\~: $\H_{-\infty}^t(\eta)\subset \H_{-\infty}^t(X)$. Mais comme $\tilde \Phi(B)$ possède par construction toutes ses racines hors du disque unité, il est inversible d'inverse ne faisant intervenir que les puissances positives de $B$ : $$ X_t=\sum_{i\in \N} \tilde
\psi_i\eta_{t-i}. $$ On en déduit l'inclusion inverse : $\H_{-\infty}^t(X)\subset \H_{-\infty}^t(\eta)$ et donc l'égalité. La représentation canonique est alors bien acquise puisque, en tant que bruit blanc, le processus $(\eta_t)_{t\in \Z}$ est évidemment orthongonal à son passé, donc ici également orthogonal au passé de $(X_t)_{t\in \Z}$. 
:::



::: {#prp-ARinnov .propositon}
Si la représentation du processus AR(p) $$
\Phi(B)X_t=\varepsilon_t, \text{ pour tout }t\in \Z, 
$$ est canonique, le bruit blanc $(\varepsilon_t)_{t\in \Z}$ utilisé dans sa représentation est aussi le processus des innovations.
:::

::: {.callout-warning collapse="true" icon="false"}
### Preuve 

Par définition le processus des innovations est donné par \begin{eqnarray*}
X_{t+1}-\hat X_t(1)&=&X_{t+1}-\EL\left(
\varphi_1X_t+\cdots+\varphi_pX_{t+1-p}+\varepsilon_{t+1}|\H_{-\infty}^t(X)\right)\\
&=&X_{t+1}-\varphi_1X_t-\cdots-\varphi_pX_{t+1-p}+0 \text{ (puisque }
\varepsilon_{t+1}\perp \H_{-\infty}^t(X))\\ &=& \varepsilon_{t+1},
\end{eqnarray*} ce qui est bien le résultat annoncé.
:::

::: {.proposition #prp-ARpMAinfini}
Tout processus AR(p) de représentation canonique $$ \Phi(B)X_t=\varepsilon_t, \text{ pour tout }t\in \Z, $$ admet la représentation MA($\infty$) : $$ 
X_t=\varepsilon_t +\sum_{i=1}^{+\infty}
\psi_i\varepsilon_{t-i}. 
$$
:::

::: {.callout-warning collapse="true" icon="false"}
### Preuve 

Nous avons déjà obtenu les éléments de cette proposition. La seule nouveauté, que l'on aurait pu noter plus tôt, est l'égalité à 1 du premier coefficient $\psi_0$ dans l'inverse du polynôme $\Phi (z)$. Ceci se voit facilement par identification du terme constant.
:::

### Liaisons temporelles

Soit un processus $(X_t)_{t\in \Z}$ de représentation AR($p$) canonique : $$
\Phi(B)X_t=\varepsilon_t,\ \forall t\in \Z. 
$$

-   Variance du processus $(X_t)_{t\in \Z}$ :\
    \begin{eqnarray*}
    \gamma_X(0)&=&\V(X_t)=\C\left(\sum_{i=1}^p\varphi_iX_{t-i}+\varepsilon_t,X_t\right)\\
    &=&\sum_{i=1}^p\varphi_i \C(X_{t-i},X_t)+\C(\varepsilon_t,X_t)\\
    &=&\sum_{i=1}^p\varphi_i
    \gamma_X(i)+\C\left(\varepsilon_t,\sum_{i=1}^p\varphi_iX_{t-i}+\varepsilon_t\right)\\
    &=&\sum_{i=1}^p \varphi_i
    \underbrace{\gamma_X(i)}_{\gamma_X(0)\rho_X(i)}+\V(\varepsilon_t) \textrm{ (car
    canonique)}. 
    \end{eqnarray*} où la dernière égalité est obtenue par orthogonalité du bruit blanc par rapport à l'histoire du processus AR($p$) dans sa représentation canonique. On a alors $$
    \gamma_X(0)=\frac{\sigma^2}{1-\sum_{i=1}^p\varphi_i\rho_X(i)}. 
    $$

- Autocovariances : pour $h>0$, \begin{eqnarray*}
\gamma_X(h)=\C(X_t,X_{t+h})&=&\C\left(X_t,\sum_{i=1}^p\varphi_iX_{t+h-i}+\varepsilon_{t+h}\right)\\
&=&\sum_{i=1}^p\varphi_i \C(X_tX_{t+h-i})+\C(X_t,\varepsilon_{t+h})\\
&=&\sum_{i=1}^p\varphi_i \gamma_X(h-i) \text{ car  }\varepsilon_{t+h}\perp
\mathcal H_{-\infty}^{t+h-1}(X). 
\end{eqnarray*} 

-  Autocorrélations : elles satisfont l'équation linéaire récurrente d'ordre $p$ 
$$
\rho_X(h)=\sum_{i=1}^p\varphi_i \rho_X(h-i),\ \forall h\in \N^* 
$$ {#eq-RecurrencerhoAR}

Les autocorrélations vérifient donc l'équation matricielle dite de \textbf{Yule-Walker} :

```{=tex}
\begin{eqnarray*} 
\left( \begin{array}{c} \rho_X(1) \\ \vdots  \\
\rho_X(p) \end{array} \right)&=& \underbrace{\left( \begin{array}{cccc} 1   &
\rho_X(1)  & \cdots & \rho_X(p-1)  \\ \rho_X(1)  &1  & \ddots &\rho_X(p-2)  \\
\vdots  &  \ddots &   \ddots &\vdots\\ \rho_X(p-1) &\cdots&\rho_X(1)& 1
\end{array} \right)}_{R_X(p)} \left( \begin{array}{c} \varphi_1 \\ \vdots  \\
\varphi_p \end{array} \right)\\ 
\\ 
\text{ et }\\
\gamma_X(0)&=&\frac{\sigma^2}{1-\underset{i=1}{\stackrel{p}{\sum}}\varphi_i\rho_X(i)}. 
\end{eqnarray*}
```

::: {#prp-ACFAR .proposition}
### Autocorrélations simples d'un AR(p) 
<br>
Les autocorrélations simples d'un processus AR(p) décroissent, de manière exponentielle ou sinusoïdale amortie, vers 0.
:::

::: {.callout-warning collapse="true" icon="false"}
### Preuve 

L'équation @eq-RecurrencerhoAR vérifiée par les autocorrélations d'un AR($p$) est une équation de récurrence linéaire de polynôme caractéristique $$ r^p-\varphi_1r^{p-1}-\ldots
-\varphi_{p-1}r-\varphi_p=r^p\Phi\left(\frac{1}{r}\right). $$ Les racines (non nulles car $\varphi_p$ est non nul) de ce polynôme sont les inverses de celles du polynôme $\Phi(z)$ définissant l'AR($p$). La solution de l'équation @eq-RecurrencerhoAR dans $\mathbb C$ est donc une combinaison linéaire de solutions de la forme $$
(a_{i,0}+a_{i,1}h+\cdots+a_{i,k-1}h^{k-1})\lambda_i^h, 
$$ 
où $k$ est l'ordre de multiplicité de la racine $z_i=1/\lambda_i$ de $\Phi(z)$. Or, puisqu'on considère la représentation canonique, les racines $z_i$ de $\Phi(z)$ sont de module à l'extérieur du disque unité. Ainsi, si toutes les solutions sont réelles alors les autocorrélations décroissent effectivement de manière exponentielle vers 0. Si une des racines $z_i=1/\lambda_i$ de $\Phi(z)$ n'est pas réelle, on sait alors que son conjugué l'est aussi et que les solutions réelles correspondant à ces deux racines conjuguées s'écrivent sous la forme 
$$
\begin{array}{l l}
|\lambda_i|^h & \left[(a_{i,0}+a_{i,1}h+\cdots+a_{i,k-1}h^{k-1})\cos(\omega_i h) \right.\\
& \left. +(b_{i,0}+b_{i,1}h+\cdots+b_{i,k-1}h^{k-1})\sin(\omega_i h)\right], 
\end{array}
$$ 
où $\omega_i$ est le module de $\lambda_i$. Elles sont donc bien simusoïdales amorties.
:::

::: {#exm-ex1 .example}
Pour un AR(1) d'équation canonique 
$X_t=\varphi X_{t-1}+\varepsilon_t,$ 
on a 
$\rho_X(h)=\varphi^{|h|}.$ 
On a bien la décroissance exponentielle des autocorrelations et on constate que le coefficient $\varphi$ d'un AR(1) est en fait la première autocovariance.
:::

::: {#exm-ex2 .example}   
Soit le processus AR(2) défini par $X_t = 0.7 X_{t-1}-0.1 X_{t-2}+\varepsilon_t$. 
Les racines du polynome $\Phi(z)=0.1 z^2 -0.7 z +1$ sont $2$ et $5$. 
La @fig-exAR2ACF montre la décroissante exponentielle des autocorrélations empiriques de ce processus. 


```{r}
#| echo: false
#| fig-height: 4
#| fig-cap: ACF empirique du processus AR(2) $X_t = 0.7 X_{t-1}-0.1 X_{t-2}+\varepsilon_t$
#| label: fig-exAR2ACF
Xt1<-arima.sim(n=10000,model=list(order=c(2,0,0),ar=c(0.7,-0.1)))
ggacf(Xt1)
```

Soit le processus AR(2) défini par $X_t = \frac{3}{4} X_{t-1} - \frac{3}{16} X_{t-2}+\varepsilon_t$. 
Les racines du polynome $\Phi(z)=\frac{3}{16} z^2 - \frac 3 4 z +1$ sont $2\pm i 2\sqrt{3}/3$. 
La @fig-exAR2ACF2 montre les autocorrélations empiriques de ce processus. 

```{r}
#| echo: false
#| fig-height: 4
#| fig-cap: ACF empirique du processus AR(2) $X_t = \frac{3}{4} X_{t-1} - \frac{3}{16} X_{t-2}+\varepsilon_t$
#| label: fig-exAR2ACF2
Xt2<-arima.sim(n=500,model=list(order=c(2,0,0),ar=c(3/4,-3/16)))
ggacf(Xt2)
```

:::

<br>

::: {#prp-pACFAR .proposition}
**Autocorrélations partielles d'un AR(p)**

Les autocorrélations partielles d'un processus AR(p) sont nulles à partir du rang $p+1$ :

-   $r_X(p)=\varphi_p\neq 0$
-   $r_X(k)=0,\ \forall k>p$
:::

::: {.callout-warning collapse="true" icon="false"}
### Preuve 

Il suffit d'écrire la définition de l'autocorrélation partielle. On sait que le coefficient $r_X(k)$ est donné par le coefficient $\alpha_k(k)$ dans la projection de $X_t$ sur $\H_{t-k}^{t-1}$, i.e. : $$
\EL(X_t|\H_{t-k}^{t-1})=\alpha_1(k)X_{t-1}+\cdots+\alpha_k(k)X_{t-k}. 
$$ Ainsi, pour $k=p$, on peut écrire \begin{eqnarray*}
\EL(X_t|\H_{t-p}^{t-1})&=&\EL(\varphi_1X_{t-1}+\varphi_2X_{t-2}+\cdots+\varphi_pX_{t-p}+\varepsilon_t,|\H_{t-p}^{t-1})\\
&=&\varphi_1X_{t-1}+\varphi_2X_{t-2}+\cdots+\varphi_pX_{t-p}, \end{eqnarray*} dont on tire $r_X(p)=\varphi_p\neq0$. Pour $k>p$, on a : \begin{eqnarray*}
\EL(X_t|\H_{t-k}^{t-1})&=&\EL(\varphi_1X_{t-1}+\varphi_2X_{t-2}+\cdots+\varphi_pX_{t-p}+\varepsilon_t,|\H_{t-k}^{t-1})\\
&=&\varphi_1X_{t-1}+\varphi_2X_{t-2}+\cdots+\varphi_pX_{t-p}\\
&=&\varphi_1X_{t-1}+\varphi_2X_{t-2}+\cdots+\varphi_pX_{t-p}\\
& &+0\times X_{t-p-1}+\cdots+ 0\times X_{t-k} 
\end{eqnarray*} et donc $r_X(k)=0$.
:::

::: {.callout-note icon="false"}
### Remarques 

-   La propriété $r_X(p)=\varphi_p$ n'est vraie que dans le cas d'une représentation canonique.
-   On peut montrer que la propriété d'autocorrélations partielles nulles à partir d'un certain rang $p+1$ est caractéristique d'un AR($p$).
:::

::: {#exm-ex3 .example}
<br>
Pour illustrer la propriété que les autocorrélations partielles d'un processus $AR(p)$ sont nulles à partir du rang p+1, les autocorrélations partielles empiriques sont tracées en @fig-exARpACF pour le processus $X_t = 0.7 X_{t-1}-0.1 X_{t-2}+\varepsilon_t$ (à gauche - AR(2)) et le processus $X_t = 0.7 X_{t-1}-0.1 X_{t-2}+0.2 X_{t-3}+\varepsilon_t$ (à droite - AR(3)). 

```{r}
#| echo: false
#| fig-height: 4
#| label: fig-exARpACF
#| layout-ncol: 2
#| fig-cap: Autocorrélations partielles empiriques pour ...
#| fig-subcap:  
#|  - le processus $X_t = 0.7 X_{t-1}-0.1 X_{t-2}+\varepsilon_t$
#|  - le processus $X_t = 0.7 X_{t-1}-0.1 X_{t-2}+0.2 X_{t-3}+\varepsilon_t$

ggpacf(Xt1)+ #ggtitle(TeX(r'($X_t = 0.7 X_{t-1}-0.1 X_{t-2}+\varepsilon_t$)'))+
theme(plot.title = element_text(size = 40))
Xt3<-arima.sim(10000,model=list(order=c(3,0,0),ar=c(0.7,-0.1,0.2)))
ggpacf(Xt3)+  #ggtitle(TeX(r'($X_t = 0.7 X_{t-1}-0.1 X_{t-2}+0.2 X_{t-3}+\varepsilon_t$)'))+
theme(plot.title = element_text(size = 40))
```
:::

## Processus MA {#sec-chap4MA}

### Définition

::: {#def-MAq .definition}
On dit qu'un processus stationnaire $(X_t)_{t\in \Z}$ admet une représentation en [**moyenne mobile d'ordre $q$**]{style="color:blue;"}  (noté MA(q)) s'il vérifie l'équation 
$$
X_t=\varepsilon_t+\theta_1\varepsilon_{t-1}+\theta_2\varepsilon_{t-2}+\cdots+\theta_q\varepsilon_{t-q}
$$ où $(\varepsilon_t)_{t\in \Z}\sim\text{WN}(0,\sigma^2)$ et $(\theta_1,\ldots,\theta_q)\in\R^q$, $\theta_q\neq0$.

**Réécriture :**
<br>
$X_t=\Theta(B)\varepsilon\_t,\ \forall t\in\Z$ avec le polynôme en $B$ $\Theta(B)=I+\theta_1B+\theta_2B^2+\cdots+\theta_qB^q.$

Cette représentation est dite [**canonique**]{style="color:blue;"} si les racines du polynôme $\Theta(z)$ sont toutes à l'extérieur du disque unité.
:::

::: {.callout-note icon="false"}
### Remarque 
Contrairement à un AR, un MA est entièrement spécifié. A bruit blanc et paramètres $\theta_1,\ldots,\theta_q$ fixés, il ne correspond qu'un seul processus MA($q$) qui est le filtrage linéaire de $(\varepsilon_t)_{t\in \Z}$ par le filtre 
$$
\Theta(B)=I+\theta_1B+\theta_2B^2+\cdots+\theta_qB^q.
$$
:::

### Propriétés

::: {.proposition #prp-MAprop}
Soit $(\varepsilon_t)_{t\in \Z}$ un bruit blanc et $\Theta(B)$ un polynôme en $B$ de degré $q$ exactement dont toutes les racines (non néc. distinctes) sont de module différent de 1.

Soit $(X_t)_{t\in \Z}$ un processus $\text{MA}(q)$ vérifiant $X_t=\Theta(B)\varepsilon_t, \forall t\in \Z.$

-   Le processus $(X_t)_{t\in \Z}$ satisfait plusieurs représentations MA(q) différentes : soit $\tilde \Theta(B)$ un nouveau polynôme en $B$ ayant pour racines celles de $\Theta(B)$ ou leurs inverses, le processus $(X_t)_{t\in \Z}$ admet aussi la représentation : $$
    X_t=\tilde \Theta(B)\eta_t,\ \forall t\in \Z, \textrm{ où }(\eta_t)_{t\in \Z} \textrm{un bruit blanc}. 
    $$
-   Une seule représentation est canonique. Elle est obtenue en prenant le polynome avec toutes les racines à l'extérieur du disque unité.
-   Un processus $\text{MA}$ est stationnaire **quelque soit sa représentation** car filtrage linéaire d'un bruit blanc.
-   Un processus $\text{MA}$ est toujours centré.
:::

::: { .proposition #prp-MAARinfini}
Soit $(X_t)_{t\in \Z}$ un processus MA(q) de représentation canonique $$ 
X_t=\Theta(B)\varepsilon_t,\ \forall t\in\Z
\textrm{ avec } (\varepsilon_t)_{t\in \Z}\sim\text{WN}(0,\sigma^2) 
$$ 

Alors le processus $(X_t)_{t\in \Z}$ possède une représentation AR($\infty$) : 
$$
\varepsilon_t=\sum_{i=0}^{+\infty}\pi_iX_{t-i} \textrm{ avec }\pi_0=1
$$
:::

::: {.callout-warning collapse="true" icon="false"}
### Preuve 

Par hypothèse la représentation est canonique donc les racines du polynôme $\Theta(z)$ sont toutes situées à l'extérieur du disque unité. Ainsi l'opérateur $\Theta(B)$ est inversible d'inverse 
$$
\Theta^{-1}(B)=\sum_{i=0}^{+\infty}\pi_iB^i=I+\sum_{i=1}^{+\infty}\pi_iB^i,
$$
la dernière égalité étant obtenue par identification des termes constants. On a donc bien 
$$
\varepsilon_t=X_t+\sum_{i=1}^{+\infty}\pi_iX_{t-i}.
$$

:::

::: { .proposition #prp-MAinnovation}
Soit $(X_t)_{t\in \Z}$ un processus MA(q) de représentation canonique 
$$ 
X_t=\Theta(B)\varepsilon_t,\ \forall t\in\Z
\textrm{ avec } (\varepsilon_t)_{t\in \Z}\sim\text{WN}(0,\sigma^2) 
$$ 
Alors le processus des innovations correspond au bruit blanc $(\varepsilon_t)_{t\in \Z}$ de sa représentation canonique. 
:::

::: {.callout-warning collapse="true" icon="false"}
### Preuve 

A partir de la @prp-MAARinfini, on a que 
$$ 
\varepsilon_t=X_t+\sum_{i=1}^{+\infty}\pi_iX_{t-i}. 
$$ 

Les écritures MA($q$) et AR($\infty$) du processus $(X_t)_{t\in \Z}$ montrent que $\H_{-\infty}^t(X)= \H_{-\infty}^t(\varepsilon)$. On peut ainsi écrire 
\begin{eqnarray*}
\EL(X_t|\H_{-\infty}^{t-1}(X))&=&\EL(X_t|\H_{-\infty}^{t-1}(\varepsilon))\\
&=&\EL(
\varepsilon_t+\theta_1\varepsilon_{t-1}+\theta_2\varepsilon_{t-2}+\cdots+\theta_q\varepsilon_{t-q}|\H_{-\infty}^{t-1}(\varepsilon))\\
&=&\theta_1\varepsilon_{t-1}+\theta_2\varepsilon_{t-2}+\cdots+\theta_q\varepsilon_{t-q},
\end{eqnarray*} par orthoganilité du bruit blanc par rapport à son passé. On en déduit pour l'innovation $$
X_t-\EL(X_t|\H_{-\infty}^{t-1}(X))=X_t-\left(\theta_1\varepsilon_{t-1}+\theta_2\varepsilon_{t-2}+\cdots+\theta_q\varepsilon_{t-q}\right)=\varepsilon_t,
$$ ce qui achève la démonstration.
:::

### Liaisons temporelles

On considère un processus $MA(q)$ 
$$X_t=\Theta(B)\varepsilon_t,\ \forall t\in\Z$$ 
avec $\Theta(B)=I+\theta_1B+\theta_2B^2+\cdots+\theta_qB^q$ et $(\varepsilon_t)_{t\in\Z}\sim\text{WN}(0,\sigma^2)$.

- Variance du processus $(X_t)_{t\in\Z}$ : <br>  
Par la propriété d'orthogonalité d'un bruit blanc, on a

```{=tex}
\begin{eqnarray*} 
\V(X_t) &=&\V\left( \varepsilon_t +
\sum_{j=1}^q\theta_j\varepsilon_{t-j}\right)\\
&=&\sigma^2\left(1+\sum_{j=1}^q\theta_j^2\right). 
\end{eqnarray*}
```

- Fonction d'autocovariance de $(X_t)_{t\in\Z}$ : 

\begin{eqnarray*} 
\gamma_X(h)&=&\C(X_t,X_{t+h})\\
&=&\C\left(\varepsilon_t +
\sum_{j=1}^q\theta_j\varepsilon_{t-j},\varepsilon_{t+h} +
\sum_{i=1}^q\theta_i\varepsilon_{t+h-i}\right). 
\end{eqnarray*}

A partir de cette expression, on peut immédiatement remarquer que 
$\gamma_X(h)=0$ pour $h>q$.  

Pour $h=1$,  $\C(X_t,X_{t+1}) = \E\left[(\varepsilon_t +
    \sum_{j=1}^q\theta_j\varepsilon_{t-j})(\varepsilon_{t+1} +
    \sum_{i=1}^q\theta_i\varepsilon_{t+1-i})\right]
  =\sigma^2(\theta_1+\theta_1\theta_2+\cdots+\theta_{q-1}\theta_q)$. 
  
Pour $h=2$,  $\C(X_t,X_{t+2}) =\E\left[(\varepsilon_t +
    \sum_{j=1}^q\theta_j\varepsilon_{t-j})(\varepsilon_{t+2} +
    \sum_{j=1}^q\theta_j\varepsilon_{t+2-j})\right]
=\sigma^2(\theta_2+\theta_1\theta_3+\cdots+\theta_{q-2}\theta_q)$.

On en déduit pour tout $h$ dans $\N^*$ : 
\begin{eqnarray*}
\gamma_X(h)&=&\sigma^2(\theta_h+\theta_1\theta_{1+h}+\cdots+\theta_{q-h}\theta_q)
=\sigma^2\left(\theta_h+\sum_{i=1}^{q-h}\theta_i\theta_{i+h}\right).
\end{eqnarray*}

- Fonction d'autocorrélation de $(X_t)_{t\in\Z}$ : 

::: {.proposition #prp-MAautocorr}
### Autocorrélations simples d'un MA(q) 

- Les autocorrélations simples d'un processus MA(q) sont nulles à partir du rang $q+1$ : 
$$\rho_X(q)\neq 0 \textrm{ et }\rho_X(h)=0,\ \forall h>q.$$ 
- La propriété d'autocorrélations simples nulles à partir d'un certain rang $q+1$ est caractéristique d'un MA($q$)
:::

::: {.example }
On considère le processus MA(2) défini par 
$X_t = \varepsilon_t - 0.7 \varepsilon_{t-1}+0.1 \varepsilon_{t-2}$. 
La @fig-exMA2ACF représente les autocorrélations empiriques de ce processus. On a bien des autocorrélations proche de 0 à partir de $h>2$.  

```{r}
#| echo: false
#| fig-height: 4
#| fig-cap: ACF empirique du processus MA(2) $X_t = \varepsilon_t - 0.7 \varepsilon_{t-1}+0.1 \varepsilon_{t-2}$
#| label: fig-exMA2ACF
Xt1<-arima.sim(n=10000,model=list(order=c(0,0,2),ma=c(-0.7,0.1)))
ggacf(Xt1)+ #ggtitle(TeX(r'($X_t = \varepsilon_t - 0.7 \varepsilon_{t-1}+0.1 \varepsilon_{t-2}$)'))+
theme(plot.title = element_text(size = 40))
```


La @fig-exMA3ACF représente les autocorrélations empiriques du processus défini par 
$X_t = \varepsilon_t - 0.7 \varepsilon_{t-1}+0.1 \varepsilon_{t-2} -0.2 \varepsilon_{t-3}$. Les autocorrélations empiriques sont bien proche de 0 à partir de $h>3$. 

```{r}
#| echo: false
#| fig-height: 4
#| fig-cap: ACF empirique du processus MA(3) $X_t = \varepsilon_t - 0.7 \varepsilon_{t-1}+0.1 \varepsilon_{t-2} -0.2 \varepsilon_{t-3}$
#| label: fig-exMA3ACF
Xt2<-arima.sim(n=10000,model=list(order=c(0,0,3),ma=c(-0.7,0.1,-0.2)))
ggacf(Xt2)+  #ggtitle(TeX(r'($X_t = \varepsilon_t - 0.7 \varepsilon_{t-1}+0.1 \varepsilon_{t-2} -0.2 \varepsilon_{t-3}$)'))+
theme(plot.title = element_text(size = 40))
```
:::

<br>

::: {.proposition #prp-MAautocorpart}
### Autocorrélations partielles d'un MA(q) 
<br>
Les autocorrélations partielles d'un processus MA(q) sont solutions d'une équation linéaire récurrente d'ordre $q$. Elles décroissent, de manière exponentielle ou sinusoïdale amortie, vers 0.
:::

::: {.example #exm-MAbis}
<br>
On reprend le processus MA(2) défini par $X_t = \varepsilon_t - 0.7 \varepsilon_{t-1}+0.1 \varepsilon_{t-2}$. Les racines du polynome $\Theta(z)$ associé sont 
2 et 5. La @fig-exMA2pACF représente les autocorrélations partielles empiriques. 

```{r}
#| echo: false
#| fig-height: 4
#| fig-cap: pACF empirique du processus MA(2) $X_t = \varepsilon_t - 0.7 \varepsilon_{t-1}+0.1 \varepsilon_{t-2}$
#| label: fig-exMA2pACF
Xt1<-arima.sim(n=10000,model=list(order=c(0,0,2),ma=c(-0.7,0.1)))
ggpacf(Xt1)
```

La @fig-exMA2pACF2 représente les autocorrélations partielles empirique du processus MA(2) défini par MA(2) $X_t = \varepsilon_t + 0.4 \varepsilon_{t-1} - 0.45 \varepsilon_{t-2}$, où les racines du polynome $\Theta(z)$ associé sont $2\pm i 2\sqrt{3}/3$.

```{r}
#| echo: false
#| fig-height: 4
#| fig-cap: pACF empirique du processus MA(2) $X_t = \varepsilon_t + 0.4 \varepsilon_{t-1} - 0.45 \varepsilon_{t-2}$
#| label: fig-exMA2pACF2
Xt2<-arima.sim(n=10000,model=list(order=c(0,0,2),ma=c((3/4),-3/16)))
ggpacf(Xt2)
```
:::

## Processus ARMA

Nous allons maintenant étudier des processus stationnaires avec une partie AR et une partie MA. On les appelle processus ARMA pour [**AutoRegressive Moving Average**]{style="color:blue;"}. Ils sont très importants en pratique car on peut montrer que tout processus stationnaire peut être approché par un processus ARMA. 

### Définition

::: {#def-ARMA .definition}
On dit qu'un processus stationnaire $(X_t)_{t\in \Z}$ admet une représentation [**ARMA**$(p,q)$]{style="color:blue;"} s'il vérifie l'équation 
$$ 
X_t=\textcolor{red}{\underset{j=1}{\stackrel{p}{\sum}} \varphi_j
X_{t-j}} + \textcolor{blue}{\varepsilon_t +\underset{i=1}{\stackrel{q}{\sum}}
\theta_i\varepsilon_{t-i}} 
$$ avec

-   $(\varepsilon_t)_{t\in \Z}\sim\text{WN}(0,\sigma^2)$
-   $(\varphi_1,\ldots, \varphi_p)\in\R^p,\ \varphi_p\neq0$
-   $(\theta_1,\ldots,\theta_q)\in\R^{q},\ \theta_q\neq 0$

**Réécriture :** 
<br>
En utilisant les polynômes en $B$ 
\begin{eqnarray*} \Phi(B)&=&I-\varphi_1B-\varphi_2B^2-\cdots-\varphi_pB^p\\
\text{ et }\Theta(B)&=&I+\theta_1B+\theta_2B^2+\cdots+\theta_qB^q,
\end{eqnarray*} un processus **ARMA(p,q)** vérifie l'équation : 
$$
\textcolor{red}{\Phi(B)X_t}=\textcolor{blue}{\Theta(B)\varepsilon_t},\ \forall t\in\Z. 
$$
:::


::: {#def-typeARMA .definition}
La représentation d'un processus ARMA$(p,q)$ $\Phi(B)X_t=\Theta(B)\varepsilon_t$ est dite

-   **minimale** si les polynômes $\Phi(z)$ et $\Theta(z)$ n'ont pas de racine commune
-   **causale** si le polynôme $\Phi(z)$ a toutes ses racines à l'extérieur du disque unité
-   **inversible** si le polynôme $\Theta(z)$ a toutes ses racines à l'extérieur du disque unité
-   **canonique** si elle est causale et inversible.
:::

::: {.callout-note icon="false"}
### Remarques 

Si la représentation n'est pas minimale ($\Phi(z)$ et $\Theta(z)$ ont une ou des racine(s) commune(s)), alors 

-   Soit aucune de ces racines communes n'est sur le cercle unité. Dans ce cas, le processus $(X_t)_{t\in \Z}$ admet aussi la représentation $$ 
    \tilde\Phi(B)X_t=\tilde \Theta(B)\varepsilon_t,\ \forall t\in\Z 
    $$ où les nouveaux polynômes sont obtenus à partir des précédents en enlevant les racines communes.

-   Si au moins une des racines communes est sur le cercle unité, alors il peut y avoir plus d'un unique processus stationnaire vérifiant l'équation.<br> 
**Dans la suite, on ne considèrera que des représentations minimales.**
:::

### Ecritures MA($\infty$) et AR($\infty$)

::: {#prp-ARMAMAinf .proposition}
## Ecriture MA($\infty$)

Soit $(X_t)_{t\in \Z}$ un processus ARMA(p,q) de **représentation minimale et causale**. Il admet alors la représentation MA($\infty$) $$
X_t=\Phi^{-1}(B)\Theta(B)\varepsilon_t=\varepsilon_t+\sum_{i=1}^{+\infty}\psi_i\varepsilon_{t-i},
$$ où les coefficients $(\psi_i)_{i\in N}$ forment une famille absolument sommable et vérifient l'équation de récurrence linéaire : $$
\psi_i-\sum_{j=1}^p\varphi_j\psi_{i-j}=\theta_i,\ \forall i\in \N, 
$$ avec $\psi_i=0$ pour $i<0$, $\psi_0=1$, $\theta_0=1$ et $\theta_i=0$ pour $i>q$.
:::

::: {.callout-warning collapse="true" icon="false"}
### Preuve 

La représentation étant causale, les racines du polynôme $\Phi(z)$ sont toutes à l'extérieur du disque unité. L'inverse de $\Phi(z)$ s'écrit alors comme une série de puissances positives de $B$. Multipliée par $\Theta(B)$, on garde une série en puissances positives de $B$ et on a donc l'écriture : $$
X_t=\Phi^{-1}(B)\Theta(B)\varepsilon_t=\varepsilon_t+\sum_{i=1}^{+\infty}\psi_i\varepsilon_{t-i},
\text{ pour tout }t\in \Z. 
$$ Grâce à cette écriture MA($\infty$) de $(X_t)_{t\in \Z}$, on peut réécrire l'équation $$
X_t-\varphi_1X_{t-1}-\varphi_2X_{t-2}-\cdots-\varphi_pX_{t-p}=\varepsilon_t
+\theta_1\varepsilon_{t-1}+\theta_2\varepsilon_{t-2}+\cdots+\theta_q\varepsilon_{t-q}
$$ sous la forme \begin{eqnarray*} 
&\left(
\varepsilon_t+\sum_{i=1}^{+\infty}\psi_i\varepsilon_{t-i}\right)-\varphi_1\left(\varepsilon_{t-1}+\sum_{i=1}^{+\infty}\psi_i\varepsilon_{t-1-i}\right)-\cdots
-\varphi_p
\left(\varepsilon_{t-p}+\sum_{i=1}^{+\infty}\psi_i\varepsilon_{t-p-i}\right)\\
&=\varepsilon_t
+\theta_1\varepsilon_{t-1}+\theta_2\varepsilon_{t-2}+\cdots+\theta_q\varepsilon_{t-q}.
\end{eqnarray*} 
En identifiant les coefficients devant chaque $\varepsilon_t$, on obtient bien la formule annoncée.
:::

::: {#prp-ARMAARinf .proposition}
## Ecriture AR($\infty$)

Soit $(X_t)_{t\in \Z}$ un processus ARMA(p,q) de **représentation minimale et inversible**. Il admet alors la représentation AR($\infty$) $$
\varepsilon_t=\Theta^{-1}(B)\Phi(B)X_t=X_t+\sum_{i=1}^{+\infty}\pi_iX_{t-i}, 
$$ où les coefficients $(\pi_i)_{i\in N}$ forment une famille absolument sommable et vérifient l'équation de récurrence linéaire : $$
\pi_i+\sum_{j=1}^q\theta_j\pi_{i-j}=-\varphi_i,\ \forall i\in \N, 
$$ avec $\pi_i=0$ pour $i<0$, $\varphi_0=-1$ et $\varphi_i=0$ pour $i>p$.
:::

::: {.callout-warning collapse="true" icon="false"}
### Preuve 

La représentation étant inversible, les racines du polynôme $\Theta(z)$ sont toutes à l'extérieur du disque unité. L'inverse de $\Theta(z)$ s'écrit alors comme une série de puissances positives de $B$. Multipliée par $\Phi(B)$, on garde une série en puissances positives de $B$ et on a donc l'écriture : 
$$
\varepsilon_t=\Theta^{-1}(B)\Phi(B)X_t=X_t+\sum_{i=1}^{+\infty}\pi_iX_{t-i},
\text{ pour tout }t\in \Z. 
$$ Grâce à cette écriture AR($\infty$), on peut réécrire l'équation 
$$
X_t-\varphi_1X_{t-1}-\varphi_2X_{t-2}-\cdots-\varphi_pX_{t-p}=\varepsilon_t
+\theta_1\varepsilon_{t-1}+\theta_2\varepsilon_{t-2}+\cdots+\theta_q\varepsilon_{t-q}
$$ sous la forme 
\begin{eqnarray*} 
& & X_t-\varphi_1 X_{t-1}-\varphi_2X_{t-2}-\cdots-\varphi_pX_{t-p} \\
&=& \left(X_t+\sum_{i=1}^{+\infty}\pi_iX_{t-i}\right)
+\theta_1 \left(X_{t-1}+\sum_{i=1}^{+\infty}\pi_iX_{t-i-1}\right)+\cdots+\theta_q \left(X_{t-q}+\sum_{i=1}^{+\infty}\pi_iX_{t-i-q}\right)
\end{eqnarray*} 
En identifiant les coefficients devant chaque $X_t$, on obtient bien la formule annoncée.
:::



### Liaisons temporelles

On considère un processus $(X_t)_{t\in\Z}$ qui est un ARMA($p,q$) d'équation 
$$X_{t}-\underset{j=1}{\stackrel{p}{\sum}}\varphi_jX_{t-j}=\varepsilon_{t} +\underset{i=1}{\stackrel{q}{\sum}}\theta_i\varepsilon_{t-i}.$$


- Fonction d'autocorrélation à partir de l'écriture ARMA($p,q$) :
<br>
On considère 
$X_{t+h}-\underset{j=1}{\stackrel{p}{\sum}}\varphi_jX_{t+h-j}=\varepsilon_{t+h} +\underset{i=1}{\stackrel{q}{\sum}}\theta_i\varepsilon_{t+h-i}$ et on prend la covariance avec $X_t$  ce qui donne 
    \begin{eqnarray*}
    \gamma_X(h)-\underset{j=1}{\stackrel{p}{\sum}}\varphi_j\gamma_X(h-j) &=&
    \C\left(\varepsilon_{t+h}
    +\underset{i=1}{\stackrel{q}{\sum}}\theta_i\varepsilon_{t+h-i},X_t\right)\\ &=&
    \C\left(\varepsilon_{t+h}
    +\underset{i=1}{\stackrel{q}{\sum}}\theta_i\varepsilon_{t+h-i},\varepsilon_t+\sum_{i=1}^{+\infty}\psi_i\varepsilon_{t-i}\right).
    \end{eqnarray*}

On a ausi que pour $h>q$, $\gamma_X(h)-\underset{j=1}{\stackrel{p}{\sum}}\varphi_j\gamma_X(h-j)=0.$<br>
Pour $0\leq h\leq q$, on obtient 
$$
\gamma_X(h)-\underset{j=1}{\stackrel{p}{\sum}}\varphi_j\gamma_X(h-j)=\sigma^2\left(\sum_{i=0}^{q-h}\theta_{h+i}\psi_i{}\right)\textrm{ avec } \psi_0=1 
$$

- Fonction d'autocorrélation avec l'écriture MA($\infty$) : 

En utilisant l'écriture MA($\infty$) du processus $(X_t)_{t\in \Z}$ on obtient cette fois-ci 
$$ 
\gamma_X(h)=\C\left(
\varepsilon_t+\sum_{i=1}^{+\infty}\psi_i\varepsilon_{t-i},\varepsilon_{t+h}+\sum_{i=1}^{+\infty}\psi_i\varepsilon_{t+h-i}\right)=\sigma^2\sum_{i=0}^{+\infty}
\psi_i\psi_{i+h}. 
$$

::: {.callout-note icon="false"}
### Remarques 

-   La première expression obtenue sous forme de relation de récurrence, permet de montrer que les autocorrélations simples décroissent de manière exponentielle ou sinusoïdale amortie vers 0 avec $h$. On peut montrer le même genre de résultat pour les autocorrélations partielles.
-   On constate, qu'à la différence des cas particuliers des processus AR($p$) ou MA($q$), il n'existe pas de caractérisation aisée pour les modèles ARMA($p,q$). Les autocorrélations simples ou partielles ne s'annulent pas à partir d'un certain rang.
:::


<!--

::: {#prp- .proposition}
Soient, pour $\left( i,j\right)\in\N^2$, \begin{align*} 
\Omega_{i,j} &=\left[ \begin{array}[c]{ccccc}
\rho_X\left( i\right) &\rho_X\left( i-1\right) &\ldots &\ldots &\rho_X\left(
i-j+1\right)\\ \rho_X\left( i-1\right) &\rho_X\left( i\right) &\rho_X\left(
i-1\right) &\ldots &\rho_X\left( i-j\right)\\ \rho_X\left( i-2\right)
&\rho_X\left( i-1\right) &\rho_X\left( i\right) &\ddots &\vdots\\ \vdots &\vdots
&\ddots &\ddots &\rho_X\left( i-1\right)\\ \rho_X\left( i-j+1\right)
&\rho_X\left( i-j\right) &\ldots &\rho_X\left( i-1\right) &\rho_X\left( i\right)
\end{array} \right] ,\\ \Delta_{i,j} &=\det\left(\Omega_{i,j}\right).
\end{align*} Pour un processus $\text{ARMA}\left( p,q\right)$, on a :

-   $\forall\left(i,j\right)\in\N^2,i>q,j>p:\Delta_{i,j}=0$
-   $\forall\left( i,j\right)\in\N^2,i\leq q:\Delta_{i,p}\neq0$
-   $\forall\left( i,j\right)\in\N^2,j\leq p:\Delta_{q,j}\neq0.$
:::

### Méthode du coin

::: {#prp-MethCoin .proposition}
## Méthode du coin

On peut visualiser le résultat précédent sous forme matricielle en représentant la matrice (pour $k$ assez grand) $$
M=\left(\Delta_{i,j}\right)_{\left(i,j\right)\in\left\{1,\ldots,k\right\} ^{2}}
$$ et faire ainsi apparaître un coin : $$ 
M=\left[ \begin{tabular}[c]{cccccc} $\Delta_{1,1}$ &$\ldots$
&$\Delta_{1,p}$ &$\Delta_{1,p+1}$ &$\ldots$ &$\Delta_{1,k}$\\ $\vdots$
&&$\vdots$ &$\vdots$ &&$\vdots$\\ $\Delta_{q,1}$ &$\ldots$ &$\Delta_{q,p}$
&$\Delta_{q,p+1}$ &$\ldots$ &$\Delta_{q,k}$\\\cline{4-6} $\Delta_{q+1,1}$
&$\ldots$ &$\Delta_{q+1,p}$ &\multicolumn{1}{|c}{} &&\\ $\vdots$ &&$\vdots$
&\multicolumn{1}{|c}{} &0 &\\ $\Delta_{k,1}$ &$\ldots$ &$\Delta_{k,p}$
&\multicolumn{1}{|c}{} && \end{tabular} \right]. 
$$
:::

-->

## Vers les processus ARIMA/ SARIMA

### ARIMA

Dans la pratique, les processus sont rarement stationnaires. Les modèles **ARIMA (AutoRegressive Integrated Moving Average)** sont une extension des processus ARMA aux processus non stationnaires. Ils sont basés sur l'idée générale suivante essentiellement conçue pour les processus non stationnaires à tendance polynomiale :  on peut différencier suffisamment le processus initial afin d'obtenir un processus sans tendance et appliquer un modèle ARMA sur le processus différencié. 

::: {#def-arima .definition}
On dit qu'un processus $(X_t)_{t\in \Z}$ admet une représentation [**ARIMA**$(p,d,q)$]]{style="color:blue;"} ($p,d,q\in\mathbb N$) s'il vérifie l'équation : 
$$
\Phi(B)(I-B)^dX_t= c + \Theta(B)\varepsilon_t,\ \forall t\in\Z 
$$ où \begin{eqnarray*} 
& & c\in\mathbb{R}\\
& & \Phi(B)=I-\varphi_1B-\varphi_2B^2-\cdots-\varphi_pB^p,\ (\varphi_1,\ldots, \varphi_p)\in\R^p,\ \varphi_p\neq0\\
& & \Theta(B)= I+\theta_1B+\theta_2B^2+\cdots+\theta_qB^q,\ (\theta_1,\ldots,\theta_q)\in\R^q,\ \ \ \theta_q\neq 0\\
& & (\varepsilon_t)_{t\in \Z}\sim\text{WN}(0,\sigma^2)
\end{eqnarray*}
:::

::: {.callout-note icon="false"}
### Remarques 

-   On peut montrer que, dans le cas d'un processus ARIMA$(p,d,q)$, le processus $(I-B)^dX_t$ est asympotiquement (au sens quand $t\to +\infty$) un processus ARMA(p,q).

-   On peut aussi établir des représentations AR($\infty$) et MA($\infty$) pour les processus ARIMA.
:::

Par rapport aux caractéristiques évoquées précédemment (voir @sec-liaisonsAR et @sec-liaisonsMA) sur les autocorrélations et autocorrélations partielles pour les processus AR et MA (voir @sec-liaisonsAR et @sec-liaisonsMA), on a le résultat suivant. 

::: {.proposition #prp-ARIMA}

- Si la série temporelle suit un ARIMA(p,d,0) alors l'ACF et le PACF sur les **données différenciées** vérifient

  -   l'ACF décroit exponentiellement ou sinusoidal
  -   Un pic significatif au lag $p$ sur le PACF et pas après le lag $p$

- Si la série temporelle suit un ARIMA(0, d, q) alors l' ACF et le PACF sur les **données différenciées** vérifient :

  -   le PACF est décroissant exponentiellement ou sinusoidal
  -   Un pic significatif au lag $q$ pour l'ACF et pas après le lag $q$  
:::  

::: {.example #exm-ARIMA}
Pour illustrer la @prp-ARIMA, la @fig-exARIMA310 montre les autocorrélations et autocorrélations partielles empiriques pour les mesures différenciées une fois du processus 
$$
(I-0.1 B -0.2 B^2 - 0.6 B^3)(I-B) X_t = \varepsilon_t,\ (\varepsilon_t)_{t\in\Z}\sim \text{WN}(0,1)
$$

```{r}
#| echo: false
#| fig-height: 4
#| label: fig-exARIMA310
#| layout-ncol: 2
#| fig-cap: Résultats pour le processus ARIMA(3,1,0) $(I-0.1 B -0.2 B^2 - 0.6 B^3)(I-B) X_t = \varepsilon_t,\ (\varepsilon_t)_{t\in\Z}\sim \text{WN}(0,1)$
#| fig-subcap:  
#|  - Autocorrélations empiriques
#|  - Autocorrélations partielles empiriques

ts.sim<-arima.sim(list(order = c(3,1,0), ar = c(0.1,0.2,0.6)), n = 500)
autoplot(acf(diff(ts.sim),plot=F)) #+ggtitle("ARIMA(3,1,0)")
autoplot(pacf(diff(ts.sim),plot=F)) #+ggtitle("ARIMA(3,1,0)")
```

Pour illustrer la @prp-ARIMA, la @fig-exARIMA013 montre les autocorrélations et autocorrélations partielles empiriques pour les mesures différenciées une fois du processus 
$$
(I-B) X_t = (I-0.6 B -0.6 B^2 - 0.2 B^3)\varepsilon_t,\ (\varepsilon_t)_{t\in\Z}\sim \text{WN}(0,1)
$$

```{r}
#| echo: false
#| fig-height: 4
#| label: fig-exARIMA013
#| layout-ncol: 2
#| fig-cap: Résultats pour le processus ARIMA(0,1,3) $(I-B) X_t = (I-0.6 B -0.6 B^2 - 0.2 B^3)\varepsilon_t,\ (\varepsilon_t)_{t\in\Z}\sim \text{WN}(0,1)$
#| fig-subcap:  
#|  - Autocorrélations empiriques
#|  - Autocorrélations partielles empiriques
ts.sim<-arima.sim(list(order = c(0,1,3), ma = c(0.6,0.6,0.2)), n = 500)
autoplot(acf(diff(ts.sim),plot=F))  #+ggtitle("ARIMA(0,1,3)")
autoplot(pacf(diff(ts.sim),plot=F)) #+ggtitle("ARIMA(0,1,3)")
#grid.arrange(g1,g2,ncol=2)
```

:::

### Sélection de modèle

Quand on étudie une série temporelle, on peut être amené à hésiter entre plusieurs modèles pour la modéliser au mieux. 
Supposons que l'on considère une collection de modèles ARIMA $\mathcal M = \left\{\texttt{mod}_1,\ldots,\texttt{mod}_K\right\}$. 
On est dans le cadre classique d'un problème de sélection de modèles. On traite donc ce problème par les étapes suivantes :  

-   Ajustement de chaque modèle $\texttt{mod}_k$: Pour chaque modèle fixé, on estime les paramètres (par maximum de vraisemblance le plus souvent)

-   On minimise un critère pénalisé pour choisir le meilleur modèle. Par exemple, 

    -   le critère AIC : $\mbox{AIC}(\texttt{mod}_k) = -2 \mbox{ log. vrais.} + 2 (p+q+1+\mathbb{1}_{c\neq 0})$

    -   le critère AIC corrigé : $\mbox{AICc}(\texttt{mod}_k) = \mbox{AIC}(\texttt{mod}_k) + \frac{2 (p+q+1+\mathbb{1}_{c\neq 0})(p+q+2+\mathbb{1}_{c\neq 0})}{n-p-q-2-\mathbb{1}_{c\neq 0}}$

    -   le critère BIC : $\mbox{BIC}(\texttt{mod}_k) = -2 \mbox{ log. vrais.} + (p+q+1+\mathbb{1}_{c\neq 0}) \log(n)$


::: { .example #exm-selmod}
Pour illustrer la sélection de modèle, on a ici simulées une série temporelle selon le modèle ARIMA(1,1,2) suivant

$$
(I-0.8 B)(I-B) X_t = (I-0.3B+0.6 B^2)\varepsilon_t,\ \ (\varepsilon_t)_t\sim \text{WN}(0,1.5)
$$

En @fig-exselmod, on trace la série temporelle étudiée $(X_t)_{t\in\Z}$ ainsi que les autocorrélations et autocorrélations partielles empiriques de $((I-B)X_t)_{t\in\Z}$. 

```{r}
#| echo: false
#| fig-height: 4
#| label: fig-exselmod
#| layout-ncol: 2
#| fig-cap: Résultats pour une série temporelle simulées selon un modèle ARIMA(1,1,2).  
#| fig-subcap:  
#|  - Série temporelle observée sur les 200 premiers temps
#|  - Autocorrélations empiriques
#|  - Autocorrélations partielles empiriques
ts.sim<-arima.sim(n=200,list(order = c(1,1,2),ar=c(0.8),ma=c(-0.3,0.6)),sd=sqrt(1.5))
autoplot(ts.sim,series="Yt")
autoplot(acf(diff(ts.sim),plot=F))  
autoplot(pacf(diff(ts.sim),plot=F))
```


On considère la collection de modèles suivante $\mathcal M = \{ARIMA(p,1,q),\ p\in\{1,2,3,4\},\ q\in\{0,1,2\}\}$. On calcule les différents critères évoqués précédemment. Au vu des résultats ci-dessous, les trois critères sélectionnent un modèle ARIMA(1,1,2). 

```{r,echo=F}
orderAux<-matrix(c(1,1,0,2,1,0,3,1,0,4,1,0,1,1,1,2,1,1,3,1,1,4,1,1,1,1,2,2,1,2,3,1,2,4,1,2),nrow=3)
AIC<-AICc<-BIC<-rep(0,ncol(orderAux))

# doute sur le code, je ne comprends pas tout. 
# pris sur internet

for (j in 1:ncol(orderAux)){
  mod<-arima(ts.sim,order=orderAux[,j])
  AIC[j]<-mod$aic
  npar <- length(mod$coef) + 1
  nstar <- length(mod$residuals) - mod$arma[6] - mod$arma[7] * mod$arma[5]
  BIC[j] <- mod$aic + npar * (log(nstar) - 2)
  AICc[j] <- mod$aic + 2 * npar * (nstar/(nstar - npar - 1) - 1)
}

df<-data.frame(
  mod=apply(orderAux,2,function(x){paste("ARIMA(",x[1],",",x[2],",",x[3],")",sep="")}),
  AIC=round(AIC,2),AICc=round(AICc,2),BIC=round(BIC,2))
df
#rmarkdown::paged_table(df)
```

```{r}
#| echo: false
#| eval: false
ts.sim<-arima.sim(list(order = c(3,1,0), ar = c(0.1,0.2,0.6)), n = 1000)
orderAux<-matrix(c(1,1,0,2,1,0,3,1,0,4,1,0,1,1,1,2,1,1,3,1,1,4,1,1,1,1,2,2,1,2,3,1,2),nrow=3)
AIC<-AICc<-BIC<-rep(0,ncol(orderAux))

for (j in 1:ncol(orderAux)){
  mod<-arima(ts.sim,order=orderAux[,j])
  AIC[j]<-mod$aic
  npar <- length(mod$coef) + 1
  nstar <- length(mod$residuals) - mod$arma[6] - mod$arma[7] * mod$arma[5]
  BIC[j] <- mod$aic + npar * (log(nstar) - 2)
  AICc[j] <- mod$aic + 2 * npar * (nstar/(nstar - npar - 1) - 1)
}

df<-data.frame(
  mod=apply(orderAux,2,function(x){paste("ARIMA(",x[1],",",x[2],",",x[3],")",sep="")}),
  AIC=round(AIC,2),AICc=round(AICc,2),BIC=round(BIC,2))
df
#rmarkdown::paged_table(df)
```

:::


### SARIMA

Les modèles SARIMA, pour Seasonal ARIMA, sont une extension des modèles ARIMA à des séries temporelles présentant une saisonnalité.

::: {.definition #def-SARIMA}
On dit qu'un processus $(X_t)_{t\in \Z}$ admet une représentation [**SARIMA**$(p,d,q)(P,D,Q)[s]$]{style="color:blue;"} s'il vérifie l'équation : 
$$
\textcolor{red}{(I-B^s)^D\Phi_P(B^s)}\textcolor{blue}{\Phi_p(B)(I-B)^d}\ X_t=c + \textcolor{blue}{\Theta_q(B)}\textcolor{red}{\Theta_Q(B^s)}\varepsilon_t,\ \forall t\in\Z 
$$ 
où 
\begin{eqnarray*} 
& & c\in\mathbb{R}\\
& & \Phi_p(B)=I-\varphi_1B-\varphi_2B^2-\cdots-\varphi_pB^p,\ (\varphi_1,\ldots, \varphi_p)\in\R^p,\ \varphi_p\neq0\\
& & \Phi_P(B^s)=I-\phi_1B^s-\phi_2(B^s)^2-\cdots-\phi_P(B^s)^P,\ (\phi_1,\ldots, \phi_P)\in\R^P,\ \phi_P\neq 0\\
& & \Theta_q(B)= I+\theta_1B+\theta_2B^2+\cdots+\theta_qB^q,\ (\theta_1,\ldots,\theta_q)\in\R^q,\ \ \ \theta_q\neq 0\\
& & \Theta_Q(B^s)= I+\vartheta_1(B^s)+\vartheta_2(B^s)^2+\cdots+\vartheta_Q(B^s)^Q,\ (\vartheta_1,\ldots,\vartheta_Q)\in\R^Q,\ \ \ \vartheta_Q\neq 0\\
& & (\varepsilon_t)_{t\in \Z}\sim\text{WN}(0,\sigma^2)
\end{eqnarray*}
:::

En pratique, voici comment on peut procéder pour "identifier" les différents ordres : 

-   Choix de $d$, $D$ et $s$ :

    -   Allure de la série pour détecter la non-stationnarité et la saisonnalité $s$
    -   Différencier la série successivement jusqu'à stationnarité ($d=1,2$ et $D=0,1$ suffisent souvent) \vspace{0.3cm}

-   Choix de $p$, $q$ ($P$ et $Q$ idem sur les lags $k\times s$) : A partir de ACF et pACF sur la série différenciée

    -   Cassure de l'ACF au lag $q$ -\> MA(q)
    -   Cassure pACF au lag $p$ -\> AR(p)
    -   Aucune cassure -\> ARMA
    -   Décroissance très lente ou inexistante, il faut peut être encore différencier (mais avec parcimonie)

::: {#exm-sarima .example}

Dans cet exemple, on simule une série temporelle selon un modèle SARIMA(0,1,1)(0,1,1)\[12\] 
$$
(I-B^{12})(I-B) X_t = (I+0.8 B)(I+0.4 B^{12})\varepsilon_t
$$

La série est représentée en @fig-exSARIMA1-Xt. 

```{r}
#| echo: false
#| fig-height: 4
#| label: fig-exSARIMA1-Xt
#| fig-cap: Série temporelle simulée selon un modèle SARIMA(0,1,1)(0,1,1)\[12\]
set.seed(1234)
#Xt<-sarima.sim(d=1, ma=0.8, D=1, sma=0.4, S=12, n=360)
Xt<-sarima.sim(d=1, ma=0.8, D=1, sma=0.4, S=12, n=600)
autoplot(Xt)
```

La @fig-exSARIMAmonth permet de partir sur une saisonnalité par mois pour la série temporelle $(s=12)$. 


```{r}
#| echo: false
#| fig-height: 4
#| label: fig-exSARIMAmonth
#| layout-ncol: 2
#| fig-cap: Pour l'étude de la saisonnalité.  
#| fig-subcap:  
#|  - Tracé de la série par mois
#|  - Tracé des points $(X_t,X_{t+h})$ pour $h\in\{1,\ldots,12\}$

ggmonthplot(ts(Xt,frequency = 12))
gglagplot(Xt,lags=12,do.lines=F)
```


On trace ensuite les autocorrélations et autocorrélations empiriques pour la série $(X_t)$ (voir @fig-exSARIMAACF)).


```{r}
#| echo: false
#| fig-height: 4
#| label: fig-exSARIMAACF
#| layout-ncol: 2
#| fig-cap: Autocorrélations et autocorrélations partielles de la série $(X_t)_{t\in\Z}$.  
#| fig-subcap:  
#|  - Autocorrélations empiriques
#|  - Autocorrelations partielles empiriques
autoplot(acf(Xt,lag.max=60,plot=F))
autoplot(pacf(Xt,lag.max=60,plot=F))
```

La @fig-exSARIMAdiffXt (resp. @fig-exSARIMAdiff12Xt) représente la série temporelle différentiée $(I-B)X_t$ (resp. $(I-B^{12})X_t$)


```{r}
#| echo: false
#| fig-height: 4
#| label: fig-exSARIMAdiffXt
#| fig-cap: Tracé de la série temporelle différenciée  $((I-B)X_t)_{t\in\Z}$.  
autoplot(diff(Xt))
```


```{r}
#| echo: false
#| fig-height: 4
#| label: fig-exSARIMAdiff12Xt
#| fig-cap: Tracé de la série temporelle différenciée  $((I-B^{12})X_t)_{t\in\Z}$. 
autoplot(diff(diff(Xt),12))
```

On trace ensuite en @fig-exSARIMAYt, les autocorrélations et autocorrélations partielles empiriques de la série
$$
Y_t = (I-B^{12})(I-B) X_t
$$


```{r}
#| echo: false
#| fig-height: 4
#| label: fig-exSARIMAYt
#| layout-ncol: 2
#| fig-cap: Autocorrélations et autocorrélations partielles de la série $Y_t = (I-B^{12})(I-B) X_t$.  
#| fig-subcap:  
#|  - Autocorrélations empiriques
#|  - Autocorrelations partielles empiriques
Yt<-diff(diff(Xt),12)
autoplot(acf(Yt,lag.max=50,plot=F))
autoplot(pacf(Yt,lag.max=50,plot=F))+ylab("pACF")
```

On décide alors d'ajuster sur la série $(X_t)_{t\in\Z}$ un modèle SARIMA(0,1,1)(0,1,1)[12]. L'estimation des paramètres est donnée ici à l'aide de la fonction `arima()` et on teste ensuite la blancheur des résidus avec la fonction `Box.test()` :   

\scriptsize
```{r}
#| echo: true
res<-arima(Xt,order=c(0,1,1),seasonal=list(order=c(0,1,1),period=12))
res
Box.test(res$residuals)
```
\normalsize
:::

Pour finir ce chapitre, on étude un jeu de données réelles concernant la pollution en oxyde d'azote à Londre. 

::: {#exm-ozone .example}
## Pollution en oxyde d'azote à Londron

La @fig-azoteLondre représente la mesure de pollution en oxyde d'azote à Londre par heure entre le 1/01/98 et le 21/01/98. On trace les autocorrélations empiriques (voir en @fig-azoteLondreACF) et les autocorrélations partielles empiriques (voir en @fig-azoteLondrepACF) de cette série temporelle $(X_t)$. 

```{r}
#| echo: false
#| fig-height: 4
#| label: fig-azoteLondre
#| fig-cap: Mesure de l'oxyde d'azote dans l'air à Londre par heure entre le 1/01/98 et le 21/01/98. 
set.seed(1234)
polluant<-readRDS("../TP/dataEssaiTP/air_pollution_london_short.RDS")
firstHour <- 24*(as.Date("1998-01-21 23:00:00")-as.Date("1998-01-01 00:00:00"))
tt <- ts(polluant$nox,start=c(1998,firstHour),frequency=24*365)
autoplot(tt)+ylab("oxyde d'azote")
```

```{r}
#| echo: false
#| fig-height: 4
#| label: fig-azoteLondreACF
#| fig-cap: Autocorrélations de la série $X_t$
tta<-ts(tt,start=1,frequency=24)
autoplot(acf(tta,plot=F,lag.max=200))
```

```{r}
#| echo: false
#| fig-height: 4
#| label: fig-azoteLondrepACF
#| fig-cap: Autocorrélations partielles de la série $X_t$
tta<-ts(tt,start=1,frequency=24)
autoplot(pacf(tta,plot=F,lag.max=200))
```


On analyse ensuite les autocorrélations et autocorrélations partielles empiriques de la série différenciée $Y_t = (I-B^{24}) X_t$ (voir @fig-azoteLondreACFdiff). 
On peut alors considérer des modèles SARIMA avec $0\leq q\leq 12$, $0\leq p \leq 2$, $0\leq Q \leq 2$, $0\leq P \leq 1$. 

```{r}
#| echo: false
#| fig-height: 4
#| label: fig-azoteLondreACFdiff
#| layout-ncol: 2
#| fig-cap: Autocorrélations et autocorrélations partielles de la série $Y_t = (I-B^{24})X_t$.  
#| fig-subcap:  
#|  - Autocorrélations empiriques
#|  - Autocorrelations partielles empiriques
autoplot(acf(diff(tta,24),plot=F,lag.max=100))
autoplot(pacf(diff(tta,24),plot=F,lag.max = 100))
```

On met donc en place une sélection de modèles avec le critère AIC à l'aide de la fonction `auto.arima()`. On retient un modèle SARIMA$(0,1,1)(0,0,1)[24]$ et on contrôle la blancheur des résidus obtenus.

```{r}
#| echo: true
set.seed(1234)
res<-auto.arima(tta,ic=c("aic"),
                max.p=2,max.q=12,
                max.P=1,max.Q=2,seasonal=TRUE)
res
Box.test(res$residuals)
```


On représente en @fig-azoteLondrefitted la série temporelle ajustée par ce modèle par rapport à la série temporelle observée.  

```{r}
#| echo: false
#| fig-height: 4
#| label: fig-azoteLondrefitted
#| fig-cap: Série temporelle estimée avec un modèle SARIMA(0,1,1)(0,0,1)(24) par rapport à la série temporelle observée.  

g1<-autoplot(tta)+forecast::autolayer(res$fitted)
g2<-autoplot(res$residuals)
g1
#grid.arrange(g1,g2,ncol=2)
```

```{r}
#| echo: false
#| eval: false
#| fig-height: 4

fit <- arima(tta, order = c(0,0,1),
             seasonal = list(order = c(0,1,1)))
prediction<-predict(fit,n.ahead=5*24)
Aux<-cbind(tta,prediction$pred)
colnames(Aux)<-c("Yt","Pred")
autoplot(Aux)
```
:::
