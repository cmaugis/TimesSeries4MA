[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction aux séries temporelles",
    "section": "",
    "text": "Préface\nCe polycopié a été rédigé dans le cadre de l’UF de Processus Stochastiques de 4ème année, spécialité Mathématiques Appliquées de l’INSA Toulouse.\nJe tiens tout particulièrement à remercier Jean-Yves DAUXOIS qui a accepté de partager ses sources de cours pour mes premiers pas dans l’enseignement des séries temporelles !\n\\[\n\\newcommand\\mcy{{\\mathcal{y}}}\n\\]\n\n\n\n\nAragon, Yves. 2016. Séries Temporelles Avec r. EDP sciences.\n\n\nBrockwell, Peter J, and Richard A Davis. 2002. Introduction to Time Series and Forecasting. Springer.\n\n\n———. 2009. Time Series: Theory and Methods. Springer science & business media.\n\n\nDauxois, Jean-Yves. 2020. “Introduction à l’étude Des Séries Temporelles.” Polycopié cours INSA Toulouse."
  },
  {
    "objectID": "intro.html#quelques-exemples-de-séries-temporelles",
    "href": "intro.html#quelques-exemples-de-séries-temporelles",
    "title": "1  Introduction",
    "section": "1.1 Quelques exemples de séries temporelles",
    "text": "1.1 Quelques exemples de séries temporelles\nLes séries temporelles sont présentes dans de nombreux domaines d’applications. On donne ici quelques exemples illustratifs.\n\nExample 1.1 On peut par exemple s’intéresser à l’évolution de la taille d’une population. La Figure 1.1 représente la population (en million d’habitants) en France entre 1846 à 1951 (à gauche) et aux USA entre 1790 et 1990 (à droite). Ces deux séries temporelles n’ont pas la même tendance générale.\n\n\n\n\n\nFigure 1.1: Taille de la population en France (à gauche) et aux USA (à droite).\n\n\n\n\n\n\n\nExample 1.2 On donne ici un exemple bien connu en écologie. La Figure 1.2 représente le nombre de fourrures de lièvres (Hare) et de lynx échangées à la Compagnie de la Baie d’Hudson de 1845 à 1935. On constate la présence de cycles des populations de lynx (le prédateur) et du lièvre (sa proie). Ce jeu de données est disponible dans la librairie fpp3 sous le nom de pelt.\n\n\n\n\n\nFigure 1.2: Nombre de fourrures échangées de lièvres et de lynx à la Compagnie de la Baie d’Hudson de 1845 à 1935.\n\n\n\n\n\n\n\nExample 1.3 On peut aussi donner un exemple en climatologie. La Figure 1.3 représente l’évolution de la surface de la glace dans l’Artique de 1972 à 2018. On peut observer sur cette série temporelle une périodicité et une tendance à décroitre.\n\n\n\n\n\n\nFigure 1.3: Evolution de la surface de la glace dans l’Artique. En haut à gauche, la série temporelle. En haut à droite, chaque courbe représente l’évoluton d’une année selon les différents mois. En bas, regroupement des mesures par mois.\n\n\n\n\n\n\n\nExample 1.4 Comme dernier exemple, on s’intéresse à la célèbre série temporelle AirPassengers donnant le nombre mensuel (en milliers) de passagers des lignes aériennes entre les années 1949 et 1960 (à gauche de la Figure 1.4). On peut observer une périodicité dans la série temporelle et une croissance exponentielle avec une augmentation de la variabilité. On peut transformer cette dernière série en considérant le logarithme népérien du nombre de passagers aériens (à droite de la Figure 1.4).\n\n\n\n\n\nFigure 1.4: Nombre de passagers (en milliers) ayant emprunté les lignes aériennes de 1949 à 1960 à gauche. La série est log-transformée à droite.\n\n\n\n\n\n\nLes exemples de séries temporelles ne manquent pas. On pourrait encore citer l’évolution de la température sur une période donnée, la concentration en polluants au cours du temps, le cours d’une action en finance, la consommation en électricité, l’évolution des recherches d’un mot sur internet, l’électrocardiogramme d’une personne en médecine, ….\nDans la suite de ce cours, on suppose que la série observée est une réalisation d’une suite de variables aléatoires. Il faut bien noter que l’ordre est important puisque l’on étudie un phénomène au cours du temps.\n\n\n\n\n\n\nOn note \\(Y_t\\) la valeur du phénomène au temps \\(t\\in T\\) où \\(T\\) est l’espace de temps discret (souvent \\(T=\\N\\) voire \\(\\Z\\)). Le processus \\((Y_t)_{t\\in T}\\) est alors appelé série temporelle."
  },
  {
    "objectID": "intro.html#quelles-questions-autour-de-létude-dune-série-temporelle",
    "href": "intro.html#quelles-questions-autour-de-létude-dune-série-temporelle",
    "title": "1  Introduction",
    "section": "1.2 Quelles questions autour de l’étude d’une série temporelle ?",
    "text": "1.2 Quelles questions autour de l’étude d’une série temporelle ?\nNous sommes tout d’abord confrontés au problème de la modélisation d’une série temporelle. On va chercher à ajuster un modèle qui décrit “au mieux” le comportement de la série temporelle. Ceci va nécessiter d’estimer les paramètres pour ajuster le modèle, tester son adéquation à la série temporelle étudiée, voire faire de la sélection de modèle entre plusieurs modèles en compétition.\nIl est important de noter que l’on ne recherche pas un ajustement exact aux données. L’objectif est d’extraire la structure générale du signal et d’éliminer le bruit. On va en particulier chercher à déceler la présence d’une tendance, d’une saisonnalité dans les données.\nSi l’on reprend la série temporelle du nombre de passagers aériens (voir Example 1.4), on peut estimer une saisonnalité et en déduire la série corrigée des variations saisonnières (voir Figure 1.5). On reviendra plus tard sur cette notion.\n\n\n\n\n\nFigure 1.5: Série corrigée des variations saisonnières de la série temporelle AirPassengers (logtransformée).\n\n\n\n\nAprès l’analyse et la modélisation d’une série temporelle, on est souvent intéressé par la prévision des futures valeurs de la série. Par exemple, on peut chercher à prédire la température dans les jours à venir, la concentration en ozone, … Plus formellement, à partir des \\(n\\) premiers instants \\(Y_1,\\ldots,Y_n\\) de la série, on souhaite prévoir la valeur suivante \\(Y_{n+1}\\) ou plus éloignée dans le temps \\(Y_{n+h}\\). Un exemple de prévision sur l’année suivante de la série temporelle AirPassengers est données en Figure 1.6. On devra s’intéresser alors à l’erreur de prédiction, à la taille des intervalles de prédiction, …\n\n\n\n\n\nFigure 1.6: Prévision sur l’année suivante pour la série AirPassengers avec intervalle de prédiction.\n\n\n\n\n\n\n\n\nAragon, Yves. 2016. Séries Temporelles Avec r. EDP sciences.\n\n\nBrockwell, Peter J, and Richard A Davis. 2002. Introduction to Time Series and Forecasting. Springer.\n\n\n———. 2009. Time Series: Theory and Methods. Springer science & business media.\n\n\nDauxois, Jean-Yves. 2020. “Introduction à l’étude Des Séries Temporelles.” Polycopié cours INSA Toulouse."
  },
  {
    "objectID": "chap1.html#décomposition-dune-série-temporelle",
    "href": "chap1.html#décomposition-dune-série-temporelle",
    "title": "2  Tendances et saisonnalités",
    "section": "2.1 Décomposition d’une série temporelle",
    "text": "2.1 Décomposition d’une série temporelle\nComme on a pu le constater dans les exemples de l’ introduction, quitte à faire une transformation des données au préalable, on peut décomposer une série temporelle en un modèle additif composé de trois termes de la façon suivante.\n\nDefinition 2.1 (Décomposition en modèle additif)  La série temporelle \\((Y_t)_{t\\in T}\\) se décompose en\n\\[\nY_t=m_t+s_t+X_t,\\ \\forall t\\in T\n\\tag{2.1}\\]\noù\n\n\\(m_t\\) est la tendance : une fonction déterministe à variation lente qui capte les variations de niveau et que l’on espère assez lisse\n\\(s_t\\) est la saisonnalité : une fonction déterministe périodique de période \\(r\\) (\\(s_{t+r}=s_t,\\ \\forall t\\)) telle que \\[\\sum_{h=1}^rs_{t+h}=0,\\ \\forall t \\in T\\]\n\\(X_t\\) est un bruit aléatoire stationnaire appelé parfois résidu. Ce terme sera à définir dans la suite.\n\n\n\n\n\n\n\n\nRemarque\n\n\n\n\n\nL’hypothèse de somme nulle de la saisonnalité sur la période \\(r\\) n’est pas une hypothèse contraignante. En effet, on peut s’y ramener facilement en modifiant la tendance : si \\(\\sum_{h=1}^rs_{t+h}= a\\) alors on définit \\(\\tilde s_{t} = s_t - \\frac a r\\) et \\(\\tilde m_t = m_t + \\frac a r\\).\n\n\n\nSi la saisonnalité et les variations semblent croître, on peut parfois atténuer ce phénomène en tentant une transformation des données. C’est en particulier ce que l’on peut constater sur les données AirPassengers quand on prend leur logarithme (voir Figure 1.4). Donc quitte à faire une transformation des données, on peut supposer le modèle additif (Equation 2.1).\n\n\n\n\n\n\nObjectif du cours\n\n\n\n\nApprendre à modéliser et estimer les composantes tendance \\((m_t)_{t\\in T}\\) et saisonnalité \\((s_t)_{t\\in T}\\)\nApprendre à modéliser le bruit résiduel \\((X_t)_{t\\in T}\\)\nFaire des prévisions sur les valeurs futures de la série temporelle initiale \\((Y_t)_{t\\in T}\\).\n\n\n\nLa méthode générale pour étudier une série temporelle est la suivante :\n\n\n\n\n\n\nMéthode générale d’étude\n\n\n\n\nEtape 1 : on trace la série des données observées \\((Y_1,\\ldots, Y_n)\\) et on essaie de déceler ses principales caractéristiques : une tendance, une composante saisonnière, une ou des ruptures dans le comportement de la série, une ou des observations aberrantes.\nEtape 2 : On estime / supprime la tendance \\((m_t)_{t\\in T}\\) et la composante saisonnière \\((s_t)_{t\\in T}\\) pour obtenir une série \\((X_t)_{t\\in T}\\) de résidus stationnaires. Pour cela, on peut utiliser plusieurs techniques: transformer les données, estimer les tendances et composantes saisonnières puis les supprimer des données, différencier la série.\nEtape 3 : Choisir un modèle de processus stationnaire pour la série des résidus\nEtape 4 : Prévoir les valeurs futures de la série en prévoyant d’abord celles des résidus puis remonter jusqu’à la série initiale en utilisant les transformations inverses."
  },
  {
    "objectID": "chap1.html#estimation-élimination-dune-tendance-en-labsence-de-saisonnalité",
    "href": "chap1.html#estimation-élimination-dune-tendance-en-labsence-de-saisonnalité",
    "title": "2  Tendances et saisonnalités",
    "section": "2.2 Estimation / élimination d’une tendance en l’absence de saisonnalité",
    "text": "2.2 Estimation / élimination d’une tendance en l’absence de saisonnalité\nDans cette partie, on suppose que la série \\((Y_t)_{t\\in T}\\) n’a pas de saisonnalité. Elle suit donc le modèle additif suivant\n\\[\nY_t=m_t+X_t,\\ \\forall  t\\in T.\n\\]\nSans perte de généralité, on suppose que \\((X_t)_{t\\in T}\\) est un processus centré (\\(\\mathbb E[X_t] = 0,\\ \\forall t\\in T\\)). En effet, si \\(\\mathbb E[X_t]\\neq 0\\), on remplace \\(m_t\\) et \\(X_t\\) par \\(m_t+\\mathbb E [X_t]\\) et \\(X_t-\\mathbb E[X_t]\\) respectivement.\nOn suppose également que l’on observe le processus sur les instants de temps \\(t=1,\\ldots,n\\) : \\((Y_1,\\ldots,Y_n)\\).\nSans exhaustivité, la fonction tendance peut prendre l’une de ces formes\n\nTendance linéaire : \\(m_t=\\alpha_0+\\alpha_1 t\\)\nTendance quadratique : \\(m_t=\\alpha_0+\\alpha_1 t+\\alpha_2 t^2\\)\nTendance polynomiale : \\(m_t=\\alpha_0+\\alpha_1 t+\\alpha_2 t^2+\\cdots+\\alpha_k t^k\\)\nTendance exponentielle : \\(m_t=c_0+c_1\\alpha^t\\)\nTendance de Gompertz : \\(m_t=\\exp(c_0+c_1\\alpha^t)\\)\nTendance Logistique : \\(m_t=1/(c_0+c_1\\alpha^t)\\)\nou bien des mélanges de ces types de fonctions.\n\n\n2.2.1 Estimation de la tendance par moindres carrés\nDans cette section, on suppose que la tendance est une combinaison linéaire de fonctions temporelles, connues et déterministes : \\[\nm_t=\\sum_{j=1}^p\\alpha_j m_t^{(j)}.\n\\]\nPour déterminer \\(m_t\\), on cherche donc à estimer les coefficients inconnus \\(\\alpha_j\\), pour \\(j=1,\\ldots,p\\). Pour cela, nous pouvons utiliser l’estimation par moindres carrés.\n\\[\\begin{eqnarray*}\n(\\hat \\alpha_1,\\ldots,\\hat \\alpha_p)\n&=&\\underset{(\\alpha_1,...,\\alpha_p)\\in \\mathbb R^p}{\\mbox{argmin}} \\sum_{t=1}^n(Y_t-m_t)^2 \\\\\n&=&\\underset{(\\alpha_1,...,\\alpha_p)\\in \\mathbb R^p}{\\mbox{argmin}} \\sum_{t=1}^n \\left(Y_t-\\alpha_1 m_t^{(1)}-\\ldots - \\alpha_p m_t^{(p)}\\right)^2.\n\\end{eqnarray*}\\]\nOn peut constater que l’on se ramène à un problème de régression linéaire de la forme\n\\[\n\\underbrace{\n\\left(\n\\begin{array}{c}\n  Y_1   \\\\\n  \\vdots   \\\\\n  Y_n\n\\end{array}\n\\right)}_{\\mathbb Y}\n=\n\\underbrace{\n\\left(\n\\begin{array}{ccc}\nm^{(1)}_1 & \\cdots  & m^{(p)}_1   \\\\\n\\vdots  & \\ddots  & \\vdots   \\\\\nm^{(1)}_n &  \\cdots &   m^{(p)}_n\n\\end{array}\n\\right)}_{\\mathbb X}\n\\\n\\underbrace{\n\\left(\n\\begin{array}{c}\n  \\alpha_1   \\\\\n  \\vdots   \\\\\n  \\alpha_p\n\\end{array}\n\\right)}_{\\theta}\n+\n\\underbrace{\n\\left(\n\\begin{array}{c}\n  X_1   \\\\\n  \\vdots   \\\\\n  X_n\n\\end{array}\n\\right)}_{\\varepsilon}\n\\] D’après les résultats de la régression linéaire, si \\(\\mathbb X'\\mathbb X\\) est inversible, \\[\n\\hat \\theta = (\\mathbb X '\\mathbb X )^{-1}\\mathbb X' \\mathbb Y.\n\\]\nOn obtient alors les données corrigées de la tendance via l’expression suivante\n\\[\n\\hat Y^{\\text{CT}}_t=Y_t-\\hat m_t = Y_t - \\sum_{j=1}^p\\hat \\alpha_j m_t^{(j)}\n\\]\n\nExample 2.1 On considère la série temporelle \\((Y_t)_{t\\in \\mathbb N}\\) définie par la relation\n\\[Y_t = (1+0.01\\ t^2) + X_t \\textrm{ avec } X_t \\underset{\\textrm{ i.i.d}}{\\sim} \\mathcal{N}(0,4^2).\\] On observe les \\(n=100\\) premières valeurs de cette série (voir Figure 2.1, courbe noire).\nOn considère les trois fonctions tendances suivantes\n\n\\(m_{1,t} = \\alpha_0 + \\alpha_1 t\\)\n\\(m_{2,t} = \\alpha_0 + \\alpha_1 t + \\alpha_2 t^2\\)\n\\(m_{3,t} = \\alpha_0 + \\alpha_1 t + \\alpha_2 t^2 + \\alpha_3 t^3\\)\n\nLes coefficients sont estimés par moindres carrés pour les trois tendances :\n\npour une tendance linéaire :\n\n\n\n\nCall:\nlm(formula = y ~ x1, data = dataaux)\n\nCoefficients:\n(Intercept)           x1  \n    -18.547        1.045  \n\n\n\npour une tendance quadratique :\n\n\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dataaux)\n\nCoefficients:\n(Intercept)           x1           x2  \n   -0.07851     -0.04172      0.01076  \n\n\n\npour une tendance cubique :\n\n\n\n\nCall:\nlm(formula = y ~ ., data = dataaux)\n\nCoefficients:\n(Intercept)           x1           x2           x3  \n  3.993e-01   -9.711e-02    1.212e-02   -9.005e-06  \n\n\nLes estimations associées à \\(\\hat m_{1,t}\\), \\(\\hat m_{2,t}\\) et \\(\\hat m_{3,t}\\) sont représentées sur la Figure 2.1 en rouge, bleu et rose respectivement. Les courbes bleue et rose se superposent car la tendance recherchée \\(m_t = 1+0.01 t^2\\) est quadratique.\n\n\n\n\n\nFigure 2.1: Exemple pour l’estimation de la tendance par moindres carrés\n\n\n\n\n\n\n\n2.2.2 Estimation de la tendance par filtrage de moyenne mobile\n\n\n2.2.3 Opérateurs retard et avance\nPour pouvoir parler de moyenne mobile, nous devons commencer par définir deux opérateurs importants pour ce cours de séries temporelles: les opérateurs retard \\(B\\) et avance \\(F\\).\n\nDefinition 2.2  L’opérateur retard \\(B\\) sur une série temporelle \\((Y_t)_{t\\in T}\\) est défini par : \\[\nB\\ Y_t=Y_{t-1},\\ \\forall t\\in T.\n\\] On note de manière naturelle : \\(B^{h}\\ Y_t=Y_{t-h},\\ \\forall t\\in T \\textrm{ et } \\forall h\\in \\mathbb N^*\\). \nL’opérateur avance \\(F\\) sur une série temporelle \\((Y_t)_{t\\in T}\\) est défini par : \\[\nF\\ Y_t=Y_{t+1},\\ \\forall t\\in T.\n\\] On note aussi \\(F^{h}\\ Y_t=Y_{t+h},\\ \\forall t\\in T \\textrm{ et } \\forall h\\in \\mathbb N^*\\) et \\(B^{-h}\\)=\\(F^{h}\\), pour tout \\(h\\).\n\n A partir de ces deux opérateurs, on peut définir la notion de moyenne mobile.\n\nDefinition 2.3  Une moyenne mobile est un opérateur linéaire de la forme \\[\nM=\\sum_{h=-m_1}^{m_2}\\theta_h B^{-h},\n\\] où \\((m_1,m_2)\\in \\mathbb N\\times\\mathbb N\\) et \\(\\theta_h\\in \\mathbb R\\) pour tout \\(h\\).\nL’ordre de la moyenne mobile est l’entier \\(m_1+m_2+1\\).\nLa moyenne mobile est dite\n\nnormalisée si \\(\\sum_{h=-m_1}^{m_2} \\theta_h=1.\\)  Il s’agit alors d’une moyenne au sens où on l’entend habituellement.\ncentrée si \\(m_1=m_2\\)  On prend autant d’instants du passé que du futur.\nsymétrique si \\(m_1=m_2=m\\) et \\(\\theta_h=\\theta_{-h}\\), pour \\(h=1,\\ldots,m\\).  On donne des poids identiques aux instants passés et futurs de même ordre.\n\n\n\nAinsi la moyenne mobile \\(M\\) appliquée à la série temporelle \\((Y_t)_{t\\in T}\\) donne\n\\[\\begin{eqnarray*}\nM\\ Y_t&=& \\sum_{h=-m_1}^{m_2}\\theta_h B^{-h}\\ Y_t\\\\\n&=&\\theta_{-m_1}Y_{t-m_1}+\\cdots+\\theta_{-1}Y_{t-1}\\\\\n\\\\\n&+&\\theta_0 Y_t\\\\\n\\\\\n&+&\\theta_1 Y_{t+1}+\\cdots+\\theta_{m_2}Y_{t+m_2}\n\\end{eqnarray*}\\]\nAppliquer l’opérateur \\(M\\) revient donc à faire une “moyenne locale pondérée” des termes \\(Y_{t-m_1},\\ldots,Y_0,\\ldots,Y_{t+m_2}\\). \n\nExample 2.2 (Exemple important de la moyenne mobile \\(M_{2q+1}\\))  Soit \\(q\\in \\mathbb N^*\\). On considère la moyenne mobile \\(M_{2q+1}\\) définie par\n\\[\nM_{2q+1}Y_t=\\frac{1}{2q+1} \\sum_{j=-q}^qY_{t-j}.\n\\] \\(M_{2q+1}\\) est une moyenne mobile avec \\(m_1=m_2=q\\) et \\(\\theta_h=\\frac{1}{1+2q} \\mathbb 1_{|h|\\leq q}\\). C’est donc une moyenne mobile finie, symétrique et normalisée d’ordre \\(2q+1\\). Au vu de la définition des coefficients, cette moyenne mobile peut être vue comme un filtre “passe-bas”.\nLa moyenne mobile \\(M_{2q+1}\\) laisse invariante les tendances linéaires : si \\(m_t=a+bt\\) alors \\[\nM_{2q+1}m_t = \\frac{1}{2q+1}\\sum_{j=-q}^qm_{t-j}=\n\\frac{1}{2q+1}\\sum_{j=-q}^q (a + bt -b j) = a+bt = m_t.\n\\]\nSoit une série temporelle \\((Y_t)_{t\\in T}\\) de la forme \\(Y_t=m_t+X_t\\), où \\(m_t\\) est la tendance et \\(X_t\\) un processus centré. Si la tendance est pratiquement linéaire et que la moyenne empirique des \\(X_t\\) est proche de 0 (ce qui est en tout cas vrai pour \\(q\\) grand), on a \\[\nM_{2q+1}Y_t=\\frac{1}{2q+1}\\sum_{j=-q}^qm_{t-j}+\\frac{1}{2q+1}\\sum_{j=-q}^qX_{t-j}\\approx m_t.\n\\]\n\\(\\Longrightarrow\\) la moyenne mobile nous donne une estimation de la tendance quand elle est pratiquement linéaire.\nPour illustrer ce point, on observe les \\(50\\) premières réalisations d’une série temporelle \\((Y_t)_{t\\in \\mathbb N}\\) définie par \\(Y_t = (5+0.1 t) + X_t \\textrm{ avec } X_t \\underset{\\textrm{ i.i.d }}{\\sim} \\mathcal{N}(0,1)\\).\nLa Figure 2.2 montre l’estimation de la tendance par la moyenne mobile \\(M_{2q+1}\\) \\[\n  \\hat m_t = \\frac{1}{2q+1}\\underset{j=-q}{\\stackrel{q}{\\sum}}\\ Y_{t-j},\\ \\ \\forall q+1\\leq t \\leq n-q\n\\] pour \\(2q+1=3\\) et \\(2q+1=7\\).\n\n\n\n\n\nFigure 2.2: Exemples d’estimation de la tendance d’une série temporelle à l’aide d’une moyenne mobile \\(M_{2q+1}\\).\n\n\n\n\n\n\n\n2.2.4 Estimation de la tendance par lissage exponentiel\n\nDefinition 2.4 Le lissage exponentiel simple consiste à estimer la tendance via la formule récursive suivante : pour \\(\\alpha\\in [0,1]\\), \\[\n\\left\\{\n\\begin{array}{ll}\n\\hat m_t=\\alpha Y_t + (1-\\alpha)\\hat m_{t-1}& \\textrm{ pour }t=2,\\ldots,n\\\\\n\\hat m_1=Y_1 &\n\\end{array}\\right.\n\\]\n\n\n\nProposition 2.1 Le lissage exponentiel simple est une moyenne mobile normalisée.\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nEn résolvant l’équation de récurrence on a : \\[\\begin{eqnarray*}\n\\hat m_t&=&\\alpha Y_t + (1-\\alpha)\\hat m_{t-1}=\\alpha Y_t + \\alpha(1-\\alpha)Y_{t-1}+(1-\\alpha)^2\\hat m_{t-2}\\\\\n&=&\\alpha Y_t + \\alpha(1-\\alpha)Y_{t-1}+\\alpha(1-\\alpha)^2Y_{t-2}+(1-\\alpha)^3\\hat m_{t-3}\\\\\n&=&\\sum_{k=0}^{t-2}\\alpha(1-\\alpha)^kY_{t-k}+(1-\\alpha)^{t-1}Y_1\\\\\n&=& \\underset{h=0}{\\stackrel{t-1}{\\sum}}\\theta_h Y_{t-h}\n\\end{eqnarray*}\\] avec les coefficients \\[\n\\theta_h = \\left\\{\n\\begin{array}{l l}\n\\alpha (1-\\alpha)^h & \\textrm{si } 0\\leq h \\leq t-2\\\\\n(1-\\alpha)^{t-1} & \\textrm{si } h=t-1\\\\\n0 & \\textrm{sinon}.\n\\end{array}\\right.\n\\]\nDonc \\(\\underset{h=0}{\\stackrel{t-1}{\\sum}} \\theta_h = \\underset{h=0}{\\stackrel{t-2}{\\sum}} \\alpha (1-\\alpha)^h + (1-\\alpha)^{t-1} = \\alpha \\frac{1-(1-\\alpha)^{t-1}}{1-(1-\\alpha)} + (1-\\alpha)^{t-1}=1\\).\n\n\n\nOn parle de lissage exponentiel car c’est une moyenne pondérée des valeurs précédentes avec une décroissance exponentielle des poids (voir Figure 2.3 (b)). Ainsi les observations les plus récentes ont le plus de poids. Le choix de \\(\\alpha\\) est fondamental (voir Figure 2.3 (a)): Plus \\(\\alpha\\) est proche de 1, plus on donne de poids à la dernière observation. L’estimation est alors moins lisse et on tend vers du sur-ajustement. Plus \\(\\alpha\\) est proche de 0, plus le lissage exponentiel s’appuie sur une mémoire longue de la série temporelle.\n\n\n\n\n\n\n\n(a) Les poids \\(\\alpha(1-\\alpha)^h\\) en fonction de \\(\\alpha\\) pour plusieurs valeurs de \\(h\\)\n\n\n\n\n\n\n\n(b) Les poids \\(\\theta_h\\) en fonction de \\(h\\) pour plusieurs valeurs de \\(\\alpha\\)\n\n\n\n\nFigure 2.3: Comportement des poids du lissage exponentiel\n\n\nLe lissage exponentiel simple est parfois utilisé pour la prévision. Si l’on observe la série temporelle sur les instants \\(\\{1,\\ldots,n\\}\\) alors une prévision à l’horizon \\(h\\) de la série est donnée par :\n\\[\n\\hat Y_{n,h} = \\hat Y_{n+h}=\\hat Y_{n+1}=\\sum_{k=0}^{n-2}\\alpha(1-\\alpha)^kY_{n-k}+(1-\\alpha)^{n-1}Y_1 = \\hat m_n.\n\\]\nLa formule de mise jour du lissage exponentiel permet de voir qu’une observation supplémentaire de la série ne nécessite pas de recalculer entièrement la prévision. En effet, si on observe en plus la valeur au temps \\(n+1\\) alors \\[\n\\hat Y_{n+1,1} = \\hat m_{n+1} = \\alpha Y_{n+1} + (1-\\alpha) \\hat m_n.\n\\]\nComme évoqué précédemment, le choix du paramètre \\(\\alpha\\) est important. On peut chercher à minimiser l’erreur de prévision :\n\\[\n\\hat \\alpha = \\underset{\\alpha_1,\\ldots,\\alpha_p}{\\mbox{argmin}}\\ \\underset{t=1}{\\stackrel{n-h}{\\sum}}\\left(Y_{t+h}-\\hat Y_{t+h}^{(\\alpha_i)}\\right)^2\n\\]\n\nExample 2.3 On applique le lissage exponentiel simple pour deux valeurs de \\(\\alpha\\) sur les trois séries temporelles suivantes, observées pour \\(t\\in\\{1,\\ldots,50\\}\\) :\n\n\\(Y_t^{[1]} = 1 + X_t\\) avec \\(X_t\\underset{\\textrm{i.i.d}}{\\sim} \\mathcal{N}(0,0.01)\\)\n\\(Y_t^{[2]} = 1 + 0.05 t + X_t\\) avec \\(X_t\\underset{\\textrm{i.i.d}}{\\sim} \\mathcal{N}(0,0.1)\\)\n\\(Y_t^{[3]} = \\mathbb{1}_{1\\leq t \\leq 25}+ 2 \\mathbb{1}_{t&gt;25} + X_t\\) avec \\(X_t\\underset{\\textrm{i.i.d}}{\\sim} \\mathcal{N}(0,0.1)\\)\n\n\n\n\n\n\n\n\n(a) La série temporelle \\(Y_t^{[1]}\\)\n\n\n\n\n\n\n\n(b) La série temporelle \\(Y_t^{[2]}\\)\n\n\n\n\n\n\n\n\n\n(c) La série temporelle \\(Y_t^{[3]}\\)\n\n\n\n\nFigure 2.4: Exemples de lissage exponentiel simple\n\n\n\nDans le cadre du lissage exponentiel simple, on vient de voir que la prévision est constante. Une extension est le lissage exponentiel double où l’on souhaite une prédiction linéaire. Cette prédiction est donnée par\n\\[\n\\hat Y_{n+h} = \\hat a_n h + \\hat b_n\n\\] avec\n\\[\n\\left\\{\\begin{array}{l}\n\\hat a_n = \\hat a_{n-1} + (1-\\alpha)^2 (Y_n - \\hat Y_{n-1,1})\\\\\n\\hat b_n = \\hat b_{n-1} +\\hat a_{n-1}+ (1-\\alpha^2) (Y_n - \\hat Y_{n-1,1})\\\\\n\\hat a_2=Y_2-Y_1,\\ \\hat b_2=Y_1\n\\end{array}\\right.\n\\]\nDans la même famille de méthode, on peut citer la méthode de Holt-Winters qui est un lissage exponentiel double avec une formule de mise à jour différente (voir Figure 2.5).\n\nExample 2.4 On reprend les trois séries de l’exemple Example 2.3 et on applique un lissage exponentiel double avec la méthode de Holt-Winters.\n\n\n\n\n\n\n\n(a) La série temporelle \\(Y_t^{[1]}\\)\n\n\n\n\n\n\n\n(b) La série temporelle \\(Y_t^{[2]}\\)\n\n\n\n\n\n\n\n\n\n(c) La série temporelle \\(Y_t^{[3]}\\)\n\n\n\n\nFigure 2.5: Reprise des exemples précédents avec ici la méthode de Holt-Winters.\n\n\n\n\n\n2.2.5 Elimination de la tendance par différenciation\n\nDefinition 2.5 Soit \\((Y_t)_{t\\in T}\\) une série temporelle.\n\nL’opérateur différenciation à l’ordre 1 \\(\\nabla\\) d’une série temporelle est défini par \\[\n\\nabla Y_t=Y_t-Y_{t-1}=Y_t-BY_t=(I-B)Y_t,\\ \\forall t\\in T.\n\\] La différenciation à l’ordre \\(k\\) de la série \\((Y_t)_{t\\in T}\\) est définie par \\[\n\\nabla^kY_t=(I-B)^kY_t,\\ \\forall t\\in T.\n\\]\n\n\n\n\n\n\n\nAttention à la manipulation de cet opérateur.\n\n\n\n\n\\(\\nabla^2 Y_t = (I-B)(I-B) Y_t = Y_t - 2 Y_{t-1} + Y_{t-2}\\)\n\\(\\nabla^3 Y_t = (I-B)\\nabla^2 Y_t = Y_t - 3 Y_{t-1} + 3 Y_{t-2} - Y_{t-3}\\)\n\n\n\n\nProposition 2.2 Une tendance polynomiale \\(m_t\\) de degré \\(k\\) est réduite à une constante par une différenciation à l’ordre \\(k\\).\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nOn raisonne par récurrence.\n\nPour \\(k=1\\) : pour une tendance linéaire \\(m_t=a+bt\\), on a \\(\\nabla m_t=a+bt-a -b(t-1)=b.\\)\nOn suppose la proposition vraie au rang \\(k\\)\nAu rang \\(k+1\\): soit la tendance \\(m_t = \\underset{j=0}{\\stackrel{k+1}{\\sum}} a_j t^j = a_{k+1} t^{k+1} + P_k(t)\\). Alors \\[\\begin{eqnarray*}\n  \\nabla^{k+1} m_t &=& \\nabla^k (I-B)(a_{k+1} t^{k+1} + P_k(t))\\\\\n  &=& \\nabla^k \\left[a_{k+1} (t^{k+1} - (t-1)^{k+1}) + P_k(t) - P_k(t-1)\\right]\\\\\n  &=& \\nabla^k \\left[a_{k+1} (t-t+1) Q_{k}(t) + P_k(t) - P_k(t-1)\\right]\\\\\n  &=& \\textrm{constante}\n  \\end{eqnarray*}\\]\n\n\n\n\nLa différenciation permet d’éliminer les tendances polynomiales et donc pratiquement toutes les tendances car elles peuvent très souvent être approchées par des polynômes. Attention, il faut bien noter que cette technique permet d’éliminer la tendance mais ne l’estime pas.\n\nExample 2.5 En Figure 2.6, on a simulé une série temporelle définie par \\(Y_t = 1 + \\frac{1}{20} t + X_t \\textrm{ avec } X_t \\underset{\\textrm{ i.i.d }}{\\sim} \\mathcal{N}(0,0.25)\\). On voit que la différenciation de la série \\(\\nabla Y_t\\) élimine la tendance linéaire.\n\n\n\n\n\n\n\n(a) Représentation de la série \\((Y_t)_{t\\in T}\\) et la tendance linéaire \\(t\\mapsto 1 + \\frac{1}{20} t\\) en bleu\n\n\n\n\n\n\n\n(b) Représentation de la différenciation de la série \\(\\nabla Y_t\\)\n\n\n\n\nFigure 2.6: Illustration de l’élimination d’une tendance linéaire par la différenciation à l’ordre 1.\n\n\n\n\n\nExample 2.6 Dans cet exemple, on a simulé une série temporelle définie par \\(Y_t = 1 - 5\\ t + 0.25\\ t^2 + X_t \\textrm{ avec } X_t \\underset{\\textrm{ i.i.d }}{\\sim} \\mathcal{N}(0,100)\\).\n\n\n\n\n\n\n\n(a) Représentation de la série \\((Y_t)_t\\) et de la tendance quadratique en rouge\n\n\n\n\n\n\n\n(b) Différenciation à l’ordre 1\n\n\n\n\n\n\n\n\n\n(c) Différenciation à l’ordre 2\n\n\n\n\nFigure 2.7: Illustration de l’élimination d’une tendance quadratique par une différenciation d’ordre 2 mais pas d’ordre 1"
  },
  {
    "objectID": "chap1.html#estimation-élimination-de-la-tendance-et-de-la-saisonnalité",
    "href": "chap1.html#estimation-élimination-de-la-tendance-et-de-la-saisonnalité",
    "title": "2  Tendances et saisonnalités",
    "section": "2.3 Estimation / élimination de la tendance et de la saisonnalité",
    "text": "2.3 Estimation / élimination de la tendance et de la saisonnalité\nOn considère maintenant le cas général d’une série temporelle avec une tendance et une saisonnalité présentes dans la décomposition :\n\\[\nY_t=m_t+s_t+X_t,\\ \\forall t\\in T\n\\]\navec \\(\\mathbb E[X_t]=0\\), \\(s_{t+r}=s_t\\) et \\(\\underset{k=1}{\\stackrel{r}{\\sum}} s_{t+k}=0,\\ \\forall t\\in T\\).\n\n2.3.1 Estimation par moindres carrés\nOn peut reprendre la méthode des moindres carrés en supposant cette fois-ci que la tendance mais aussi la composante saisonnière sont des combinaisons linéaires de fonctions connues. On suppose donc qu’il existe des fonctions \\(m_t^{(j)}\\) pour \\(j=1,\\ldots,p\\), et \\(s_t^{(\\ell)}\\) pour \\(\\ell=1,\\ldots,q\\) telles que : \\[\nY_t=\\sum_{j=1}^p\\alpha_j m_t^{(j)}+\\sum_{\\ell=1}^q\\beta_\\ell s_t^{(\\ell)}+X_t,\\ \\forall t\\in T.\n\\]\nPour les fonctions de saisonnalité \\(s_t^{(\\ell)}\\), on peut par exemple considérer\n\ndes indicatrices : par exemple dans le cas d’une saisonnalité trimestrielle \\[\n\\forall \\ell\\in \\{1,\\ldots,4\\}, s_t^{(\\ell)}=\n      \\left\\{\n      \\begin{array}{cl}\n      1  & \\text{si le trimestre à l'instant } t \\text{ est }  \\ell    \\\\\n      0  & \\text{sinon}  \n      \\end{array}.\n      \\right.\n\\]\nune combinaison de fonctions sinusoïdales.\n\nOn peut noter que les saisonnalités \\(s_t^{(\\ell)}\\) peuvent avoir des périodes différentes.\nOn cherche donc à estimer le vecteur des coefficients inconnus \\(\\theta = (\\alpha_1,\\ldots,\\alpha_p,\\beta_1,\\ldots,\\beta_q)'\\) par la méthode des moindres carrés. On obtient alors les estimateurs \\(\\hat \\theta = (\\hat\\alpha_1,\\ldots,\\hat\\alpha_p,\\hat\\beta_1,\\ldots,\\hat\\beta_q)'\\) et on récupère les données ajustées\n\\[\n\\hat Y_t=\\hat m_t+\\hat s_t=\\sum_{j=1}^p\\hat \\alpha_j m_t^{(j)}+\\sum_{\\ell=1}^q \\hat\\beta_\\ell s_t^{(\\ell)},\\ \\forall t \\in T.\n\\]\nLa série corrigée des variations saisonnières (CVS) est alors définie par\n\\[\\hat Y^{\\text{CVS}}_t=Y_t-\\hat s_t.\\]\n\nExample 2.7 Dans cet exemple, on s’intéresse à la série temporelle \\(Y_t = 0.5 t + 3 \\mbox{cos}\\left(\\frac{\\pi\\ t}{6}\\right)+X_t,\\ X_t \\underset{\\textrm{ i.i.d }}{\\sim}\\mathcal{N}(0,1)\\). On considère les fonctions de tendance polynomiales \\(m_t^{(j)}=t^j\\) pour \\(j\\in\\{1,2,3\\}\\) et les fonctions de saisonnalité définies par \\(s_t^{(\\ell)}=\\mbox{cos}(\\theta_{\\ell} t)\\) avec \\((\\theta_1,\\theta_2,\\theta_3)=(\\frac{\\pi}{6},\\frac{\\pi}{4},\\frac{\\pi}{3})\\). A l’aide de la fonction lm(), on ajuste un modèle de régression linéaire\n\n\n\nCall:\nlm(formula = Yt ~ ., data = dfaux)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.47779 -0.63375 -0.02225  0.67613  2.20892 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.541e-02  7.165e-01   0.063 0.949761    \nm1           4.701e-01  1.207e-01   3.895 0.000338 ***\nm2           2.578e-03  5.484e-03   0.470 0.640591    \nm3          -4.399e-05  7.094e-05  -0.620 0.538477    \ns1           3.117e+00  2.385e-01  13.068  &lt; 2e-16 ***\ns2          -4.606e-01  2.379e-01  -1.936 0.059497 .  \ns3          -3.389e-01  2.364e-01  -1.434 0.158912    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.168 on 43 degrees of freedom\nMultiple R-squared:  0.9808,    Adjusted R-squared:  0.9781 \nF-statistic: 365.5 on 6 and 43 DF,  p-value: &lt; 2.2e-16\n\n\nOn retrouve des estimations des coefficients en cohérence avec les données simulées. La Figure 2.8 représente les \\(n=50\\) premiers temps simulés de la série \\(Y_t\\), les données ajustées \\(\\hat Y_t\\) et les données corrigées des variations saisonnières \\(\\hat Y^{\\text{CVS}}_t\\).\n\n\n\n\n\nFigure 2.8: Illustration de l’estimation de la tendance et saisonnalité par moindres carrés.\n\n\n\n\n\n\n\n2.3.2 Estimation par Moyenne Mobile\nOn suppose toujours que l’on a la décomposition\n\\[\nY_t=m_t+s_t+X_t,\\ \\ \\forall t\\in T\n\\]\nL’idée générale est de trouver une moyenne mobile \\(M\\) qui vérifie les propriétés suivantes :\n\nla moyenne mobile laisse invariante la tendance : \\(M\\ m_t=m_t\\)\nla moyenne mobile absorbe la saisonnalité : \\(M\\ s_t=0\\)\n\nla moyenne mobile réduit la variance du processus observé : \\(M\\ Y_t\\) a une variance plus faible que \\(Y_t\\).\n\nSi on trouve une telle moyenne mobile \\(M\\), \\(MY_t\\) estime la tendance \\(m_t\\) et on travaille ensuite sur \\(\\hat Y^{\\text{CT}}_t=Y_t-MY_t\\) pour estimer la saisonnalité.\n\n2.3.2.1 Cas d’une périodicité impaire\nOn suppose que la série temporelle admet une saisonnalité de période impaire \\(r=2q+1\\) et on observe cette série temporelle aux instants \\(t=1,\\ldots,n\\).\nOn reprend la moyenne mobile \\(M_{2q+1}\\) (voir Example 2.2) symétrique, normalisée et d’ordre \\(2q+1\\) : \\[\nM_{2q+1}=\\frac{1}{2q+1} \\sum_{h=-q}^q B^{h}.\n\\]\nCette moyenne mobile \\(M_{2q+1}\\) appliquée sur la série \\((Y_t)_{t\\in T}\\) donne directement une estimation de la tendance et annule la saisonnalité. En effet, \\[\nM_{2q+1}Y_t=\\frac{1}{2q+1} \\sum_{h=-q}^qm_{t-h}+\\frac{1}{2q+1} \\sum_{h=-q}^q s_{t-h}+\\frac{1}{2q+1} \\sum_{h=-q}^qX_{t-h}\n\\]\noù\n\nle premier terme donne une estimation de la tendance si celle-ci est assez lisse\nle second terme est nul par l’hypothèse \\(\\sum_{j=1}^r s_{t+j}=0,\\ \\forall t\\in T\\)\nle dernier terme est quasiment nul si \\(q\\) est assez grand (convergence vers \\(\\mathbb E[X_t]=0\\))\n\nUne fois que l’on a estimé la tendance \\(\\hat m_t=M_{2q+1}Y_t=\\frac{1}{2q+1} \\sum_{h=-q}^qY_{t-h}\\), on considère la série corrigée de la tendance \\(\\hat Y^{\\text{CT}}_t=Y_t-\\hat m_t\\), sur laquelle on va estimer la saisonnalité.\nPour \\(k=1,\\ldots,r\\), on calcule la moyenne \\(\\omega_k\\) des valeurs de la série corrigée de la tendance sur tous les points à une même distance \\(r\\) : \\[\n\\omega_k=\\frac{1}{\\text{Card}\\{j:q&lt;k+jr\\le n-q\\}}\\sum_{\\{j:q&lt;k+jr\\le n-q\\}}\\hat Y^{\\text{CT}}_{k+jr}\n\\]\nMais comme les termes \\(\\omega_k\\) ne sont pas nécessairement de somme nulle, on définit comme estimateur de la composante saisonnière \\[\\begin{eqnarray*}\n\\hat s_k=\\left\\{\n\\begin{array}{ll}\n\\omega_k-\\frac{1}{r}\\underset{i=1}{\\stackrel{r}{\\sum}}\\ \\omega_i &\\textrm{pour }k=1,\\ldots,r\\\\\n\\hat s_{k-r}&\\textrm{pour }k&gt;r.\n\\end{array}\n\\right.\n\\end{eqnarray*}\\] Ainsi on assure que \\((\\hat s_t)_{t\\in T}\\) est bien \\(r\\)-périodique.\n\n\n2.3.2.2 Cas d’une périodicité paire\nSi la périodicité est paire \\(r=2q\\), on peut utiliser une approche similaire en considérant la moyenne mobile symétrique normalisée : \\[\nM_{2q}Y_t=\\frac{1}{2q}\\left(Y_{t-q+\\frac{1}{2}}+\\ldots+Y_{t-\\frac{1}{2}}+Y_{t+\\frac{1}{2}}+\\ldots+Y_{t+q-\\frac{1}{2}}\\right),\n\\] où \\(Y_{t-\\frac{1}{2}}=\\frac{1}{2}\\left(Y_{t-1}+Y_t\\right)\\) pour estimer la tendance et éliminer la saisonnalité.\nPour se convaincre que la saisonnalité est éliminée, on peut réécrire \\(M_{2q}Y_t\\) : \\[\\begin{eqnarray*}\nM_{2q}Y_t &=& \\frac{1}{2q} \\underset{h=-(q-1)}{\\stackrel{q}{\\sum}} Y_{(t+h)-\\frac 1 2} \\\\\n&=& \\frac{1}{2q} \\underset{h=-(q-1)}{\\stackrel{q}{\\sum}} \\frac 1 2 (Y_{t+h-1}+Y_{t+h}) \\\\\n&=& \\frac{1}{4q} \\left\\{\\underset{h=-q}{\\stackrel{q-1}{\\sum}}  Y_{t+h} +  \\underset{h=-q+1}{\\stackrel{q}{\\sum}}  Y_{t+h}\\right\\} \\\\\n&=& \\frac{1}{2r} \\left\\{Y_{t-q} + 2 \\underset{h=-q+1}{\\stackrel{q-1}{\\sum}}  Y_{t+h} +Y_{t+q}\\right\\}\n\\end{eqnarray*}\\]\nAinsi \\(M_{2q} s_t = \\frac{1}{2r} \\left\\{\\underset{h=-q}{\\stackrel{q-1}{\\sum}} s_{t+h} + \\underset{h=-q+1}{\\stackrel{q}{\\sum}} s_{t+h}\\right\\}=0\\) car sommes de \\(r\\) termes consécutifs de \\(s_t\\).\nOn poursuit ensuite le même raisonnement que dans le cas d’une période impaire.\nEn général, on ré-estime la tendance en considérant les données corrigées des variations saisonnières et en appliquant une des techniques vues précédemment. On obtient alors \\(\\tilde m_t\\). La série des bruits est alors \\[\nX_t=Y_t-\\tilde m_t -\\hat s_t,\\forall t\\in T\n\\]\n\nExample 2.8 On s’intéresse à la consommation trimestrielle en électricité d’une entreprise de 1997 à 1999. Les valeurs observées sont données dans la Table 2.1 et représentées en Figure 2.9. On observe une périodicité par trimestre \\(r=2q=4\\).\n\n\nTable 2.1: Consommation électrique par trimestre de 1997 à 1999\n\n\n\n1997\n1998\n1999\n\n\n\n\nT1\n4.5\n5.5\n7.2\n\n\nT2\n4.1\n4.9\n6.4\n\n\nT3\n3.7\n4.4\n4.8\n\n\nT4\n5.1\n6.5\n6.8\n\n\n\n\n\n\n\n\n\nFigure 2.9: Représentation de la consommation électrique par trimestre\n\n\n\n\nOn commence donc par appliquer la moyenne mobile \\(M_4\\) sur la série \\(Y_t\\) pour estimer la tendance : \\(\\hat m_t = M_4 Y_t\\)\n\n\n       Qtr1   Qtr2   Qtr3   Qtr4\n1997     NA     NA 4.4750 4.7000\n1998 4.8875 5.1500 5.5375 5.9375\n1999 6.1750 6.2625     NA     NA\n\n\nOn forme alors la série corrigée de la tendance \\(\\hat Y^{\\text{CT}} = Y_t - \\hat m_t\\) :\n\n\n        Qtr1    Qtr2    Qtr3    Qtr4\n1997      NA      NA -0.7750  0.4000\n1998  0.6125 -0.2500 -1.1375  0.5625\n1999  1.0250  0.1375      NA      NA\n\n\nOn calcule alors les termes \\(\\omega_k\\)\n\\[\n\\begin{array}{l}\n\\omega_1 = \\frac 1 2 (\\hat Y^{\\text{CT}}_5 + \\hat Y^{\\text{CT}}_9) \\approx 0.8188\\\\\n\\omega_2 = \\frac 1 2 (\\hat Y^{\\text{CT}}_6 + \\hat Y^{\\text{CT}}_{10}) \\approx - 0.0562\\\\\n\\omega_3 = \\frac 1 2 (\\hat Y^{\\text{CT}}_3 + \\hat Y^{\\text{CT}}_7) \\approx -0.9562\\\\\n\\omega_4 = \\frac 1 2 (\\hat Y^{\\text{CT}}_4 + \\hat Y^{\\text{CT}}_8) \\approx 0.4812\\\\\n\\end{array}\n\\]\net \\(\\sum_{k=1}^4 \\omega_k = 0.0719\\). On en déduit alors la saisonnalité estimée \\(\\hat s_t\\) :\n\n\n          Qtr1      Qtr2      Qtr3      Qtr4\n1997  0.746875 -0.128125 -1.028125  0.409375\n1998  0.746875 -0.128125 -1.028125  0.409375\n1999  0.746875 -0.128125 -1.028125  0.409375\n\n\nCette procédure est faite par la fonction decompose() :\n\nelec&lt;-c(4.5,4.1,3.7,5.1,5.5,4.9,4.4,6.5,7.2,6.4,4.8,6.8)\nautoplot(decompose(ts(data=elec,start=c(1997,1),end=c(1999,4),frequency=4),type=\"additive\"))\n\n\n\n\n\n\nExample 2.9 On reprend la série AirPassengers (voir Example 1.4). La Figure 2.10 donne la sortie de la fonction decompose() appliquée sur la série logtransformée. On retrouve la saisonnalité (de période une année) et une tendance croissante pratiquement linéaire.\n\nautoplot(decompose(log(AirPassengers),type=\"additive\"))\n\n\n\n\nFigure 2.10: Sortie de la fonction decompose() sur la série AirPassengers logtransformée.\n\n\n\n\n\n\n\n\n2.3.3 Elimination de la saisonnalité par différenciation\nPour supprimer une saisonnalité de périodicité \\(r\\), on peut utiliser une méthode de différenciation en utilisant l’opérateur \\(\\nabla_r=I-B^r\\) à la série temporelle \\((Y_t)_{t\\in T}\\) afin d’obtenir la série \\[\n\\nabla_r Y_t=Y_t-Y_{t-r},\\ \\forall t\\geq r\n\\]\nCette série ne contient plus de saisonnalité car \\(\\nabla_r Y_t=m_t-m_{t-r}+X_t-X_{t-r}\\).\nOn peut alors utiliser une des techniques vues précédemment pour estimer / éliminer la tendance en l’absence de saisonnalité sur la série désaisonnalisée \\((\\nabla_r Y_t)_{t\\geq r}\\).\n\n\n\n\n\n\n\\(\\nabla_r \\neq \\nabla^r\\)\n\n\n\nAttention à ne pas confondre l’opérateur \\(\\nabla_r\\) et l’opérateur \\(\\nabla^k\\) (voir Definition 2.5).\n\n\n\nExample 2.10 On applique l’opérateur de différenciation \\(\\nabla_{12}\\) à la série AirPassengers logtransformée. Figure 2.11 montre la série initiale à gauche et la série différenciée \\(\\nabla_{12} Y_t\\) à droite. Cette dernière ne présente plus de saisonnalité.\n\n\n\n\n\nFigure 2.11: Illustration de la différenciation de période 12 sur la série AirPassengers logtransformée.\n\n\n\n\n\n\n\n\n\n\nAragon, Yves. 2016. Séries Temporelles Avec r. EDP sciences.\n\n\nBrockwell, Peter J, and Richard A Davis. 2002. Introduction to Time Series and Forecasting. Springer.\n\n\n———. 2009. Time Series: Theory and Methods. Springer science & business media.\n\n\nDauxois, Jean-Yves. 2020. “Introduction à l’étude Des Séries Temporelles.” Polycopié cours INSA Toulouse."
  },
  {
    "objectID": "chap2.html#processus-stochastiques",
    "href": "chap2.html#processus-stochastiques",
    "title": "3  Modélisation aléatoire des séries temporelles",
    "section": "3.1 Processus stochastiques",
    "text": "3.1 Processus stochastiques\n\n3.1.1 Définition\n\nDefinition 3.1 On appelle processus stochastique toute famille de variables aléatoires (v.a.) \\((X_t)_{t\\in T}\\) d’un espace probabilisé \\((\\Omega,\\mathcal A, P)\\) vers un espace probabilisable \\((E, \\mathcal E)\\) \\[\n\\forall t\\in T,\\ X_t\\text{ est une v.a. de } (\\Omega,\\mathcal A, P) \\text{ vers }(E, \\mathcal E).\n\\]\nL’ensemble \\(T\\) est appelé espace des temps et \\(E\\) espace des états. Chacun de ces espaces peut être discret ou continu. \nPour \\(\\omega\\in \\Omega\\), on appelle trajectoire du processus la fonction (déterministe) : \\[\nt\\mapsto X_t(\\omega).\n\\]\n\n\nDans ce cours, les séries temporelles sont vues comme des réalisations d’un processus stochastique à espace des temps discret, à savoir \\(T=\\N\\), ou pour des raisons mathématiques \\(T=\\Z\\). Elles ne sont observées que sur un intervalle de temps fini (on observe qu’une partie de la trajectoire du processus). En général, on considère \\(E=\\R\\) comme espace des états et la tribu borélienne \\(\\mathcal E=\\mathcal B_{\\R}\\).\nDans ce cours on n’abordera pas les processus spatiaux (\\(\\mbox{dim}(T)\\geq 2\\)), ni les séries temporelles multivariées (\\(E=\\R^n\\)).\n\n\n3.1.2 Premiers exemples de processus\n\n3.1.2.1 Bruit blanc fort / faible\n\nDefinition 3.2 Un bruit blanc fort est une suite de variables indépendantes et identiquement distribuées (i.i.d.) \\((X_t)_{t\\in T}\\) centrées et de variance \\(\\sigma^2\\).\nOn note \\((X_t)_{t\\in T} \\sim \\text{IID}(0,\\sigma^2)\\).\n\n Dans le cas d’un bruit blanc fort, il n’y a aucun dépendance entre les observations. En particulier, la connaissance de \\(X_1,\\ldots,X_n\\) n’informe en rien sur la valeur (future) de \\(X_{n+h}\\). Si la loi commune des v.a. est gaussienne, on parle de bruit blanc gaussien (la Figure 3.1 donne un exemple de simulation d’une partie de trajectoire).\n\nDefinition 3.3 On appelle bruit blanc gaussien, tout bruit blanc fort pour lequel la loi commune des v.a.r. \\((X_t)_{t\\in T}\\) est une \\(\\mathcal N(0,\\sigma^2)\\).\n\n\n\n\n\n\nFigure 3.1: Trajectoire d’un bruit blanc gaussien \\(\\mathcal N(0,1)\\)\n\n\n\n\nL’hypothèse d’i.i.d est très forte, on peut considérer une version faible d’un bruit blanc en ne demandant que la non-corrélation.\n\nDefinition 3.4 On appelle bruit blanc faible toute suite de v.a.r. \\((X_t)_{t\\in T}\\)\n\ncentrées et de variance \\(\\sigma^2\\)\nnon corrélées\n\nOn note \\((X_t)_{t\\in T} \\sim \\text{WN}(0,\\sigma^2)\\)\n\n En tant que tels, ces processus n’ont pas d’intéret direct pour un objectif de prévision. Mais nous verrons qu’ils jouent un rôle important dans la modélisation de séries temporelles plus complexes.\n\n\n3.1.2.2 Marche aléatoire\nA partir d’un bruit blanc fort, on peut construire une marche aléatoire.\n\nDefinition 3.5 Une marche aléatoire \\((S_t)_{t\\in \\mathbb N}\\) est obtenue par \\[\nS_t=X_1+\\ldots+X_t,\\ \\ \\forall t\\in \\mathbb{N}\n\\] où \\((X_t)_{t\\in \\mathbb N}\\) est un bruit blanc fort.\nSi ce dernier est un processus binaire (\\(\\mathbb{P}(X_t=1)=\\mathbb{P}(X_t=-1)=\\frac 1 2\\)) alors la marche aléatoire \\((S_t)_{t\\in \\mathbb N}\\) est dite symétrique simple.\n\n La Figure 3.2 montre des exemples de trajectoires de marches aléatoires.\n\n\n\n\n\n\n\n(a) … symétrique simple\n\n\n\n\n\n\n\n(b) … à partir d’un bruit blanc gaussien de variance \\(0.5^2\\)\n\n\n\n\nFigure 3.2: Trajectoire d’une marche aléatoire …\n\n\n\n\n3.1.2.3 Processus gaussien\n\nDefinition 3.6 Un processus gaussien à temps discret \\((X_t)_{t\\in \\mathbb Z}\\) est une série temporelle telle que la loi de n’importe quel vecteur extrait est gaussien \\[\n\\forall n\\in \\N^*, \\forall (t_1,\\ldots,t_n)\\in \\Z^n : (X_{t_1},\\ldots,X_{t_n}) \\text{ est un vecteur gaussien.}\n\\]\n\n Un bruit blanc gaussien est donc un processus gaussien.\n\n\n3.1.2.4 Processus MA(1)\n\nDefinition 3.7 On appelle processus moyenne mobile d’ordre 1 toute série temporelle \\((X_t)_{t\\in \\mathbb Z}\\) définie par : \\[\nX_t=\\varepsilon_t+\\theta\\ \\varepsilon_{t-1},\\ \\forall t\\in \\mathbb Z,\n\\] où \\((\\varepsilon_t)_{t\\in \\mathbb Z}\\sim \\text{WN}(0,\\sigma^2)\\) et \\(\\theta \\in \\mathbb R\\).\nOn note \\((X_t)_{t\\in \\mathbb Z}\\sim\\)MA(1).\n\n\nLa Figure 3.3 donne des exemples de trajectoires de processus MA(1). Cette notion de processus moyenne mobile sera étendue à des ordres supérieurs et fera l’objet d’une étude approfondie dans la Section 5.3 du Chapter 5.\n\n\n\n\n\n\n\n(a) pour \\(\\theta=0.2\\)\n\n\n\n\n\n\n\n(b) pour \\(\\theta=1\\)\n\n\n\n\nFigure 3.3: Exemple de trajectoires d’un processus MA(1)\n\n\n\n\n3.1.2.5 Processus AR(1)\n\nDefinition 3.8 On appelle processus autorégressif d’ordre 1 toute série temporelle \\((X_t)_{t\\in \\mathbb Z}\\) définie par : \\[\nX_t=\\varphi_0+\\varphi_1X_{t-1}+ \\varepsilon_t,\\ \\forall t\\in \\mathbb Z,\n\\] où \\((\\varepsilon_t)_{t\\in \\mathbb Z}\\sim \\text{WN}(0,\\sigma^2)\\) et \\((\\varphi_0,\\varphi_1) \\in \\mathbb R^2\\).\nOn note \\((X_t)_{t\\in \\mathbb Z}\\sim\\)AR(1).\n\n\nLa Figure 3.4 donne des exemples de trajectoires de processus AR(1). Cette notion de processus autorégressif sera étendue à des ordres supérieurs et fera l’objet d’une étude approfondie dans la Section 5.2 du Chapter 5.\n\n\n\n\n\nFigure 3.4: Exemple de trajectoire d’un processus AR(1)"
  },
  {
    "objectID": "chap2.html#rappels-sur-lespace-l2-et-les-processus-du-second-ordre",
    "href": "chap2.html#rappels-sur-lespace-l2-et-les-processus-du-second-ordre",
    "title": "3  Modélisation aléatoire des séries temporelles",
    "section": "3.2 Rappels sur l’espace \\(L^2\\) et les processus du second ordre",
    "text": "3.2 Rappels sur l’espace \\(L^2\\) et les processus du second ordre\n\n3.2.1 Espace \\(L^2\\)\n\nDefinition 3.9 On dit qu’une v.a.r. \\(X\\) est dans l’espace \\(L^2(\\Omega, \\mathcal A, P)\\) si l’on a \\(\\E[X^2]&lt;+\\infty.\\)\nCet espace \\(L^2(\\Omega, \\mathcal A, P)\\) est un espace de Hilbert muni du produit scalaire \\[\n\\langle X,Y\\rangle_{L^2}=\\E[XY]\n\\] et de la norme \\[\n||X||_{L^2}=\\sqrt{\\E[X^2]}.\n\\]\n\n\nDans cet espace \\(L^2\\), on a donc que \\(X\\) et \\(Y\\) sont orthogonaux (\\(X\\perp Y\\)) si \\(\\E[XY]=0\\). Si de plus les v.a. \\(X\\) et \\(Y\\) sont centrées, cela revient à avoir \\(\\C(X,Y)=0\\).\n\nTheorem 3.1 (Inégalité de Cauchy-Schwarz) Si \\(X\\) et \\(Y\\) sont deux v.a. dans \\(L^2(\\Omega, \\mathcal A, P)\\) alors la v.a. \\(XY\\) est dans \\(L^1(\\Omega, \\mathcal A, P)\\) et on a : \\[||XY||_{L^1}=\\E[\\ |XY|\\ ]\\leq ||X||_{L^2}||Y||_{L^2}.\\]\n\n\n\nDefinition 3.10 Un processus \\((X_t)_{t\\in T}\\) est dit du second ordre si la v.a. \\(X_t\\) est dans \\(L^2(\\Omega, \\mathcal A, P)\\) pour tout \\(t\\) dans \\(T\\).\n\n\n\n3.2.2 Convergence dans \\(L^2\\)\n\nDefinition 3.11 Soient \\((X_n)_{n\\in \\N}\\) une suite de v.a. dans \\(L^2(\\Omega, \\mathcal A, P)\\) et \\(X\\) une v.a. dans \\(L^2(\\Omega, \\mathcal A, P)\\). On dit que \\((X_n)_{n\\in \\N}\\) converge dans \\(L^2\\) vers \\(X\\) si l’on a : \\[\n\\lim_{n\\to +\\infty}||X_n-X||_{L^2}=0.\n\\] Notation : \\(X_n\\underset{n\\to +\\infty}{\\overset{L^2}{\\longrightarrow}}X\\)\n\n\nLa proposition suivante nous sera très utile dans la suite pour autoriser l’inversion entre le signe somme et l’espérance.\n\nProposition 3.1 Si \\(\\underset{i=0}{\\stackrel{n}{\\sum}}X_i\\) converge dans \\(L^2\\) vers \\(\\underset{i=0}{\\stackrel{+\\infty}{\\sum}}X_i\\) alors on a \\[\n\\E \\left[ \\underset{i=0}{\\stackrel{+\\infty}{\\sum}}X_i \\right]=\\underset{i=0}{\\stackrel{+\\infty}{\\sum}} \\E[X_i].\n\\]\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nNotons \\(U_n=\\sum_{i=0}^nX_i\\) et \\(U=\\sum_{i=0}^{+\\infty}X_i\\). L’inégalité classique entre les normes \\(L^1\\) et \\(L^2\\) (obtenue par l’inégalité de Jensen par exemple) nous donne les inégalités : \\[\n|\\E [U_n]-\\E[U]|\\leq \\E[ |U_n-U|]= || U_n-U ||_{L^1}\\leq || U_n-U ||_{L^2}.\n\\] L’hypothèse que \\((U_n)\\) converge dans \\(L^2\\) vers \\(U\\) nous assure donc que \\(\\E[U_n]=\\sum_{i=0}^n\\E[X_i]\\) converge vers \\(\\E[U]=\\E[\\sum_{i=0}^{+\\infty}X_i]\\), dont découle l’égalité annoncée.\n\n\n\nLa Proposition 3.1 reste valable pour une série indexée dans les deux directions :\nsi \\(\\underset{i=-m}{\\stackrel{n}{\\sum}}X_i\\) converge dans \\(L^2\\) vers \\(\\underset{i=-\\infty}{\\stackrel{+\\infty}{\\sum}}X_i\\) quand \\(n\\) et \\(m\\) tendent vers \\(+\\infty\\), on a \\[\n\\E \\left[ \\sum_{i=-\\infty}^{+\\infty}X_i \\right]=\\sum_{i=-\\infty}^{+\\infty} \\E[X_i].\n\\]\n\nProposition 3.2 Si les séries \\(\\underset{i=0}{\\stackrel{n}{\\sum}}X_i\\) et \\(\\underset{j=0}{\\stackrel{n}{\\sum}}Y_j\\) convergent dans \\(L^2\\) vers \\(\\underset{i=0}{\\stackrel{+\\infty}{\\sum}}X_i\\) et \\(\\underset{j=0}{\\stackrel{+\\infty}{\\sum}}Y_j\\) respectivement alors on a \\[\n\\E \\left[ \\sum_{i=0}^{+\\infty}X_i \\cdot \\sum_{j=0}^{+\\infty}Y_j  \\right]=\\sum_{i=0}^{+\\infty}\\sum_{j=0}^{+\\infty} \\E[X_iY_j].\n\\]\n\n\nCorollary 3.1 Si les séries \\(\\underset{i=0}{\\stackrel{n}{\\sum}}X_i\\) et \\(\\underset{j=0}{\\stackrel{n}{\\sum}}Y_j\\) convergent dans \\(L^2\\) vers \\(\\underset{i=0}{\\stackrel{+\\infty}{\\sum}}X_i\\) et \\(\\underset{j=0}{\\stackrel{+\\infty}{\\sum}}Y_j\\) respectivement alors on a \\[\n\\C \\left( \\sum_{i=0}^{+\\infty}X_i , \\sum_{j=0}^{+\\infty}Y_j  \\right)=\\sum_{i=0}^{+\\infty}\\sum_{j=0}^{+\\infty} \\C (X_i,Y_j).\n\\]\n\n\n\n3.2.3 Projection orthogonale\nLes projections dans l’espace \\(L^2\\) s’appuient sur la propriété d’Hilbert de cet espace. Rappelons d’abord la définition d’un sous-espace vectoriel fermé d’un espace de Hilbert dans notre cadre.\n\nDefinition 3.12 Un sous-espace vectoriel \\(\\mathcal H\\) de l’espace \\(L^2 (\\Omega,\\mathcal A,P)\\) est dit fermé s’il contient toutes ses limites \\[\n\\left(X_n\\in \\mathcal H, \\forall n\\in \\N, \\text{ et } X_n\\underset{n\\to +\\infty}{\\overset{L^2}{\\to}}X \\right)\\implies X\\in \\mathcal H.\n\\]\n\n\n\nTheorem 3.2 (Théorème de projection) Soit \\(X\\) une v.a. dans \\(L^2(\\Omega, \\mathcal A, P)\\) et \\(\\mathcal H\\) un sous-espace vectoriel fermé de \\(L^2(\\Omega, \\mathcal A, P)\\). Il existe alors une unique v.a. \\(\\hat X\\) dans \\(\\mathcal H\\) telle que : \\[\n||X-\\hat X||_{L^2}=\\min_{Y\\in \\mathcal H}||X-Y||_{L^2}.\n\\]\nLa v.a. \\(\\hat X\\) correspond à la projection orthogonale de \\(X\\) sur l’espace \\(\\mathcal H\\) et est donc telle que : \\[\n\\hat X \\in \\mathcal H \\text{ et }X-\\hat X\\perp \\mathcal H.\n\\]"
  },
  {
    "objectID": "chap2.html#processus-stationnaires",
    "href": "chap2.html#processus-stationnaires",
    "title": "3  Modélisation aléatoire des séries temporelles",
    "section": "3.3 Processus stationnaires",
    "text": "3.3 Processus stationnaires\nOn a vu dans le premier chapitre qu’une série temporelle peut souvent être décomposée sous la forme \\[\nY_t=m_t+s_t+X_t,\n\\] où\n\n\\(m_t\\) est une fonction à variation lente appelée tendance\n\\(s_t\\) une fonction périodique (de somme nulle) appelée saisonnalité.\n\nLe terme restant, le processus \\(X_t\\), est donc supposé être “plus stable” dans un sens que l’on va ici définir. C’est la notion de stationnarité que l’on va expliquer dans cette section.\n\nDefinition 3.13 Un processus \\((X_t)_{t\\in \\mathbb Z}\\) est dit stationnaire au sens fort si la loi de tout vecteur \\((X_{t_1},\\ldots,X_{t_n})\\) est invariante par translation temporelle : \\[\n\\mathcal L (X_{t_1},\\ldots,X_{t_n})=\\mathcal L (X_{t_1+h},\\ldots,X_{t_n+h}),\\ \\forall (t_1,\\ldots,t_n)\\in \\mathbb Z^n \\textrm{ et } h\\in \\mathbb Z.\n\\]\n\nPar conséquent, sous la stationnarité forte, toutes les v.a.r \\(X_t\\) ont la même loi. On a immédiatement qu’un bruit blanc fort est stationnaire au sens fort.\nCette hypothèse de stationnarité forte est très contraignante et peu réaliste en pratique. Aussi on va lui préférer la notion de stationnarité faible. Pour introduire la notion de stationnarité faible pour les processus du second ordre, nous avons besoin de définir tout d’abord la fonction moyenne et la fonction covariance des processus du second ordre.\n\nDefinition 3.14 Soit \\((X_t)_{t\\in \\mathbb Z}\\) un processus du second ordre.\nLa fonction moyenne de \\((X_t)_{t\\in \\mathbb Z}\\) est définie par \\[\nt\\in \\Z \\mapsto \\mathbb E[X_t].\n\\]\nLa fonction covariance de \\((X_t)_{t\\in \\mathbb Z}\\) est définie par : \\[\\begin{eqnarray*}\n(s,t)\\in \\mathbb Z^2 \\mapsto \\C(X_s,X_t)\n&=&\\mathbb E\\left[(X_s-\\E[X_s])(X_t-\\E[X_t]) \\right]\\\\\n&=&\\langle X_s-\\E[X_s],X_t-\\E[X_t]\\rangle_{L^2}\\\\\n&=&\\mathbb E[X_sX_t]-\\mathbb E[X_s]\\mathbb E[X_t]\n\\end{eqnarray*}\\]\n\n\n\nDefinition 3.15 Un processus du second ordre \\((X_t)_{t\\in \\mathbb Z}\\) est dit faiblement stationnaire si\n\nsa fonction moyenne est constante \\(\\E[X_t]=\\mu_X,\\ \\forall t\\in \\mathbb Z\\);\nsa fonction covariance ne dépend que de la différence en temps \\[\\begin{eqnarray*}\n\\C(X_s,X_t)\n&=&\\C(X_{s+u},X_{t+u}), \\forall (s,t,u)\\in \\Z^3\\\\\n&=&\\C(X_0,X_{t-s})=f(t-s)\n\\end{eqnarray*}\\]\n\n\n\n\n\n\n\n\nRemarques\n\n\n\n\nUn processus fortement stationnaire est stationnaire au sens faible. Mais la réciproque est fausse !\nSi le processus est faiblement stationnaire alors \\(\\V (X_t)=\\V (X_0),\\ \\forall t\\in \\mathbb Z.\\)\n\n\n\n\nExample 3.1 (Bruit blanc fort / faible) Si le processus \\((X_t)_{t\\in \\mathbb Z}\\) est un bruit blanc fort dans \\(L^2\\) alors il est faiblement stationnaire puisque : \\[\n\\mu_X(t)=0,\\ \\forall t\\in\\mathbb Z\n\\textrm{  et }\n\\C(X_{t+h},X_t)=\\sigma^2 \\mathbb{1}_{h=0} = \\left\\{\n\\begin{array}{ccc}\n\\sigma^2&\\textrm{ si }&h=0\\\\\n0&\\textrm{ si }&h\\ne 0\n\\end{array}\n\\right.,\\ \\forall t\\in\\mathbb Z\n\\]\nUn bruit blanc faible \\(\\text{WN}(0,\\sigma^2)\\) est également faiblement stationnaire.\n\n\nExample 3.2 (Marche aléatoire non stationnaire) Soit \\((S_t)_{t\\in \\mathbb N}\\) la marche aléatoire \\(S_t = X_1+\\ldots+X_t\\) avec \\((X_t)_{t\\in\\mathbb Z}\\sim \\text{IID}(0,\\sigma^2)\\) . On a \\[\\begin{eqnarray*}\n& &\\mathbb E[S_t]=\\mathbb E\\left[ \\sum_{k=1}^t X_k\\right]=\\sum_{k=1}^t\\mathbb E[X_k]=0\\\\\n& &\\V(S_t)=\\V\\left(\\sum_{k=1}^t X_k\\right)=\\sum_{k=1}^t \\V(X_k)=t\\sigma^2.\n\\end{eqnarray*}\\] \\(\\Longrightarrow\\) le processus \\((S_t)_{t\\in \\mathbb N}\\) n’est pas stationnaire puisque sa variance n’est pas constante.\nOn peut le vérifier plus largement sur la fonction covariance : pour \\(h\\in\\N\\), \\[\\begin{eqnarray*}\n\\C(S_{t+h},S_t)\n&=&\\C(S_t+X_{t+1}+\\cdots +X_{t+h},S_t)\\\\\n&=&\\V(S_t)+\\sum_{k=1}^h \\C(X_{t+k},S_t)=\\V(S_t)=t\\sigma^2.\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "chap2.html#fonctions-dautocovariance-et-dautocorrélation",
    "href": "chap2.html#fonctions-dautocovariance-et-dautocorrélation",
    "title": "3  Modélisation aléatoire des séries temporelles",
    "section": "3.4 Fonctions d’autocovariance et d’autocorrélation",
    "text": "3.4 Fonctions d’autocovariance et d’autocorrélation\n\n3.4.1 Fonction d’autocovariance (ACVF)\n\nDefinition 3.16 Soit \\((X_t)_{t\\in\\Z}\\) un processus faiblement stationnaire.\nSa fonction d’autocovariance \\(\\gamma_X(\\cdot)\\) est la fonction définie par \\[\\begin{eqnarray*}\n\\begin{array}{rcl}\n\\gamma_X:\\mathbb Z&\\rightarrow & \\mathbb R\\\\\nh&\\mapsto & \\gamma_X(h)=\\C(X_h,X_0)=\\C(X_{t+h},X_t),\\ \\forall t.\n\\end{array}\n\\end{eqnarray*}\\]\nLe tracé de la fonction \\(h\\in\\mathbb N \\mapsto \\gamma_X(h)\\) est appelé autocovariogramme.\n\n\n\nProposition 3.3 La fonction d’autocovariance \\(\\gamma_X(.)\\) vérifie les propriétés suivantes :\n\n\\(\\gamma_X(0)\\geq 0\\)\n\\(|\\gamma_X(h)|\\leq \\gamma_X (0)\\) pour tout \\(h\\) donc l’ACVF est bornée\n\\(\\gamma_X(h)=\\gamma_X(-h)\\) pour tout \\(h\\) donc l’ACVF est une fonction paire\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\n\nOn a \\(\\gamma_X(0)=\\V(X_t)\\geq 0\\).\nComme une corrélation est toujours comprise dans l’intervalle \\([-1,1]\\), on a donc \\(|\\gamma_X(h)|\\leq \\gamma_X(0)\\).\nOn peut écrire \\(\\gamma_X(h)=\\C (X_{t+h}, X_t)=\\C ( X_t,X_{t+h})=\\gamma_X(-h)\\).\n\n\n\n\n\n\n3.4.2 Fonction d’autocorrélation (ACF)\n\nDefinition 3.17 Soit \\((X_t)_{t\\in\\Z}\\) un proecssus faiblement stationnaire.\nSa fonction d’autocorrélation \\(\\rho_X(\\cdot)\\) est définie par \\[\\begin{eqnarray*}\n\\begin{array}{rcl}\n\\rho_X:\\mathbb Z&\\rightarrow & [-1,1]\\\\\nh&\\mapsto & \\rho_X(h)=\\displaystyle \\frac{\\gamma_X(h)}{\\gamma_X(0)}=\\mbox{Corr}(X_{t+h},X_t)\\\\\n& & \\\\\n& & =\\displaystyle\\frac{\\C(X_{t+h},X_t)}{\\sqrt{\\V(X_t) \\V(X_{t+h})}},\\ \\forall t.\n\\end{array}\n\\end{eqnarray*}\\]\nLe tracé de la fonction \\(h\\in\\mathbb N\\mapsto \\rho_X(h)\\) est appelé autocorrélogramme.\n\n\nAinsi les fonctions ACVF et ACF mesurent le degré de dépendance entre les valeurs d’une série temporelle à des instants différents. Ce sont des notions très importantes pour la suite du cours.\n\n\n3.4.3 Exemples\n\nExample 3.3 (Bruit blanc gaussien)  On considère la série \\((\\varepsilon_t)_{t\\in T}\\) avec les \\(\\varepsilon_t\\) i.i.d \\(\\mathcal N(0,1)\\). La trajectoire observée de cette série temporelle est représentée en Figure 3.5 (a). En traçant le nuage de points de coordonnées \\((X_t,X_{t+1})\\) (voir Figure 3.5 (b)), on constate qu’il n’y a pas de dépendance entre deux temps successifs. Sur l’autocorrélogramme empirique (Figure 3.5 (c)), on a un pic à 1 pour \\(h=0\\) et des valeurs proches de 0 pour \\(h\\geq 1\\).\n\n\n\n\n\n\n\n(a) Trajectoire de la série temporelle étudiée\n\n\n\n\n\n\n\n(b) Nuage de points \\((X_t, X_{t+1})\\)\n\n\n\n\n\n\n\n\n\n(c) Tracé de l’autocorrélogramme empirique\n\n\n\n\nFigure 3.5: Exemple du bruit blanc gaussien\n\n\n\n\n\nExample 3.4 (Moyenne mobile d’ordre 1 MA(1))  Soit \\((X_t)_{t\\in \\Z}\\) une série temporelle MA(1) définie par \\[\nX_t = \\varepsilon_t + \\theta\\ \\varepsilon_{t-1}, \\forall t\\in\\mathbb Z \\textrm{ avec } (\\varepsilon_t)_t\\sim \\text{WN}(0,\\sigma^2).\n\\]\nC’est un processus stationnaire au sens faible car\n\n\\(\\E[X_t]=\\E[\\varepsilon_t] + \\theta\\ \\E[\\varepsilon_{t-1}]=0\\)\nLa fonction d’autocovariance vaut\n\n\\[\\begin{eqnarray*}\n\\gamma_X(h)=\\C(X_t,X_{t+h})\n&=&\\mathbb E\\left[(\\varepsilon_t+\\theta \\varepsilon_{t-1})(\\varepsilon_{t+h}+\\theta \\varepsilon_{t+h-1})\\right]\\\\\n&=& \\E[\\varepsilon_t\\ \\varepsilon_{t+h}] +\n\\E[\\varepsilon_t\\ \\theta \\varepsilon_{t+h-1}]\\\\\n& & +\n\\E[\\theta \\varepsilon_{t-1} \\varepsilon_{t+h}]+\n\\E[\\theta \\varepsilon_{t-1}\\ \\theta \\varepsilon_{t+h-1}]\\\\\n&=& \\sigma^2 (\\mathbb{1}_{h=0} + \\theta \\mathbb{1}_{h=1} + \\theta\\mathbb{1}_{h=-1} + \\theta^2\\mathbb{1}_{h=0})\\\\\n&=&\\left\\{\n\\begin{array}{lcc}\n\\sigma^2(1+\\theta^2)& \\textrm{ si } &h=0\\\\\n\\theta \\sigma^2 & \\textrm{ si } & |h|=1\\\\\n0 & \\textrm{ si } &|h|\\geq 2\n\\end{array}\n\\right.\n\\end{eqnarray*}\\]\nOn en déduit l’expression de la fonction d’autocorrélation :\n\\[\n\\rho_X(h)=\\frac{\\gamma_X(h)}{\\gamma_X(0)}\n=\\left\\{\n\\begin{array}{lcc}\n1& \\textrm{ si } &h=0\\\\\n\\frac{\\theta}{1+ \\theta^2} & \\textrm{ si } & |h|=1\\\\\n0 & \\textrm{ si } &|h|\\geq 2\n\\end{array}\n\\right.\n\\]\nPour illustrer, on observe la trajectoire d’une série temporelle issue du modèle suivant \\[\nX_t = \\varepsilon_t -0.7 \\varepsilon_{t-1}, \\forall t\\in\\mathbb Z \\textrm{ avec } (\\varepsilon_t)_t\\sim \\text{WN}(0,0.5^2)\n\\]\nLa trajectoire observée de cette série temporelle est représentée en Figure 3.6 (a). On constate une dépendance linéaire quand on trace le nuage de points de coordonnées \\((X_t,X_{t+1})\\) (voir Figure 3.6 (b)) et aucune pour le nuage de points de coordonnées \\((X_t,X_{t+2})\\) (voir Figure 3.6 (c)). Sur l’autocorrélogramme empirique (Figure 3.6 (d)), on a un pic à 1 pour \\(h=0\\), un pic proche de \\(\\frac{-0.7}{1 + (-0.7)^2}\\approx -0.47\\) pour \\(h=1\\), et des valeurs proches de 0 pour \\(h\\geq 2\\).\n\n\n\n\n\n\n\n(a) Trajectoire de la série temporelle étudiée\n\n\n\n\n\n\n\n(b) Nuage de points \\((X_t, X_{t+1})\\)\n\n\n\n\n\n\n\n\n\n(c) Nuage de points \\((X_t, X_{t+2})\\)\n\n\n\n\n\n\n\n(d) Tracé de l’autocorrélogramme empirique\n\n\n\n\nFigure 3.6: Résultats pour la série MA(1) \\(X_t = \\varepsilon_t -0.7 \\varepsilon_{t-1}\\)\n\n\nComme second exemple, on observe la trajectoire d’une série temporelle issue du modèle suivant \\[\nX_t = \\varepsilon_t +\\varepsilon_{t-1}, \\forall t\\in\\mathbb Z \\textrm{ avec } (\\varepsilon_t)_t\\sim \\text{WN}(0,0.5^2)\n\\] La trajectoire observée de cette série temporelle est représentée en Figure 3.7 (a). On constate une dépendance linéaire avec pente positive quand on trace le nuage de points de coordonnées \\((X_t,X_{t+1})\\) (voir Figure 3.7 (b)) et aucune pour le nuage de points de coordonnées \\((X_t,X_{t+2})\\) (voir Figure 3.7 (c)). Sur l’autocorrélogramme empirique (Figure 3.7 (d)), on a un pic à 1 pour \\(h=0\\), un pic proche de \\(\\frac{1}{1 + (1)^2}=\\frac 1 2\\) pour \\(h=1\\), et des valeurs proches de 0 pour \\(h\\geq 2\\).\n\n\n\n\n\n\n\n(a) Trajectoire de la série temporelle étudiée\n\n\n\n\n\n\n\n(b) Nuage de points \\((X_t, X_{t+1})\\)\n\n\n\n\n\n\n\n\n\n(c) Nuage de points \\((X_t, X_{t+2})\\)\n\n\n\n\n\n\n\n(d) Tracé de l’autocorrélogramme empirique\n\n\n\n\nFigure 3.7: Résultats pour la série MA(1) \\(X_t = \\varepsilon_t + \\varepsilon_{t-1}\\)\n\n\n\n\n\nExample 3.5 (Processus autorégressif d’ordre 1 AR(1)) \nSoit \\((X_t)_{t\\in\\mathbb Z}\\) le processus stationnaire défini par \\[\nX_t=\\phi X_{t-1}+\\varepsilon_t,\\ \\forall t\\in \\mathbb Z\n    \\textrm{ avec }(\\varepsilon_t)\\sim \\text{WN}(0,\\sigma^2) \\textrm{ et } 0&lt;|\\phi|&lt;1\n\\]\nOn suppose que le processus \\((\\varepsilon_t)_{t\\in\\mathbb Z}\\) est tel que, pour tout \\(t\\), \\[\\begin{eqnarray*}\n\\C(\\varepsilon_t,X_s)=0, \\ \\forall s&lt;t\n&\\iff& \\langle \\varepsilon_t,X_s\\rangle_{L^2}=0, \\ \\forall s&lt;t\\\\\n&\\iff& \\varepsilon_t \\bot \\mathcal{H}_{t-1}:=sp\\{X_{t-1},X_{t-2},\\ldots\\}.\n\\end{eqnarray*}\\]\nLe processus \\(X_t\\) est centré car : \\(\\E[X_t] = \\phi \\E[X_{t-1}] + \\E[\\varepsilon_t] = \\phi \\mathbb E [X_{t-1}] + 0\\). Or le processus \\((X_t)_{t\\in\\Z}\\) est stationnaire donc \\(\\E[X_t]=\\E[X_{t-1}]\\). On obtient donc que \\(\\E[X_t] = \\phi \\mathbb E [X_{t}]\\) d’où \\(\\mathbb E[X_t] =0\\) car \\(\\phi\\neq 0\\).\nDéterminons maintenant la fonction d’autocovariance / autocorrélation : soit \\(h\\in\\mathbb N\\), \\[\\begin{eqnarray*}\n\\gamma_X(h)=\\C(X_t,X_{t+h})\n&=&\\mathbb E[X_tX_{t+h}]\\\\\n&=&\\mathbb E[\\left(\\phi X_{t+h-1}+\\varepsilon_{t+h}\\right)X_t]\\\\\n&=&\\phi \\C(X_{t+h-1},X_t)+ \\C(\\varepsilon_{t+h},X_t).\n\\end{eqnarray*}\\]\nOr par hypothèse, \\(\\varepsilon_t \\bot \\mathcal{H}_{t-1}\\) donc \\[\n\\gamma_X(h)=\\phi\\gamma_X(h-1)=\\cdots=\\phi^h\\gamma_X(0).\n\\]\nD’où \\(\\rho_X(h)=\\frac{\\gamma_X(h)}{\\gamma_X(0)}=\\phi^h,\\ \\forall h&gt;0\\). Finalement, comme \\(\\gamma_X(h)= \\gamma_X(-h)\\), \\(\\rho_X(h)= \\phi^{|h|},\\ \\forall h\\in \\mathbb Z.\\)\nPour illustrer ce résultat, on observe une série temporelle simulée selon \\[\nX_t = 0.8 X_{t-1} +\\varepsilon_{t}, \\forall t\\in\\mathbb Z \\textrm{ avec } (\\varepsilon_t)_t\\sim \\text{WN}(0,0.5^2)\n\\]\nLa trajectoire observée de cette série temporelle est représentée en Figure 3.8 (a). On constate des dépendances linéaires quand on trace le nuage de points de coordonnées \\((X_t,X_{t+1})\\) (voir Figure 3.8 (b)) et le nuage de points de coordonnées \\((X_t,X_{t+2})\\) (voir Figure 3.8 (c)). Sur l’autocorrélogramme empirique (Figure 3.8 (d)), on constate la décroissance exponentielle vers 0 (\\(\\rho_X(h) = e^{|h| \\ln(0.8)}\\approx e^{-0.22\\ |h|}\\)).\n\n\n\n\n\n\n\n(a) Trajectoire de la série temporelle étudiée\n\n\n\n\n\n\n\n(b) Nuage de points \\((X_t, X_{t+1})\\)\n\n\n\n\n\n\n\n\n\n(c) Nuage de points \\((X_t, X_{t+2})\\)\n\n\n\n\n\n\n\n(d) Tracé de l’autocorrélogramme empirique\n\n\n\n\nFigure 3.8: Résultats pour la série AR(1) \\(X_t = 0.8 X_{t-1} +\\varepsilon_{t}\\)\n\n\nPour la seconde illustration, on considère la série temporelle \\[\nX_t = -0.8 X_{t-1} +\\varepsilon_{t}, \\forall t\\in\\mathbb Z \\textrm{ avec } (\\varepsilon_t)_t\\sim \\text{WN}(0,0.5^2)\n\\] La trajectoire observée de cette série temporelle est représentée en Figure 3.9 (a). On constate des dépendances linéaires quand on trace le nuage de points de coordonnées \\((X_t,X_{t+1})\\) (voir Figure 3.9 (b)) et le nuage de points de coordonnées \\((X_t,X_{t+2})\\) (voir Figure 3.9 (c)), avec des pentes qui changent de signe car \\(\\phi&lt;0\\). Sur l’autocorrélogramme empirique (Figure 3.9 (d)), on constate la décroissance vers 0 avec alternance du signe (\\(\\rho_X(h) = (-1)^{|h|} e^{|h| \\ln(0.8)}\\approx (-1)^{|h|} e^{-0.22\\ |h|}\\)).\n\n\n\n\n\n\n\n(a) Trajectoire de la série temporelle étudiée\n\n\n\n\n\n\n\n(b) Nuage de points \\((X_t, X_{t+1})\\)\n\n\n\n\n\n\n\n\n\n(c) Nuage de points \\((X_t, X_{t+2})\\)\n\n\n\n\n\n\n\n(d) Tracé de l’autocorrélogramme empirique\n\n\n\n\nFigure 3.9: Résultats pour la série AR(1) \\(X_t = -0.8 X_{t-1} +\\varepsilon_{t}\\)\n\n\n\n\n\n3.4.4 CNS pour une fonction d’autocovariance\n\nDefinition 3.18 Une fonction \\(K: \\mathbb Z \\rightarrow \\mathbb R\\) est dite semi-définie positive si l’on a \\[\n\\sum_{i,j=1}^na_ia_jK(i-j)\\geq 0,\n\\] pour tout \\(n\\) et tout vecteur \\((a_1,\\ldots,a_n)\\in\\R^n\\).\n\n\n\nTheorem 3.3 Une fonction réelle définie sur \\(\\Z\\) est une fonction d’autocovariance d’une série temporelle si et seulement si elle est paire et semi-définie positive.\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\n\\(\\Rightarrow\\)) : Soit \\(\\gamma_X(.)\\) la fonction d’autocovariance de \\((X_t)_{t\\in\\Z}\\). On a déjà montré précédemment qu’elle est paire. Montrons qu’elle est semi-définie positive: soit \\((a_1,\\ldots,a_n)\\in\\R^n\\),\n\\[\\begin{eqnarray*}\n\\underset{i=1}{\\stackrel{n}{\\sum}}\\underset{j=1}{\\stackrel{n}{\\sum}} a_i a_j \\gamma_X(i-j) &=& \\underset{i=1}{\\stackrel{n}{\\sum}}\\underset{j=1}{\\stackrel{n}{\\sum}} a_i a_j \\C(X_{t-i},X_{t-j})\\\\\n&= & \\C\\left(\\underset{i=1}{\\stackrel{n}{\\sum}} a_i X_{t-i}, \\underset{j=1}{\\stackrel{n}{\\sum}}a_j X_{t-j}\\right)\\\\\n&=& \\V\\left(\\underset{i=1}{\\stackrel{n}{\\sum}} a_i X_{t-i}\\right) \\geq 0.\n\\end{eqnarray*}\\]\n \\(\\Leftarrow\\)) Soit \\(K:\\Z\\rightarrow\\R\\) paire et demi-définie positive. On veut montrer qu’il existe un processus stationnaire tel que \\(\\gamma_X(.)=K(.)\\). \n\\(\\forall n\\in\\N^\\star,\\ \\forall \\mathbf{t}=(t_1,\\ldots,t_n)\\in\\Z^n\\). Soit \\(F_{\\mathbf{t}}\\) la loi sur \\(\\R^n\\) de fonction caractéristique \\[\n\\Phi_{\\mathbf{t}}(\\mathbf{u}) = \\exp\\left[-\\frac 1 2 \\mathbf{u}' \\mathbb K \\mathbf{u}\\right]\n\\] où \\(\\mathbf{u}=(u_1,\\ldots,u_n)'\\in\\R^n\\) et la matrice \\(\\mathbb{K}=(K(t_i - t_j))_{i,j=1,\\ldots,n}\\).  Comme \\(K\\) est semi-définie positive, \\(\\mathbb{K}\\) est une matrice semi-définie positive et donc \\(\\Phi_{\\mathbf{t}}(.)\\) est la fonction caractéristique d’un vecteur gaussien \\(\\mathcal{N}(0,\\mathbb K)\\).\nOn peut alors vérifier que \\[\n\\underset{u_i\\rightarrow 0}{\\lim} \\Phi_{\\mathbf t}(\\mathbf u) = \\Phi_{\\mathbf{t}_{-i}}(\\mathbf{u}_{-i}) = \\exp\\left[-\\frac 1 2 \\mathbf{u}_{-i}' \\mathbb{K}_{\\mathbf{t}_{-i}} \\mathbf{u}_{-i}\\right]\n\\] où \\(\\mathbf{t}_{-i} = (t_1,\\ldots,t_{i-1},t_{i+1},\\ldots,t_n)'\\) et \\(\\mathbf{u}_{-i} = (u_1,\\ldots, u_{i-1}, u_{i+1},\\ldots,u_n)'\\). On conclut la preuve en utilisant le théorème d’existence de Kolmogorov :  \\(\\{F_{\\mathbf{t}}(.),\\ \\mathbf{t}\\in \\mathcal{T}\\}\\) sont les fonctions de distribution d’un processus stochastique si et seulement si pour tout \\(n\\), \\(\\mathbf{t}=(t_1,\\ldots,t_n)\\in\\mathcal{T}\\), \\(\\mathbf{x}\\in\\R^n\\) et \\(1\\leq k \\leq n\\), \\[\n\\underset{x_k\\rightarrow \\infty}{\\lim}\nF_{\\mathbf{t}}(\\mathbf{x}) = F_{\\mathbf{t}_{-k}}(\\mathbf{x}_{-k})\n\\] où \\(\\mathbf{t}_{-k} = (t_1,\\ldots,t_{k-1},t_{k+1},\\ldots,t_n)'\\) et \\(\\mathbf{x}_{-k} = (x_1,\\ldots, x_{k-1}, x_{k+1},\\ldots,x_n)'\\).\n\n\n\n\nRemarque : Pour la fonction d’autocorrélation, on a les mêmes propriétés + \\(\\rho_X(0)=1\\).\n\n\n3.4.5 Matrice d’autocorrélation\n\nDefinition 3.19 Soit \\((X_t)_{t\\in \\Z}\\) est un processus stationnaire du second ordre. On appelle matrice d’autocorrélation de \\((X_t,\\ldots,X_{t+h-1})\\) pour \\(h\\in\\N^*\\) \\[\nR_{X,h}=\\left(\n\\begin{array}{cccc}\n1 & \\rho_X(1)  & \\cdots & \\rho_X(h-1)  \\\\\n\\rho_X(1)  &1   & \\ddots &\\rho_X(h-2)  \\\\\n\\vdots  &  \\ddots &   \\ddots &\\vdots\\\\\n\\rho_X(h-1) &\\cdots&\\rho_X(1)& 1\n\\end{array}\n\\right).\n\\]\n\n\nCette matrice d’autocorrélation est une matrice de Toeplitz (matrice à diagonales constantes) et\n\\[\nR_{X,h}=\\left(\n\\begin{array}{ccc|c}\n  & &  & \\rho_X(h-1)  \\\\\n&R_{X,h-1}  &  &\\rho_X(h-2)  \\\\\n  &   &    &\\vdots\\\\\n    &   &    &\\rho_X(1)\\\\\n  \\hline\n\\rho_X(h-1) &\\cdots&\\rho_X(1)& 1\n\\end{array}\n\\right)\n\\]\nLa fonction d’autocorrélation vérifie la même propriété de semi-définie positivité que la fonction d’autocovariance. La proposition suivante donne en outre une condition équivalente en terme du déterminant des matrices \\(R_{X,h}\\).\n\nProposition 3.4 La fonction d’autocorrélation est également une fonction semi-définie positive. D’après le critère de Sylvester, cette propriété est équivalente à la positivité de tous les déterminants des mineurs principaux \\[\n\\mbox{det}(R_{X,h})\\geq 0,\\ \\textrm{ pour tout } h\n\\]\n\nCette propriété fixe une infinité de contraintes sur les corrélations\n\n\\(\\mbox{det}(R_{X,2})\\geq 0\\) donne \\(\\rho_X^2(1)\\leq 1\\)\n\\(\\mbox{det}(R_{X,3})\\geq 0\\) donne \\((1-\\rho_X(2))(1+\\rho_X(2)-2\\rho_X^2(1))\\geq 0\\), et donc \\(1+\\rho_X(2)-2\\rho_X^2(1)\\geq 0\\),\net ainsi de suite…\n\n\n\n3.4.6 Premiers pas vers les processus ARMA\nLes processus ARMA seront au coeur du Chapter 5. Nous les abordons ici très rapidement pour parler de filtrage linéaire.\nDans la modélisation ARMA, on utilise un opérateur de série en \\(B\\) de coefficients \\((\\psi_j)_{j\\in \\Z}\\) \\[\n\\psi(B)=\\sum_{j\\in \\Z}\\psi_j B^j\n\\]\nque l’on applique sur un processus stationnaire. On parle alors de filtrage linéaire. Si \\((X_t)_{t\\in \\Z}\\) est une série temporelle stationnaire alors \\[\nY_t=\\psi(B) X_t = \\sum_{j\\in\\mathbb Z}\\psi_j X_{t-j}\n\\]\nOn reviendra dans le Chapter 5 sur cette notion de série en \\(B\\) pour mieux la définir et établir des résultats. Nous allons ici nous intéresser à la propriété de maintien de la propriété de stationnairité par le filtrage linéaire sous une condition de sommabilité de ses coefficients.\n\nProposition 3.5 (Filtrage linéaire) Soit \\((X_t)_{t\\in \\Z}\\) une série temporelle stationnaire de fonction moyenne \\(\\mu_X\\) et de fonction d’autocovariance \\(\\gamma_X(\\cdot)\\).\nSi la suite de réels \\((\\psi_j)_{j\\in \\Z}\\) est sommable (\\(\\sum_{j=-\\infty}^{+\\infty}|\\psi_j|&lt;+\\infty\\)) alors la série \\((Y_t)_{t\\in \\Z}\\) obtenue par filtrage linéaire de \\((X_t)_{t\\in \\Z}\\), \\[\nY_t=\\psi(B)X_t=\\sum_{j=-\\infty}^{+\\infty}\\psi_j X_{t-j},\n\\] existe et est également stationnaire.\nLa moyenne est donnée par la relation \\[\n\\mu_Y = \\mu_X\\sum_{j=-\\infty}^{+\\infty}\\psi_j\n\\]\net la fonction d’autocovariance vaut \\[\n\\gamma_Y(h) =\\sum_{i=-\\infty}^{+\\infty}\\sum_{j=-\\infty}^{+\\infty}\\psi_i\\psi_j\\gamma_X(h+i-j),\\ \\forall h\\in\\Z\n\\]\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\n\nExistence de la série \\((Y_t)_{t\\in\\Z}\\) :\n\nOn a \\(\\underset{j\\in \\Z}{\\sum} \\|\\psi_j X_{t-j}\\|_{L^2} = \\underset{j\\in \\Z}{\\sum} |\\psi_j| \\|X_{t-j}\\|_{L^2}\\). Or \\(\\|X_{t-j}\\|_{L^2}^2=\\E[X_{t-j}^2]= \\V(X_{t-j}) + \\E[X_{t-j}]^2 = \\gamma_X(0)+\\mu_X^2.\\) Ainsi \\(\\underset{j\\in \\Z}{\\sum} \\|\\psi_j X_{t-j}\\|_{L^2} = \\sqrt{\\gamma_X(0)+\\mu_X^2}\\underset{j\\in \\Z}{\\sum} |\\psi_j| &lt;+\\infty\\). Donc la série \\((Y_t)_{t\\in\\Z}\\) est bien définie.\n\nSérie du second ordre :\n\n\\(\\|Y_t\\|_{L^2} = \\|\\underset{j\\in\\Z}{\\sum} \\psi_j X_{t-j}\\|_{L^2} \\leq \\underset{j\\in\\Z}{\\sum} \\|\\psi_j X_{t-j}\\|_{L^2} &lt; +\\infty\\)\n\nLa fonction moyenne de \\((Y_t)_{t\\in\\Z}\\) : \\(\\E[Y_t] = \\E\\left[ \\underset{j\\in\\Z}{\\sum} \\psi_j X_{t-j} \\right] = \\underset{j\\in\\Z}{\\sum} \\psi_j \\E\\left[X_{t-j} \\right] = \\mu_X \\left(\\underset{j\\in\\Z}{\\sum} \\psi_j \\right)\\) car \\(\\underset{j\\in\\Z}{\\sum}|\\psi_j|&lt;+\\infty\\). Donc la fonction moyenne est constante.\nLa fonction d’autocovariance de \\((Y_t)_{t\\in\\Z}\\) : \\[\\begin{eqnarray*}\n\\C(Y_t,Y_{t+h})\n&=& \\C\\left(\\underset{i\\in\\Z}{\\sum} \\psi_j X_{t-i}, \\underset{j\\in\\Z}{\\sum} \\psi_i X_{t+h-j} \\right)\\\\\n&=& \\underset{j\\in\\Z}{\\sum} \\underset{i\\in\\Z}{\\sum} \\psi_i\\psi_j \\C\\left(X_{t-i},X_{t+h-j}\\right)\\\\\n&=&\\underset{j\\in\\Z}{\\sum} \\underset{i\\in\\Z}{\\sum} \\psi_i\\psi_j \\gamma_X(h+i-j)\\\\\n&=& f(h)\n\\end{eqnarray*}\\] car \\(\\underset{j\\in\\Z}{\\sum}|\\psi_j|&lt;+\\infty\\) (échange \\(\\C(.,.)\\) et \\(\\sum\\), voir Corollary 3.1).\n\nAinsi \\((Y_t)_{t\\in\\Z}\\) est un processus faiblement stationnaire.\n\n\n\n\nExample 3.6 (Filtrage linéaire d’un bruit blanc)  Dans cet exemple, on considère que le processus initial est un bruit blanc \\[\n(\\varepsilon_t)_{t\\in\\mathbb Z}\\sim \\text{WN}(0,\\sigma^2)\n\\] qui est donc stationnaire.\nLe processus \\((Y_t)_{t\\in \\Z}\\) défini par \\[\nY_t=\\sum_{j\\in \\Z}\\psi_j\\ \\varepsilon_{t-j}\n\\] avec \\(\\sum_{j\\in \\Z}|\\psi_j | &lt;+\\infty\\) est bien défini et stationnaire. La fonction moyenne de \\((Y_t)_{t\\in\\Z}\\) est \\[\n\\mu_Y=\\mu_\\varepsilon \\sum_{j\\in \\Z}\\psi_j =0\n\\] et de fonction d’autocovariance \\[\\begin{eqnarray*}\n\\gamma_Y(h)&=&\\sum_{i\\in \\Z} \\sum_{j\\in \\Z} \\psi_i \\psi_j \\gamma_\\varepsilon(h+i-j)\\\\\n&=& \\sigma^2 \\sum_{i\\in \\Z} \\sum_{j\\in \\Z} \\psi_i \\psi_j \\mathbb{1}_{h+i-j=0}\\\\\n&=&\\sigma^2\\ \\sum_{i\\in \\Z} \\psi_i \\psi_{h+i}\n\\end{eqnarray*}\\] car \\(\\gamma_\\varepsilon(u)= \\sigma^2\\ \\mathbb{1}_{u = 0}\\).\nEt pour une somme finie ?\nSoit \\((\\varepsilon_t)_{t\\in \\Z}\\sim\\text{WN}(0,\\sigma^2)\\) et le processus \\((Y_t)_{t\\in \\Z}\\) défini par \\[\nY_t=\\sum_{i=0}^t\\varepsilon_{t-i}\n\\]\nLe processus \\((Y_t)_{t\\in \\Z}\\) existe et il est du second ordre.\nLe processus reste centré \\(\\E[Y_t]=\\underset{i=0}{\\stackrel{t}{\\sum}}\\E[\\varepsilon_{t-i}]=0.\\)\nFonction de covariance : soit \\(h\\in\\mathbb N\\), \\[\\begin{eqnarray*}\n\\C(Y_t,Y_{t+h})\n&=&\\E\\left[\\sum_{i=0}^t\\varepsilon_{t-i}\\times \\sum_{j=0}^{t+h}\\varepsilon_{t+h-j}\\right]\\\\\n&=&\\sum_{i=0}^{t} \\E[\\varepsilon_i^2]=\\sigma^2\\ (1+t).\n\\end{eqnarray*}\\]\n\\(\\Longrightarrow\\) Le processus n’est donc pas stationnaire! Ainsi, de manière paradoxale, les sommes finies peuvent poser plus de “problèmes” que les sommes infinies !"
  },
  {
    "objectID": "chap2.html#densité-spectrale",
    "href": "chap2.html#densité-spectrale",
    "title": "3  Modélisation aléatoire des séries temporelles",
    "section": "3.5 Densité spectrale",
    "text": "3.5 Densité spectrale\nJusqu’ici, on a abordé les processus stationnaires du second ordre via leur représentation temporelle. On peut également s’intéresser à leur représentation dans le domaine des fréquences. On aborde alors la notion de densité spectrale qui est une fonction contenant la même information que la fonction d’autocovariance.\n\nDefinition 3.20 Soit \\((X_t)_{t\\in \\Z}\\) un processus stationnaire de fonction d’autocovariance \\(\\gamma_X(\\cdot)\\). On appelle densité spectrale, quand elle existe, la fonction \\(f_X(\\cdot)\\) définie sur \\(\\R\\) par \\[\nf_X(\\omega)=\\frac{1}{2\\pi}\\sum_{h\\in \\Z}\\gamma_X(h)e^{-i\\omega h}.\n\\]\n\nOn peut reconnaitre que cette densité spectrale revient à la transformée de Fourier discrète de la fonction \\(\\gamma_X(\\cdot)\\) définie sur \\(\\Z\\).\nLa proposition suivante donne une propriété d’existence de la densité spectrale.\n\nProposition 3.6 (Existence de la densité spectrale)  La densité spectrale d’un processus stationnaire \\((X_t)_{t\\in \\Z}\\) de fonction d’autocovariance \\(\\gamma_X(\\cdot)\\) existe dès que l’on a : \\[\n\\sum_{h\\in \\Z}|\\gamma_X(h)|&lt;+\\infty.\n\\]\n\n\nProposition 3.7 La densité spectrale d’un processus stationnaire \\((X_t)_{t\\in \\Z}\\) est une fonction réelle, continue, positive, paire et \\(2\\pi\\)-périodique.\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nEn réécrivant la densité spectrale, on a \\[\\begin{eqnarray*}\nf_X(\\omega)\n&=& \\frac{1}{2\\pi} \\underset{h\\in\\Z}{\\sum} \\gamma_X(h) e^{-i\\omega h}\\\\\n&=& \\frac{\\gamma_X(0)}{2\\pi} + \\frac{1}{2\\pi} \\underset{h=-\\infty}{\\stackrel{-1}{\\sum}} \\gamma_X(h) e^{-i\\omega h} + \\frac{1}{2\\pi} \\underset{h=1}{\\stackrel{+\\infty}{\\sum}} \\gamma_X(h) e^{-i\\omega h}\\\\\n&=&\\frac{\\gamma_X(0)}{2\\pi} + \\frac{1}{2\\pi} \\underset{h=1}{\\stackrel{+\\infty}{\\sum}} \\gamma_X(h) \\left(e^{-i\\omega h}+ e^{i\\omega h}\\right)\\\\\n&=&\\frac{\\gamma_X(0)}{2\\pi} + \\frac{1}{\\pi} \\underset{h=1}{\\stackrel{+\\infty}{\\sum}} \\gamma_X(h) \\cos(\\omega h)\\\\\n\\end{eqnarray*}\\] donc \\(f_X(.)\\) est réelle, continue, paire et \\(2\\pi\\)-périodique.\nIl reste à montrer que \\(f_X(.)\\) est positive. Pour \\(N\\in\\N\\), on définit\n\\[\\begin{eqnarray*}\nf_N(\\omega)\n&=& = \\frac{1}{2\\pi N} \\E\\left[ \\left|\\underset{j=1}{\\stackrel{N}{\\sum}} (X_j - \\mu_X) e^{-i\\omega j}\\right|^2 \\right]\\\\\n&=& \\frac{1}{2\\pi N}\n\\E\\left[ \\underset{j=1}{\\stackrel{N}{\\sum}} (X_j - \\mu_X) e^{-i\\omega j} \\underset{k=1}{\\stackrel{N}{\\sum}} (X_k - \\mu_X) e^{i\\omega k} \\right]\\\\\n&=& \\frac{1}{2\\pi N}\n\\underset{j=1}{\\stackrel{N}{\\sum}}\\underset{k=1}{\\stackrel{N}{\\sum}} e^{-i\\omega (j-k)}\n\\E\\left[  (X_j - \\mu_X)(X_k - \\mu_X)\\right]\\\\\n&=& \\frac{1}{2\\pi N}\n\\underset{j=1}{\\stackrel{N}{\\sum}}\\underset{k=1}{\\stackrel{N}{\\sum}} e^{-i\\omega (j-k)}\n\\gamma_X(j-k)\\\\\n&=& \\frac{1}{2\\pi N}\n\\underset{h; |h|&lt;N}{\\sum}\\underset{j=|h|+1}{\\stackrel{N}{\\sum}} e^{-i\\omega h} \\gamma_X(h)\\\\\n&=& \\frac{1}{2\\pi N}\n\\underset{h; |h|&lt;N}{\\sum} (N-|h|) e^{-i\\omega h} \\gamma_X(h)\\\\\n&=& \\frac{1}{2\\pi}\n\\underset{h; |h|&lt;N}{\\sum} e^{-i\\omega h} \\gamma_X(h)\n- \\frac{1}{2\\pi}\n\\underset{h; |h|&lt;N}{\\sum} \\frac{|h|}{N} e^{-i\\omega h} \\gamma_X(h).\n\\end{eqnarray*}\\]\nComme \\(\\underset{h\\in\\Z}{\\sum}|\\gamma_X(h)|&lt;+\\infty\\), on a pour le premier terme que \\[\n\\frac{1}{2\\pi}\n\\underset{h; |h|&lt;N}{\\sum} e^{-i\\omega h} \\gamma_X(h) \\underset{N\\to +\\infty}{\\longrightarrow} \\frac{1}{2\\pi}\n\\underset{h\\in\\Z}{\\sum} e^{-i\\omega h} \\gamma_X(h) = f_X(\\omega).  \n\\]\nPour le second terme, on utilise le résultat suivant :\nSoit \\((a_h)_{h\\in\\Z}\\) une suite de réels positifs telle que \\(\\underset{h\\in\\Z}{\\sum} a_h &lt; +\\infty\\). On a alors \\[\n\\underset{N\\to +\\infty}{\\lim}  \\underset{h; |h|&lt;N}{\\sum} \\frac{|h|}{N} a_h =0.\n\\]\nIci, on a \\[\n0 \\leq \\left| \\frac{1}{2\\pi}\n\\underset{h; |h|&lt;N}{\\sum} \\frac{|h|}{N} e^{-i\\omega h} \\gamma_X(h)   \\right| \\leq \\frac{1}{2\\pi}\n\\underset{h; |h|&lt;N}{\\sum} \\frac{|h|}{N}  |\\gamma_X(h)|\n\\underset{N\\to +\\infty}{\\longrightarrow} 0.\n\\]\nAu final, on a que \\(f_N(\\omega)\\underset{N\\to +\\infty}{\\longrightarrow} f_X(\\omega),\\ \\forall \\omega\\in\\R\\) et \\(\\forall \\omega\\in\\R,\\ f_N(\\omega)\\geq 0\\) ce qui conclut à la positivité de \\(f_X(.)\\).\n\n\n\n\nTheorem 3.4 Il est équivalent de connaître la fonction d’autocovariance ou la densité spectrale, quand elle existe, d’un processus stationnaire \\((X_t)_{t\\in \\Z}\\) :\n\\[\n\\gamma_X(h)=\\int_{-\\pi}^\\pi f_X(\\omega) \\cos (\\omega h )d\\omega=\\int_{-\\pi}^\\pi f_X(\\omega) e^{i\\omega h }d\\omega.\n\\]\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nOn peut écrire : \\[\\begin{eqnarray*}\n\\int_{-\\pi}^\\pi f_X(\\omega) \\cos (\\omega h )d\\omega\n&=&\\int_{-\\pi}^\\pi f_X(\\omega) \\left( \\frac{e^{i\\omega h}+e^{-i\\omega h}}{2}\\right)d\\omega\\\\\n&=&\\frac{1}{2}\\int_{-\\pi}^\\pi f_X(\\omega)  e^{i\\omega h}d\\omega+\\frac{1}{2}\\int_{-\\pi}^\\pi f_X(\\omega)  e^{-i\\omega h}d\\omega\\\\\n&=&\\int_{-\\pi}^\\pi f_X(\\omega) e^{i\\omega h }d\\omega \\ \\ \\ \\text{par parité de la densité spectrale}\\\\\n&=& \\int_{-\\pi}^\\pi \\frac{1}{2\\pi}\\sum_{k\\in \\Z}\\gamma_X(k)e^{-i\\omega k} e^{i\\omega h }d\\omega\\\\\n&=&\\frac{1}{2\\pi}\\sum_{k\\in \\Z}\\gamma_X(k) \\int_{-\\pi}^\\pi  e^{i\\omega (h-k)}d\\omega\n\\end{eqnarray*}\\] par la convergence normale de la série. Or\n\\[\n\\int_{-\\pi}^\\pi  e^{i\\omega (h-k)}d\\omega =\n\\left\\{\n\\begin{array}{l l}\n2\\pi & \\textrm{ si } h=k\\\\\n\\\\\n\\left[\\frac{e^{i\\omega(h-k)}}{h-k} \\right]_{-\\pi}^\\pi=0 & \\text{ si } k\\neq h\n\\end{array}\n\\right.\n\\]\nAinsi, \\[\n\\int_{-\\pi}^\\pi f_X(\\omega) \\cos (\\omega h )d\\omega=\\gamma_X(h).\n\\]\n\n\n\n\nExample 3.7 (Bruit blanc faible) Soit un bruit blanc faible \\((\\varepsilon_t)_{t\\in \\Z}\\sim \\text{WN}(0,\\sigma^2)\\). Rappelons que sa fonction d’autocovariance vaut \\(\\gamma_\\varepsilon(h) = \\sigma^2 \\mathbb{1}_{h=0}\\). Sa densité spectrale vaut donc \\[\nf_\\varepsilon(\\omega)=\\frac{1}{2\\pi}\\sum_{h\\in \\Z}\\gamma_\\varepsilon(h)e^{-i\\omega h}=\\frac{\\sigma^2}{2\\pi},\\ \\forall \\omega\\in\\R.\n\\]\nEt réciproquement tout processus stationnaire de densité spectrale constante est un bruit blanc faible.\n\nLa proposition suivante concerne la densité spectrale d’un filtrage linéaire.\n\nProposition 3.8  Soit \\((X_t)_{t\\in \\Z}\\) un processus stationnaire de densité spectrale \\(f_X(\\cdot)\\). Soit \\((Y_t)_{t\\in \\Z}\\) un filtrage linéaire de la série \\((X_t)_{t\\in \\Z}\\) défini par \\[\nY_t=\\psi(B)X_t=\\sum_{j=-\\infty}^{+\\infty}\\psi_j X_{t-j}\n\\textrm{ avec }\n\\sum_{j=-\\infty}^{+\\infty}|\\psi_j|&lt;+\\infty.\n\\] Le processus \\((Y_t)_{t\\in \\Z}\\) est donc stationnaire de densité spectrale \\[\n\\forall \\omega\\in\\R,\\ f_Y(\\omega)=f_X(\\omega)\\left|\\sum_{j=-\\infty}^{+\\infty} \\psi_j e^{-i\\omega j}\\right|^2.\n\\]\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\n\nExistence de la densité spectrale de \\((Y_t)_{t\\in\\Z}\\):\n\nComme \\(\\sum_{j=-\\infty}^{+\\infty}|\\psi_j|&lt;+\\infty\\), d’après la Proposition 3.5, on a \\[\n\\gamma_Y(h) = \\sum_{j\\in\\Z}\\sum_{k\\in\\Z}\\psi_j\\psi_k\\gamma_X(h+j-k).\n\\]\nDonc \\[\\begin{eqnarray*}\n\\sum_{h\\in \\Z} \\left|\\gamma_Y(h)\\right|&=&\\sum_{h\\in \\Z} \\left| \\sum_{j\\in\\Z}\\sum_{k\\in\\Z}\\psi_j\\psi_k\\gamma_X(h+j-k) \\right|\\\\\n&\\leq&\\sum_{h\\in \\Z} \\sum_{j\\in\\Z}\\sum_{k\\in\\Z} |\\psi_j| |\\psi_k| \\left| \\gamma_X(h+j-k) \\right| \\\\\n&=&\\sum_{j\\in\\Z} |\\psi_j| \\cdot \\sum_{k\\in\\Z} |\\psi_k| \\cdot \\sum_{h\\in \\Z} \\left| \\gamma_X(h+j-k) \\right|,\n\\end{eqnarray*}\\] où \\(\\sum_{h\\in \\Z} \\left| \\gamma_X(h+j-k) \\right|&lt;+\\infty\\) par hypothèse d’existence de la densité spectrale du processus \\((X_t)_{t\\in \\Z}\\) et \\(\\sum_{j\\in\\Z} |\\psi_j|&lt;+\\infty\\) par hypothèse. Ainsi la densité spectrale du processus \\((Y_t)_{t\\in \\Z}\\) existe d’après la Proposition 3.6.\n\nCalcul de \\(f_Y(.)\\) : Maintenant, on peut écrire \\[\\begin{eqnarray*}\nf_Y(\\omega)\n&=&\\frac{1}{2\\pi}\\sum_{h\\in \\Z}\\gamma_Y(h)e^{-i\\omega h}\\\\\n&=&\\frac{1}{2\\pi}\\sum_{h\\in \\Z} \\sum_{j\\in\\Z}\\sum_{k\\in\\Z}\\psi_j\\psi_k\\gamma_X(h+j-k) e^{-i\\omega h}\\\\\n&=&\\frac{1}{2\\pi}\\sum_{j\\in\\Z} \\psi_j e^{i\\omega j}\\sum_{k\\in\\Z}\\psi_k e^{-i\\omega k}\\sum_{h\\in \\Z} \\gamma_X(h+j-k)e^{-i\\omega (h+j-k)}\\\\\n&=&\\frac{1}{2\\pi}\\left(\\sum_{j\\in\\Z}\\psi_j e^{i\\omega j}\\right)\\left(\\sum_{k\\in\\Z}\\psi_k e^{-i\\omega k} \\right)\\left(2\\pi f_X(\\omega) \\right)\\\\\n&=&f_X(\\omega) \\left|\\sum_{j\\in\\Z} \\psi_j e^{-i\\omega j}\\right|^2.\n\\end{eqnarray*}\\]\n\n\n\n\n\nExample 3.8 (Densité spectrale du filtrage linéaire d’un bruit blanc) \nSoit \\((\\varepsilon_t)_{t\\in \\Z}\\sim \\text{WN}(0,\\sigma^2)\\) et \\((Y_t)_{t\\in \\Z}\\) le filtrage linéaire de ce bruit blanc défini par \\[\n    Y_t=\\sum_{j\\in \\Z}\\psi_j \\varepsilon_{t-j}\n\\] avec \\(\\sum_{j\\in \\Z}|\\psi_j | &lt;+\\infty\\). D’après la Proposition 3.8, la densité spectrale de ce processus est alors égale à : \\[\nf_Y(\\omega)=f_\\varepsilon (\\omega) \\cdot \\left|\\sum_{j=-\\infty}^{+\\infty} \\psi_j e^{-i\\omega j}\\right|^2=\\frac{\\sigma^2}{2\\pi}\\left|\\sum_{j=-\\infty}^{+\\infty} \\psi_j e^{-i\\omega j}\\right|^2.\n\\]\n\n\n\n\n\nAragon, Yves. 2016. Séries Temporelles Avec r. EDP sciences.\n\n\nBrockwell, Peter J, and Richard A Davis. 2002. Introduction to Time Series and Forecasting. Springer.\n\n\n———. 2009. Time Series: Theory and Methods. Springer science & business media.\n\n\nDauxois, Jean-Yves. 2020. “Introduction à l’étude Des Séries Temporelles.” Polycopié cours INSA Toulouse."
  },
  {
    "objectID": "chap3.html#quelques-estimateurs",
    "href": "chap3.html#quelques-estimateurs",
    "title": "4  Statistique des processus stationnaires du second ordre",
    "section": "4.1 Quelques estimateurs",
    "text": "4.1 Quelques estimateurs\n\n4.1.1 Estimateur de la moyenne du processus stationnaire\nRappelons que la fonction moyenne \\(\\mu_X\\) est constante pour un processus stationnaire. Ainsi, on l’estime facilement par la moyenne empirique\n\\[\n\\hat \\mu_X=\\bar X_n=\\frac{1}{n} \\sum_{t=1}^n X_t.\n\\]\nCet estimateur est sans biais et \\(L^2\\)-consistant d’après le théorème suivant.\n\nTheorem 4.1 Si \\((X_t)_{t\\in \\Z}\\) est un processus stationnaire de moyenne \\(\\mu_X\\) et de fonction d’autocovariance \\(\\gamma_X(\\cdot)\\) alors :\n\nsi \\(\\gamma_X(h)\\underset{h\\to +\\infty}{\\longrightarrow} 0\\) alors \\(\\V (\\bar X_n)\\underset{n\\to+\\infty}{\\longrightarrow} 0\\)\nsi de plus \\(\\underset{h\\in \\Z}{\\sum}\\ |\\gamma_X(h)|&lt;+\\infty\\) alors \\[\nn \\V(\\bar X_n)\\underset{n\\to+\\infty}{\\longrightarrow} \\sum_{h\\in \\Z}\\gamma_X(h)= 2\\pi f_X(0)\n\\]\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nOn peut écrire \\[\\begin{eqnarray*}\nn \\V(\\bar X_n)\n&=& \\C\\left(\\frac{1}{n} \\sum_{i=1}^n X_i, \\frac{1}{n} \\sum_{j=1}^n X_j\\right)\\\\\n&=&\\frac{1}{n}\\sum_{i,j=1}^n \\C(X_i,X_j)\\\\\n&=&\\frac{1}{n}\\sum_{i,j=1}^n \\gamma_X(i-j)\\\\\n&=&\\frac{1}{n}\\sum_{h:|h|&lt;n} (n-|h|)\\gamma_X(h)\\\\\n&=&\\sum_{h:|h|&lt;n} \\left(1-\\frac{|h|}{n}\\right)\\gamma_X(h)\\\\\n&\\leq&\\sum_{h:|h|&lt;n} | \\gamma_X(h)|= |\\gamma_X(0)|+2\\sum_{h=1}^n |\\gamma_X(h)|.\n\\end{eqnarray*}\\]\nPar le théorème de Césaro, on sait que l’on a la convergence de la moyenne de Césaro \\[\n\\lim_{n \\to +\\infty}\\frac{1}{n}\\sum_{h=1}^n |\\gamma_X(h)|=\\lim_{h\\to +\\infty}|\\gamma_X(h)|,\n\\] dès lors que cette dernière limite existe. Donc comme par hypothèse \\(\\gamma_X(h)\\underset{h\\to +\\infty}{\\longrightarrow} 0\\), on obtient que \\(\\V (\\bar X_n)\\underset{n\\to+\\infty}{\\longrightarrow} 0\\).\nMaintenant, sous l’hypothèse\n\\[\n\\sum_{h\\in \\Z}|\\gamma_X(h)|&lt;+\\infty,\n\\] on peut appliquer le théorème de convergence dominée pour inverser limite et somme et obtenir \\[\\begin{eqnarray*}\n\\lim_{n \\to +\\infty}n \\V(\\bar X_n)\n&=&\\lim_{n \\to +\\infty}\\sum_{h:|h|&lt;n} \\left(1-\\frac{|h|}{n}\\right)\\gamma_X(h)\\\\\n&=&\\lim_{n \\to +\\infty}\\sum_{h\\in\\Z} \\left(1-\\frac{|h|}{n}\\right)\\gamma_X(h)\\mathbb{1}_{|h|&lt;n}\\\\\n&=&\\sum_{h\\in \\Z} \\gamma_X(h),\n\\end{eqnarray*}\\] car \\(g_n(h):=\\left(1-\\frac{|h|}{n}\\right)\\gamma_X(h)\\mathbb{1}_{|h|&lt;n}\\underset{n\\to +\\infty}{\\longrightarrow}\\gamma_X(h)\\).\nEnfin, par définition de la densité spectrale, \\(\\sum_{h\\in \\Z}\\gamma_X(h) = 2 \\pi f_X(0)\\).\n\n\n\nLe premier résultat prouve la convergence dans \\(L^2\\) de l’estimateur vers la moyenne \\(\\mu_X\\) du processus. Le second donne la variance asymptotique de l’estimateur normalisé.\nSous des hypothèses supplémentaires, on peut obtenir le comportement asymptotique gaussien de \\(\\bar X_n\\).\n\nProposition 4.1 Soit \\((X_t)_{t\\in \\Z}\\) un processus stationnaire défini, pour tout \\(t\\in \\Z\\), par \\[\nX_t=\\mu_X+\\sum_{j\\in \\Z}\\psi_j \\varepsilon_{t-j}\n\\] où \\((\\varepsilon_t)_{t\\in \\Z}\\) est un \\(\\text{IID}(0,\\sigma^2)\\) tel que \\(\\E[\\varepsilon_t^4]&lt;+\\infty\\) et où la suite des coefficients \\((\\psi_j)_{j\\in \\Z}\\) est sommable et de somme non nulle \\[\n\\sum_{j\\in \\Z}|\\psi_j | &lt;+\\infty \\text{ et }\\sum_{j\\in \\Z}\\psi_j \\neq 0.\n\\]\nOn a alors \\[\n\\sqrt n \\left( \\bar X_n -\\mu_X\\right)\\underset{n \\to +\\infty}{\\overset{\\mathcal L}{\\longrightarrow}} \\mathcal N\\left(0,\\sum_{h\\in \\Z}\\gamma_X(h)\\right).\n\\]\n\n\n\n4.1.2 Estimateur de la fonction d’auto-covariance / -corrélation\nOn rappelle que la fonction d’autocovariance est définie par \\[\n\\forall h\\in\\Z,\\ \\gamma_X(h) = \\C(X_t,X_{t+h}) = \\E\\left[(X_t - \\E[X_t])(X_{t+h} - \\E[X_{t+h}])\\right].\n\\] Donc à partir de \\(X_1,\\ldots,X_n\\), on peut estimer, pour les valeurs de \\(h\\) telles que \\(|h|&lt;n-1\\), l’autocovariance \\(\\gamma_X(h)\\) par \\[\n\\left\\{\\begin{array}{l l}\n    \\hat \\gamma_{X,n}(h)=\\displaystyle\\frac{1}{n}\\underset{t=1}{\\stackrel{n-h}{\\sum}}\\left(X_{t+h} -\\bar X_n \\right)\\left(X_{t} -\\bar X_n \\right) & \\textrm{ si } h\\geq0\\\\\n    \\\\\n    \\hat \\gamma_{X,n}(h)=\\hat \\gamma_{X,n}(-h) & \\textrm{ si } h&lt;0\n\\end{array}\\right.\n\\]\n\nProposition 4.2 L’estimateur \\(\\hat \\gamma_{X,n}(h)\\) est asymptotiquement sans biais et consistant pour \\(\\gamma_X(h)\\).\n\n\n\n\n\n\n\nRemarque\n\n\n\nEn pratique, on utilise cet estimateur pour \\(h\\leq \\frac n 4\\).\n\n\n\nA partir de l’estimateur de la fonction covariance, on en déduit l’estimateur suivant pour la fonction d’autocorrélation : Pour tout \\(|h|&lt;n-1\\), \\[\n\\hat \\rho_{X,n}(h)=\\frac{\\hat \\gamma_{X,n}(h)}{\\hat\\gamma_{X,n}(0)} = \\displaystyle\\frac{\\underset{t=1}{\\stackrel{n-|h|}{\\sum}}\\left(X_t -\\bar X_n \\right)\\left( X_{t+|h|} -\\bar X_n \\right)}{\\underset{t=1}{\\stackrel{n}{\\sum}} \\left(X_t-\\bar X_n\\right)^2}.\n\\]\n\nProposition 4.3 L’estimateur \\(\\hat \\rho_{X,n}(h)\\) est consistant.\nEn pratique on estime \\(\\rho_X(h)\\) pour \\(h \\leq \\frac n 4\\).\n\nOn a de plus le comportement asymptotique suivant :\n\nProposition 4.4 Soit \\((X_t)_{t\\in \\Z}\\) un processus stationnaire défini par \\(X_t=\\mu_X+\\underset{j\\in \\Z}{\\sum}\\psi_j \\varepsilon_{t-j}\\) où \\((\\varepsilon_t)_{t\\in \\Z}\\sim\\text{IID}(0,\\sigma^2)\\) tel que \\(\\E[\\varepsilon_t^4]&lt;+\\infty\\) et où la suite des coefficients \\((\\psi_j)_{j\\in \\Z}\\) est sommable (\\(\\underset{j\\in \\Z}{\\sum}|\\psi_j|&lt;+\\infty\\)) et de somme non nulle (\\(\\underset{j\\in \\Z}{\\sum}\\psi_j\\neq 0\\)).\nOn a alors, pour tout \\(k&gt;0\\) fixé, \\[\n\\sqrt n \\left(\n\\begin{array}{c}\n\\hat \\rho_{X,n}(1) - \\rho_X(1)   \\\\\n  \\vdots   \\\\\n\\hat \\rho_{X,n}(k) - \\rho_X(k)\n\\end{array}\n\\right)\\underset{n\\to +\\infty}{\\overset{\\mathcal L}{\\longrightarrow}}\\mathcal{N}\\left(0,\\Sigma^{[k]}\\right),\n\\] où \\(\\Sigma^{[k]}=\\left(\\Sigma_{ij}^{[k]}\\right)_{1\\leq i,j\\leq k}\\) est la matrice de covariance asymptotique déterminée par \\[\n\\Sigma_{ij}^{[k]}\n=\\sum_{h\\in \\Z}\\left\\{\\left[\\rho_X(h+i)+\\rho_X(h-i)-2\\rho_X(i)\\rho_X(h)\\right] \\times  \\left[ \\rho_X(h+j)+\\rho_X(h-j)-2\\rho_X(j)\\rho_X(h)\\right]\\right\\}\n\\] (formule de Bartlett).\n\n\n\n4.1.3 Estimateur de la matrice d’auto-covariance / -corrélation\nOn s’intéresse maintenant à l’estimation des matrices d’autocovariance et d’autocorrélation.\n\nDefinition 4.1 La matrice \\[\n\\hat \\Gamma_{X,n} := \\left(\n\\begin{array}{c c c c}\n\\hat \\gamma_{X,n}(0)& \\hat \\gamma_{X,n}(1) & \\ldots & \\hat \\gamma_{X,n}(n-1)\\\\\n\\hat \\gamma_{X,n}(1)& \\hat \\gamma_{X,n}(0) & \\ldots & \\hat \\gamma_{X,n}(n-2)\\\\\n\\vdots & \\vdots & \\vdots & \\vdots\\\\\n\\hat \\gamma_{X,n}(n-1)& \\hat \\gamma_{X,n}(n-2) & \\ldots & \\hat \\gamma_{X,n}(0)\n\\end{array}\n\\right)\n\\]\nest un estimateur de la matrice d’autocovariance.\nLa matrice \\(\\hat R_{X,n} = \\frac{\\hat\\Gamma_{X,n}}{\\hat \\gamma_{X,n}(0)}\\) est un estimateur de la matrice d’autocorrélation \\(R_{X,n}\\).\n\n\n\nProposition 4.5 Les matrices \\(\\hat \\Gamma_{X,n}\\) et \\(\\hat R_{X,n}\\) sont des matrices semi-définies positives.\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nOn commence par remarquer que l’on peut écrire \\(\\hat \\Gamma_{X,n}= \\frac 1 n T T'\\) avec \\[\nT=\\left(\n\\begin{array}{l l l l l l l l l l l l}\n0 & \\ldots & \\ldots & \\ldots & \\ldots & 0 & Y_1 & Y_2 & 0 & \\ldots & 0 & Y_n\\\\\n0 & \\ldots & \\ldots & \\ldots & 0 & Y_1 & Y_2 & 0 & \\ldots & \\ldots & Y_n & 0\\\\\n\\vdots & & & & & & \\vdots & & & & & \\vdots\\\\\n0 & Y_1 & Y_2 & 0 & \\ldots & 0 & Y_n & 0 & \\ldots & \\ldots & \\ldots & 0\n\\end{array}\n\\right)\\in\\mathcal{M}_{n\\times 2 n}(\\R)\n\\] et \\(Y_t = X_t - \\bar X_n\\), \\(t\\in\\{1,\\ldots,n\\}\\). Alors \\(\\forall a\\in\\R^n\\), \\(a' \\hat \\Gamma_{X,n} a = \\frac 1 n (a'T)(a'T)'\\geq 0\\).\n\n\n\n\n\n4.1.4 Estimateur de la densité spectrale\nSoit \\((X_t)_{t\\in\\Z}\\) un processus stationnaire de moyenne \\(\\mu_X\\) et de fonction d’autocovariance \\(\\gamma_X(.)\\) telle que \\(\\underset{h\\in\\Z}{\\sum}|\\gamma_X(h)|&lt;+\\infty\\)\nSous ces hypothèses, on a vu (voir Definition 3.20 et Proposition 3.6) que la densité spectrale existe et est définie par \\[\nf_X(\\omega)= \\frac{1}{2\\pi} \\underset{h\\in\\Z}{\\sum}\\gamma_X(h) e^{-ih \\omega},\\ \\forall \\omega\\in\\R\n\\]\nPour rappel, \\(f_X(.)\\) est une fonction paire, \\(2\\pi\\)-périodique, continue, positive.\nPour construire un estimateur de la densité spectrale, on commence par définir le périodogramme.\n\nDefinition 4.2 Le périodogramme associé à \\((X_1,\\ldots,X_n)\\) est défini par \\[\nI_n(\\omega_j) = \\frac 1 n \\left|\\underset{t=1}{\\stackrel{n}{\\sum}} X_t e^{-i t \\omega_j} \\right|^2,\\ \\forall\\omega_j\\in \\Omega_n:= \\left\\{\\omega_j=\\frac{2\\pi j}{n};\\ \\omega_j\\in]-\\pi,\\pi]\\right\\}\n\\]\n\nNous allons maintenant relier le périodogramme et l’estimateur \\(\\hat \\gamma_{X,n}\\).\n\nProposition 4.6 \\[\n\\left\\{\\begin{array}{l l }\nI_n(0) = n |\\bar X_n|^2 & \\\\\n\\\\\nI_n(\\omega_j) = \\underset{|h|&lt;n}{\\sum} \\hat \\gamma_{X,n}(h)e^{-ih\\omega_j} & \\textrm{ si }\\omega_j\\in\\Omega_n,\\ \\omega_j\\neq 0\n\\end{array}\\right.\n\\] \n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\n\n\nPour \\(\\omega_j=0\\) : \\(I_n(0)=\\frac 1 n \\left|\\underset{t=1}{\\stackrel{n}{\\sum}} X_t\\right|^2 = n |\\bar X_n|^2\\)\nSoit \\(\\omega_j\\neq 0\\). On a \\[\\begin{eqnarray*}\n  I_n(\\omega_j)\n  &=& \\frac 1 n \\left|\\underset{t=1}{\\stackrel{n}{\\sum}} X_t e^{-i t \\omega_j} \\right|^2 \\\\\n  &=& \\frac 1 n \\underset{s=1}{\\stackrel{n}{\\sum}}\\underset{t=1}{\\stackrel{n}{\\sum}} X_s X_t e^{-i t \\omega_j} e^{i s \\omega_j} \\\\\n  &=& \\frac 1 n \\underset{s=1}{\\stackrel{n}{\\sum}}\\underset{t=1}{\\stackrel{n}{\\sum}} (X_s-\\bar X_n) (X_t-\\bar X_n) e^{-i (t-s) \\omega_j}\\\\\n  &+& \\frac 1 n \\underset{s=1}{\\stackrel{n}{\\sum}}\\underset{t=1}{\\stackrel{n}{\\sum}} \\bar X_n X_t e^{-i (t-s) \\omega_j}\\\\\n  &+& \\frac 1 n \\underset{s=1}{\\stackrel{n}{\\sum}}\\underset{t=1}{\\stackrel{n}{\\sum}} X_s\\bar X_n e^{-i (t-s) \\omega_j}\\\\\n  &-& \\frac 1 n (\\bar X_n)^2 \\underset{s=1}{\\stackrel{n}{\\sum}}\\underset{t=1}{\\stackrel{n}{\\sum}} e^{-i (t-s) \\omega_j}.\\\\\n\\end{eqnarray*}\\]\n\nOr \\(\\underset{s=1}{\\stackrel{n}{\\sum}} e^{i s \\omega_j} = \\underset{t=1}{\\stackrel{n}{\\sum}} e^{- i t \\omega_j} =0\\) si \\(\\omega_j\\neq 0\\), donc les trois dernières sommes sont nulles. Ainsi\n\\[\\begin{eqnarray*}\nI_n(\\omega_j)\n&=& \\frac 1 n \\underset{s=1}{\\stackrel{n}{\\sum}}\\underset{t=1}{\\stackrel{n}{\\sum}} (X_s-\\bar X_n) (X_t-\\bar X_n) e^{-i (t-s) \\omega_j}\\\\\n&=& \\underset{|h|&lt;n}{\\sum} \\frac 1 n \\underset{s=1}{\\stackrel{n-|h|}{\\sum}} (X_s-\\bar X_n) (X_{s+h}-\\bar X_n) e^{-i h \\omega_j}\\\\\n&=& \\underset{|h|&lt;n}{\\sum} \\hat \\gamma_{X,n}(h) e^{-i h \\omega_j}\n\\end{eqnarray*}\\]\n\n\n\nOn peut alors proposer naturellement d’estimer\n\n\\(f_X(\\omega_j) = \\frac{1}{2\\pi} \\underset{h\\in\\Z}{\\sum} \\gamma_X(h) e^{-h\\omega_j}\\) par \\(\\hat f_X(\\omega_j): =\\frac{I_n(\\omega_j)}{2\\pi}\\) pour \\(\\omega_j\\neq 0\\).\n\n\\(f_X(0)=\\frac{1}{2\\pi} \\underset{h\\in\\Z}{\\sum} \\gamma_X(h)\\) par \\(\\hat f_X(0) = \\frac{1}{2\\pi} \\underset{|h|&lt;n}{\\sum} \\hat \\gamma_{X,n}(0)\\).\n\nOn cherche ensuite à étendre \\(\\hat f_X\\) à tout l’intervalle \\([-\\pi,\\pi]\\) pour estimer \\(f_X(.)\\) (qui est paire et \\(2\\pi\\)-périodique). On peut le faire en obtenant un estimateur \\(\\hat f_X(.)\\) contant par morceaux\n\n\\[\n\\hat f_X(\\omega)\n=\\left\\{\\begin{array}{l l}\n\\hat f_X(\\omega_j) & \\textrm{ si } \\omega_j-\\frac{\\pi}{n} &lt; \\omega \\leq \\omega_j+\\frac{\\pi}{n}, \\omega\\in[0,\\pi]\\\\\n\\\\\n\\hat f_X(-\\omega) & \\textrm{ si } \\omega\\in [-\\pi,0[\n\\end{array}\\right.\n\\]\n\n\n\nProposition 4.7 Si \\((X_t)_{t\\in\\Z}\\) est un processus stationnaire de moyenne \\(\\mu_X\\) et de fonction d’autocovariance \\(\\gamma_X(.)\\) telle que \\(\\underset{h\\in\\Z}{\\sum}|\\gamma_X(h)|&lt;+\\infty\\), alors\n\n\\(\\E\\left[I_n(0)\\right] - n\\mu_X^2 \\underset{n\\to +\\infty}{\\longrightarrow} 2\\pi f_X(0)\\)\n\\(\\E\\left[\\hat f_X(\\omega)\\right] \\underset{n\\to +\\infty}{\\longrightarrow} f_X(\\omega)\\) si \\(\\omega\\neq 0\\)\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\n\n\\(\\E\\left[I_n(0)\\right] - n\\mu_X^2 = \\E\\left[n\\bar X_n^2\\right] - n\\mu_X^2= n\\V(\\bar X_n)\\underset{n\\to +\\infty}{\\longrightarrow}\\underset{h\\in\\Z}{\\sum} \\gamma_X(h)=2\\pi f(0)\\) d’après Theorem 4.1.\nSoit \\(\\omega\\in]0,\\pi]\\). Soit \\(g(n,\\omega)\\) le multiple de \\(\\frac{2\\pi}{n}\\) le plus proche de \\(\\omega\\). Pour \\(n\\) assez grand, on a que \\(g(n,\\omega)\\neq 0\\). Alors \\[\\begin{eqnarray*}\n2\\pi \\E\\left[\\hat f_X(\\omega)\\right] &=& \\E\\left[I_n(\\omega)\\right]\\\\ &=& \\E\\left[I_n(g(n,\\omega))\\right]\\\\\n&=& \\E\\left[\\underset{|h|&lt;n}{\\sum} \\hat\\gamma_{X,n}(h) e^{-ih  g(n,\\omega)}\\right]\\\\\n&=& \\E\\left[\\underset{|h|&lt;n}{\\sum} \\frac 1 n \\underset{t=1}{\\stackrel{n-|h|}{\\sum}}(X_t - \\bar X_n)(X_{t+h}-\\bar X_n) e^{-ih  g(n,\\omega)}\\right]\\\\\n&=& \\E\\left[\\underset{|h|&lt;n}{\\sum}\\frac 1 n \\underset{t=1}{\\stackrel{n-|h|}{\\sum}} (X_t - \\mu)(X_{t+h}-\\mu) e^{-i h g(n,\\omega)}\\right]\\\\\n&=& \\underset{|h|&lt;n}{\\sum} \\E\\left[(X_t - \\mu)(X_{t+h}-\\mu)\\right] e^{-ih  g(n,\\omega)}\\\\\n&=& \\underset{|h|&lt;n}{\\sum} \\frac 1 n \\underset{t=1}{\\stackrel{n-|h|}{\\sum}}\\gamma_X(h) e^{-ih  g(n,\\omega)}\\\\\n&=& \\underset{|h|&lt;n}{\\sum} \\frac{n-|h|}{n}\\gamma_X(h) e^{-ih  g(n,\\omega)}\\\\\n&=& \\underset{|h|&lt;n}{\\sum} \\left(1-\\frac{|h|}{n}\\right)\\gamma_X(h) e^{-i h g(n,\\omega)}.\n\\end{eqnarray*}\\] Comme \\(\\underset{h\\in\\Z}{\\sum}|\\gamma_X(h)|&lt;+\\infty\\), par convergence dominée \\[\n\\underset{h\\in\\Z}{\\sum} \\left(1-\\frac{|h|}{n}\\right)\\gamma_X(h) e^{-ih\\lambda} 1_{|h|&lt;n} \\underset{n\\rightarrow +\\infty}{\\longrightarrow} \\underset{h\\in\\Z}{\\sum} \\gamma_X(h) e^{-ih\\lambda}= 2\\pi f(\\lambda),\n\\] et \\(g(n,\\omega)\\underset{n\\rightarrow +\\infty}{\\longrightarrow}\\omega\\). Donc \\[\n\\E\\left[I_n(\\omega)\\right]\\underset{n\\rightarrow +\\infty}{\\longrightarrow} 2\\pi f(\\omega).\n\\]\n\n\n\n\nUne autre stratégie est d’utiliser un noyau sur les \\(I_n(\\omega_j)\\) pour construire un estimateur lissé.\n\nSoit \\((m_n)_n\\) une suite d’entiers positifs tels que \\(m_n\\underset{n\\to +\\infty}{\\longrightarrow} +\\infty\\) et \\(\\frac{m_n}{n}\\underset{n\\to +\\infty}{\\longrightarrow} 0\\). Soit un noyau \\(W_n(.)\\) tel que\n\n\\(W_n(-j)=W_n(j) \\geq 0,\\ \\forall j\\)\n\\(\\underset{|j|\\leq m_n}{\\sum}W_n(j)=1\\)\n\\(\\underset{|j|\\leq m_n}{\\sum}W_n(j)^2\\underset{n\\to +\\infty}{\\longrightarrow}0\\)\n\nAlors on définit l’estimateur \\[\n\\tilde f_X(\\omega) = \\frac{1}{2\\pi}\\underset{|j|\\leq m_n}{\\sum}W_n(j)\\ I_n \\left(g(n,\\omega)+\\frac{2\\pi j}{n}\\right) \\textrm{ où } g(n,\\omega)=\\underset{\\omega_k\\in\\Omega_n}{\\mbox{argmin}} \\left|\\omega_k - \\omega\\right|.\n\\]\n\n\n\nProposition 4.8 Par les hypothèses sur la suite \\((m_n)_n\\) et le noyau \\(W_n\\), l’estimateur lissé \\(\\tilde f_X(.)\\) est asymptotiquement sans biais. \\[\n\\forall \\omega,\\ \\E\\left[\\tilde f_X(\\omega)\\right] \\underset{n\\to +\\infty}{\\longrightarrow} f_X(\\omega).\n\\]\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\n\\[\\begin{eqnarray*}\n\\left| \\E[\\tilde f_X(\\omega) - f_X(\\omega)] \\right|\n&=& \\left| \\underset{|j|\\leq m_n}{\\sum}\\frac{W_n(j)}{2\\pi} \\E\\left[I_n \\left(g(n,\\omega)+\\frac{2\\pi j}{n}\\right)\\right]- f_X(\\omega)\\right|\\\\\n&=& \\left| \\underset{|j|\\leq m_n}{\\sum} W_n(j) \\left\\{\\frac{ \\E\\left[I_n \\left(g(n,\\omega)+\\frac{2\\pi j}{n}\\right)\\right]}{2\\pi}-f_X\\left(g(n,\\omega)+\\frac{2\\pi}{j}\\right) +f_X\\left(g(n,\\omega)+\\frac{2\\pi}{j}\\right) - f_X(\\omega)\\right\\}\\right|\\\\\n\\end{eqnarray*}\\] car \\(\\underset{|j|\\leq m_n}{\\sum} W_n(j) =1\\).\nL’hypothèse sur \\((m_n)_n\\) implique que \\[\n\\underset{|j|\\leq m_n}{\\max} \\left|g(n,\\omega) + \\frac{2\\pi j}{n} -\\omega\\right|\\underset{n\\rightarrow +\\infty}{\\longrightarrow}0.\n\\] Pour tout \\(\\varepsilon&gt;0\\), avec la continuité de \\(f_X(.)\\), pour \\(n\\) assez grand \\[\n\\underset{|j|\\leq m_n}{\\max} \\left|f_X\\left(g(n,\\omega) + \\frac{2\\pi j}{n} \\right)-f_X(\\omega)\\right|\\leq \\frac \\varepsilon 2.\n\\]\nDe plus, \\(\\E[I_n(\\omega)]\\underset{n\\rightarrow +\\infty}{\\longrightarrow} 2\\pi f_X(\\omega),\\ \\forall \\omega \\neq 0\\) donc pour \\(n\\) assez grand \\[\n\\underset{|j|\\leq m_n}{\\max} \\left|\\frac{ \\E\\left[I_n \\left(g(n,\\omega)+\\frac{2\\pi j}{n}\\right)\\right]}{2\\pi} -\nf_X\\left(g(n,\\omega)+\\frac{2\\pi}{j}\\right)\n\\right|\\leq \\frac \\varepsilon 2.\n\\]\nAinsi, pour \\(n\\) assez grand, \\[\\begin{eqnarray*}\n\\left| \\E[\\tilde f_X(\\omega) - f_X(\\omega)] \\right|\n&\\leq&\n\\underset{|j|\\leq m_n}{\\sum} W_n(j) \\left\\{\n\\left| \\frac{ \\E\\left[I_n \\left(g(n,\\omega)+\\frac{2\\pi j}{n}\\right)\\right]}{2\\pi}-f_X\\left(g(n,\\omega)+\\frac{2\\pi}{j}\\right)\\right|\n+\\left|f_X\\left(g(n,\\omega)+\\frac{2\\pi}{j}\\right) - f_X(\\omega)\\right|\\right\\}\\\\\n&\\leq & \\underset{|j|\\leq m_n}{\\sum} W_n(j) \\left(\\frac \\varepsilon 2+\\frac \\varepsilon 2\\right)\\leq \\varepsilon.\n\\end{eqnarray*}\\] Donc \\[\n\\E\\left[\\tilde f_X(\\omega)\\right] \\underset{n\\to +\\infty}{\\longrightarrow} f_X(\\omega).\n\\]\n\n\n\n\nExample 4.1 Soit le processus \\((X_t)_{t\\in\\Z}\\) défini par \\(X_t = \\varepsilon_t + 0.7 \\varepsilon_{t-1},\\ (\\varepsilon_t)_{t\\in\\Z}\\sim\\text{WN}(0,1)\\).\nD’après Example 3.4, la fonction d’autocovariance de ce processus MA(1) vaut \\[\n\\gamma_X(h)=\\left\\{\n\\begin{array}{l l}\n(1+0.7^2) & \\textrm{ si } h=0\\\\\n0.7 & \\textrm{ si } |h|=1\\\\\n0 & \\textrm{ si } |h|&gt;1\\\\\n\\end{array}\n\\right.\n\\]\nLa densité spectrale de ce processus vaut alors \\[\nf_X(\\omega) = \\underset{h\\in\\Z}{\\sum} \\gamma_X(h) e^{-i \\omega h} = 1+ (0.7)^2 + 2\\times 0.7 \\times\\cos(\\omega),\\ \\forall \\omega\\in\\R\n\\]\nSur la Figure 4.1, la densité spectrale du processus est représentée en rouge et deux estimateurs par lissage (obtenus pour deux paramètres différents de lissage de la fonction smooth.periodogram) en bleu et en magenta. Les points correspondent aux valeurs de \\(2\\pi I_n(\\omega_j)\\).\n\n\n\n\n\nFigure 4.1: Illustration de l’estimation de la densité spectrale d’un processus MA(1)"
  },
  {
    "objectID": "chap3.html#prévision-linéaire-optimale",
    "href": "chap3.html#prévision-linéaire-optimale",
    "title": "4  Statistique des processus stationnaires du second ordre",
    "section": "4.2 Prévision linéaire optimale",
    "text": "4.2 Prévision linéaire optimale\nUn objectif important dans l’étude des séries temporelles est de pouvoir prédire les valeurs non encore observées de la série et cela à des horizons plus ou moins éloignés. Nous allons dans cette section introduire la méthode de prévision la plus courammennt utilisée appelée la prévision linéaire optimale. Cette méthode s’appuie sur la propriété d’espace de Hilbert de \\(L^2(\\Omega,\\mathcal{A},P)\\).\n\n4.2.1 Espaces linéaires engendrés par un processus du second ordre\n\nDefinition 4.3 Soit \\((X_t)_{t\\in \\Z}\\), un processus du second ordre. On appelle espace vectoriel fermé engendré par une famille \\((X_t)_{t\\in I}\\), où \\(I \\subset \\Z\\), le plus petit sous-espace vectoriel fermé de \\(L^2 (\\Omega,\\mathcal A,P)\\) qui contient tous les \\(X_t\\) pour \\(t\\in I\\). On le note \\(\\overline{\\text{Vect}}(X_t,t\\in I)\\).\n\n\n\nProposition 4.9 Le sous-espace vectoriel fermé engendré par une famille finie \\((X_i)_{i\\in I}\\), où \\(I \\subset \\Z\\), est l’ensemble de toutes les combinaisons linéaires, i.e. l’ensemble des v.a. \\(Y\\) de la forme \\(Y=\\sum_{i\\in I}\\alpha_i X_i\\).\n\n\nDefinition 4.4 Soit l’espace vectoriel fermé engendré par une famille finie \\((X_i)_{i\\in I}\\) de v.a. de \\(L^2 (\\Omega,\\mathcal A,P)\\), où \\(I\\subset\\Z\\) fini : \\(\\mathcal H=\\overline{\\text{Vect}}(X_i,i\\in I)\\).\nLa projection orthogonale d’une v.a. \\(X\\) sur \\(\\mathcal H\\) est l’unique élément\n\\[\n\\hat X=P_{\\mathcal H} X=\\sum_{i\\in I}\\alpha_i X_i\n\\textrm{ tel que }\n\\langle X-\\hat X,Z\\rangle_{L^2}=0,\\ \\ \\forall Z\\in \\mathcal H.\n\\] On a que \\(\\langle \\hat X,X_i\\rangle_{L^2}=\\langle X,X_i\\rangle_{L^2}, \\text{ pour tout } i\\in I.\\)\n\n\nDans la suite, on note\n\n\\(\\mathcal H^n_1=\\overline{\\text{Vect}}(1,X_1,\\ldots, X_n)\\)\n\\(\\mathcal H^n_{-\\infty}=\\overline{\\text{Vect}}(1,X_t, t\\leq n)\\).\n\n\n\n4.2.2 Régression linéaire\nPour prédire \\(X_{n+1}\\) (ou \\(X_{n+h}\\)) à partir de l’observation des \\(X_1,\\ldots,X_n\\), on pourrait s’appuyer sur la projection dans \\(L^2\\) de \\(X_{n+1}\\) (ou \\(X_{n+h}\\)) sur le sous-espace vectoriel fermé des fonctions \\(\\sigma(X_1,\\ldots,X_n)\\)-mesurables \\[\\begin{eqnarray*}\n\\hat X&=&\\E[X|\\sigma(X_1,\\ldots,X_n)]\\\\\n    &=&P_{\\mathcal M(X_1,\\ldots,X_n)}(X)\\\\\n    &=&\\mbox{arg inf}_{Y\\in  \\mathcal M(X_1,\\ldots,X_n)}||X-Y||_{L^2}\n\\end{eqnarray*}\\] où \\(\\mathcal M(X_1,\\ldots,X_n)=\\left\\{g(X_1,\\ldots,X_n); g \\text{ fonction borélienne de } \\R^n \\text{ vers } \\R\\right\\}\\) et \\(\\sigma(X_1,\\ldots,X_n)\\) est la tribu engendrée par les v.a. \\(X_1,\\ldots,X_n\\). Mais une telle espérance conditionnelle est souvent difficilement calculable. Aussi l’idée est de se restreindre à un espace plus simple, inclus dans \\(\\mathcal M(X_1,\\ldots,X_n)\\) pour lequel l’espérance conditionnelle est accessible. On va donc ici se restreindre à projeter sur l’espace vectoriel fermé \\(\\mathcal H^n_1: =\\overline{\\text{Vect}}(1,X_1,\\ldots, X_n) \\subset \\mathcal M(X_1,\\ldots,X_n)\\). Ainsi on cherche une v.a. \\(\\hat X\\) comme une combinaison linéaire des v.a. \\(1,X_1,\\ldots, X_n\\) plutôt qu’une fonction mesurable quelconque de ces variables.\n\nDefinition 4.5 On appelle régression linéaire d’une v.a. \\(Y\\) de \\(L^2(\\Omega,\\mathcal A,P)\\) sur \\(\\mathcal H^n_1=\\overline{\\text{Vect}}(1,X_1,\\ldots, X_n),\\) la projection orthogonale, au sens de la norme \\(L^2\\), de \\(Y\\) sur cet espace. On la note \\(\\EL(Y|\\mathcal H^n_1)\\).\n\n\n\nProposition 4.10 (Caractérisation) \nSoit \\(Y\\in L^2(\\Omega,\\mathcal A,P)\\). La régression linéaire \\(\\hat Y=\\EL(Y|\\mathcal H^n_1)\\) est la v.a. \\[\\hat Y= \\alpha_0 +\\sum_{t=1}^n \\alpha_t X_t\\] telle que \\(\\E[\\hat Y]=\\E[Y]\\) et \\(\\E[\\hat Y X_t]=\\E[Y X_t]\\) pour \\(t=1,\\ldots,n\\).\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nPour la preuve, il suffit de traduire que \\(Y-\\hat Y\\) est orthogonal à \\(1,X_1,\\ldots,X_n\\) donc\n\\[\\begin{eqnarray*}\n\\langle Y-\\hat Y,1\\rangle_{L^2}=\\E[(Y-\\hat Y) 1] = 0 &\\Leftrightarrow & \\E[Y]=\\E[\\hat Y]\\\\\n\\langle Y-\\hat Y,X_t\\rangle_{L^2}= \\E[(Y-\\hat Y)X_t]=0\n&\\Leftrightarrow & \\E[Y X_t]=\\E[\\hat Y X_t]\\ \\text{ pour }t=1,\\ldots,n\n\\end{eqnarray*}\\]\n\n\n\n\n\n4.2.3 Prévision linéaire optimale\n\nDefinition 4.6 Soit \\((X_t)_{t\\in \\Z}\\) un série temporelle stationnaire. La prévision linéaire optimale de \\(X_{n+1}\\) sachant son passé observé est\n\n\\(\\hat X_{n+1}=\\EL(X_{n+1}|\\mathcal H^n_1)\\) dans le cas d’un passé fini\n\\(\\hat X_{n+1}=\\EL(X_{n+1}|\\mathcal H^n_{-\\infty})\\) dans le cas d’un passé infini\n\n\n\n\nDefinition 4.7 Soit \\((X_t)_{t\\in \\Z}\\) une série temporelle stationnaire et, pour tout \\(t\\in\\Z\\), la prévision linéaire optimale \\(\\hat X_t=\\EL(X_t|\\mathcal H^{t-1}_{-\\infty})\\) de \\(X_t\\) sachant le passé (infini) du processus.\nOn appelle processus des innovations le processus \\((\\varepsilon_t)_{t\\in \\Z}\\) des erreurs de prévision successives \\[\n\\varepsilon_t=X_t-\\hat X_t,\\ \\forall t\\in\\Z.\n\\]\n\n\n\nProposition 4.11 Le processus des innovations \\((\\varepsilon_t)_{t\\in \\Z}\\) est un bruit blanc.\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\n - Par définition de \\(\\H^{t-1}_{-\\infty}\\) et de \\(\\hat X_t\\), on a \\(\\langle X_t-\\hat X_t,1\\rangle_{L^2}=0\\) donc \\(\\E[X_t]=\\E[\\hat X_t]\\), ce qui prouve que le processus des innovations est centré.\n\nSoit \\(h\\in \\N^*\\). On a aussi \\[\n\\C(\\varepsilon_t,\\varepsilon_{t+h}) = \\E[\\varepsilon_t\\varepsilon_{t+h}]=\\langle \\varepsilon_t,\\varepsilon_{t+h} \\rangle.\n\\]\n\nOr \\(\\varepsilon_{t+h} \\perp \\mathcal{H}_{-\\infty}^{t+h-1}\\) et \\(\\varepsilon_t = X_t - \\EL(X_t|\\mathcal{H}_{-\\infty}^{t-1}) \\subset \\mathcal{H}_{-\\infty}^{t+h-1}\\). Donc \\(\\C(\\varepsilon_t,\\varepsilon_{t+h}) =\\langle \\varepsilon_t,\\varepsilon_{t+h} \\rangle = 0.\\)\n\nMontrons que la variance du processus des innovations est constante :\n\nTout élément de \\(\\H^{t-1}_{-\\infty}\\) s’écrit sous la forme \\(\\sum_{i=1}^{+\\infty}\\alpha_i X_{t-i}\\) donc \\(\\varepsilon_t = X_t - \\sum_{i=1}^{+\\infty}\\alpha_i X_{t-i}\\). Ainsi, \\[\\begin{eqnarray*}\n\\V(\\varepsilon_t)\n&=&\\V\\left(X_t -\\EL(X_t|\\H^{t-1}_{-\\infty})\\right)\\\\\n&=&\\V\\left(X_t-\\sum_{i=1}^{+\\infty}\\alpha_i X_{t-i}\\right)\\\\\n&=&\\V\\left(-\\sum_{i=0}^{+\\infty}\\alpha_i X_{t-i}\\right)\\ \\ \\ \\text{ avec }\\alpha_0=-1\\\\\n&=&\\sum_{i=0}^{+\\infty}\\sum_{j=0}^{+\\infty}\\alpha_i \\alpha_j \\C(X_{t-i},X_{t-j})\\\\\n&=&\\sum_{i=0}^{+\\infty}\\sum_{j=0}^{+\\infty}\\alpha_i \\alpha_j \\gamma_X(i-j)\n\\end{eqnarray*}\\] donc la variance est constante.\n\n\n\n\n\n4.2.4 Prévision linéaire optimale dans le cas d’un passé fini\nOn se place ici dans le cas particulier où l’on observe qu’un passé fini \\(X_1,\\ldots,X_n\\) et on souhaite prédire des valeurs futures.\n\nProposition 4.12 Soit \\((X_t)_{t\\in \\Z}\\) une série temporelle stationnaire de moyenne \\(\\mu_X\\) et d’ACVF \\(\\gamma_X(\\cdot)\\). La prévision linéaire optimale de \\(X_{n+h}\\), pour \\(h\\in \\N^*\\), ayant observé le passé \\(X_1,\\ldots,X_n\\), est \\[\n\\hat X_{n}(h)=\\hat X_{n +h}=\\alpha_0 +\\sum_{t=1}^n \\alpha_t X_t,\n\\]\noù les coefficients \\(\\alpha_0,\\ldots,\\alpha_n\\) sont donnés par \\[\n\\left(\n\\begin{array}{c}\n  \\alpha_1  \\\\\n  \\vdots  \\\\\n  \\alpha_n  \n\\end{array}\n\\right)=\\Gamma_{X,n}^{-1}\\left(\n\\begin{array}{c}\n  \\gamma_X(n+h-1) \\\\\n  \\vdots  \\\\\n  \\gamma_X(h)   \n\\end{array}\n\\right)\n\\] avec la matrice de covariance du vecteur \\((X_1,\\ldots,X_n)\\), supposée inversible, \\[\n\\Gamma_{X,n}=\n\\left(\n\\begin{array}{cccc}\n\\gamma_X(0)   & \\gamma_X(1)  & \\cdots & \\gamma_X(n-1)  \\\\\n\\gamma_X(1)  &\\gamma_X(0)   & \\ddots &\\gamma_X(n-2)  \\\\\n\\vdots  &  \\ddots &   \\ddots &\\vdots\\\\\n\\gamma_X(n-1) &\\cdots&\\gamma_X(1)& \\gamma_X(0)\n\\end{array}\n\\right)\n\\] et \\[\n\\mu_X=\\alpha_0+\\mu_X\\sum_{t=1}^n \\alpha_t.\n\\]\n\nNotons que si le processus \\((X_t)_{t\\in \\Z}\\) est centré, alors le premier coefficient \\(\\alpha_0\\) est nul et \\[\n\\hat X_{n}(h)=\\hat X_{n +h}=\\sum_{t=1}^n \\alpha_t X_t.\n\\]\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nPar définition de la prévision linéaire optimale et grâce à la Proposition 4.10, on a que \\[\\begin{eqnarray*}\n\\E[\\hat X_{n}(h)]=\\E[X_{n +h}]\n&\\Leftrightarrow & \\E\\left[\\alpha_0 +\\sum_{t=1}^n \\alpha_t X_t\\right] = \\E[X_{n+h}]\\\\\n&\\Leftrightarrow & \\alpha_0+\\mu_X\\sum_{t=1}^n \\alpha_t=\\mu_X\n\\end{eqnarray*}\\] car le processus \\((X_t)_{t\\in\\Z}\\) est stationnaire. De plus, pour tout \\(i=1,\\ldots, n\\), \\(\\E[X_{n +h}X_i]=\\E[\\hat X_{n}(h)X_i]\\) se réécrit \\[\n\\E[X_{n +h}X_i] = \\alpha_0\\E[X_i]+\\sum_{t=1}^n\\alpha_t\\E[X_tX_i].\n\\] On a ainsi, pour tout \\(i=1,\\ldots, n\\), \\[\\begin{eqnarray*}\n\\E[X_{n +h}X_i]-\\E[X_{n +h}]\\E[X_i]\n&=&\\alpha_0\\E[X_i]+\\sum_{t=1}^n\\alpha_t\\E[X_tX_i]-\\mu_X\\E[X_i]\\\\\n&=&\\alpha_0\\E[X_i]+\\sum_{t=1}^n\\alpha_t\\E[X_tX_i]-\\E[X_i]\\left\\{\\alpha_0+\\sum_{t=1}^n \\alpha_t \\mu_X\\right\\}\\\\\n&=&\\sum_{t=1}^n\\alpha_t\\left( \\E[X_tX_i] -\\mu_X^2 \\right)=\\sum_{t=1}^n\\alpha_t \\C(X_t,X_i).\n\\end{eqnarray*}\\] On a donc obtenu, pour tout \\(i=1,\\ldots, n\\), \\(\\gamma_X(n+h-i)=\\sum_{t=1}^n\\alpha_t \\gamma_X(t-i)\\), ce qui donne l’équation matricielle \\[\n\\left(\n\\begin{array}{c}\n  \\gamma_X(n+h-1) \\\\\n  \\vdots  \\\\\n  \\gamma_X(h)   \n\\end{array}\n\\right)\n=\\Gamma_{X,n}\n\\left(\n\\begin{array}{c}\n  \\alpha_1  \\\\\n  \\vdots  \\\\\n  \\alpha_n  \n\\end{array}\n\\right).\n\\] L’hypothèse d’inversibilité de la matrice \\(\\Gamma_{X,n}\\) permet d’obtenir l’équation donnée dans la proposition.\n\n\n\n\nExample 4.2 (Processus autorégressif d’ordre 1 AR(1))  Soit \\(X_t=\\phi X_{t-1}+\\varepsilon_t,\\ \\forall t\\in \\mathbb Z,\\) où \\((\\varepsilon_t)\\sim \\text{WN}(0,\\sigma^2)\\) et \\(0&lt;|\\phi|&lt;1\\). On rappelle que les \\(\\varepsilon_t\\) sont indépendants du passé de la série temporelle et \\(\\gamma_X(h)=\\phi^{|h|}\\gamma_X(0),\\ \\forall h\\in \\Z\\).\nLa prévision linéaire optimale de \\(X_{n+1}\\) sur la base des observations de \\(X_1,\\ldots,X_n\\) est de la forme \\[\n\\hat X_{n}(1)=\\hat X_{n+1}=\\sum_{t=1}^n \\alpha_t X_t,\n\\] avec \\[\n    \\left(\n    \\begin{array}{cccc}\n    1   & \\phi  & \\cdots & \\phi^{n-1}  \\\\\n    \\phi &1   & \\ddots &\\phi^{n-2}  \\\\\n    \\vdots  &  \\ddots &   \\ddots &\\vdots\\\\\n    \\phi^{n-1} &\\cdots&\\phi& 1\n    \\end{array}\n    \\right)\n    \\left(\n    \\begin{array}{c}\n    \\alpha_1  \\\\\n    \\alpha_2 \\\\\n    \\vdots  \\\\\n    \\alpha_n\n    \\end{array}\n    \\right)=\n    \\left(\n    \\begin{array}{c}\n    \\phi^n  \\\\\n    \\phi^{n-1}   \\\\\n    \\vdots  \\\\\n    \\phi\n    \\end{array}\n    \\right).\n\\]\nComme \\((\\alpha_1,\\ldots,\\alpha_n)=(0,\\ldots,0,\\phi)\\) est une solution de cette équation et l’unicité de la projection orthogonale donne \\[\n\\hat X_{n}(1)=\\hat X_{n+1}=\\phi X_n.\n\\]\n\n\n\n4.2.5 Evolution des prévisions linéaires optimales en fonction de la taille de la mémoire\nSoit un processus stationnaire \\((X_t)_{t\\in \\Z}\\) tel que ses matrices d’autocorrélation \\(R_X(h)\\) sont inversibles pour tout \\(h\\) dans \\(\\N\\). On s’intéresse ici à la prévision linéaire optimale de \\(X_t\\) pour une taille de mémoire \\(k\\), c’est-à-dire en fonction de l’observation des v.a. \\(X_{t-1},\\ldots,X_{t-k}\\).\nSans perte de généralité, on suppose le processus est centré.\nD’après la Proposition 4.12, on a vu comment obtenir les coefficients \\(\\alpha_1(k),\\ldots, \\alpha_k(k)\\) de la prévision linéaire optimale de \\(X_t\\) en fonction du passé observé \\(X_{t-1},\\ldots,X_{t-k}\\) : \\[\n\\EL(X_t|\\mathcal H_{t-k}^{t-1})=\\alpha_1(k)X_{t-1}+\\cdots+\\alpha_k(k)X_{t-k}\n\\]\navec \\[\n    \\left(\n    \\begin{array}{c}\n    \\alpha_1(k)  \\\\\n    \\vdots  \\\\\n    \\alpha_k(k)  \n    \\end{array}\n    \\right)=R_X(k)^{-1}\\left(\n    \\begin{array}{c}\n    \\rho_X(1) \\\\\n    \\vdots  \\\\\n    \\rho_X(k)   \n    \\end{array}\n    \\right)\n\\]\nSi on augmente la taille de la mémoire, il faut à chaque instant d’observation supplémentaire inverser la matrice de corrélation d’après l’expression précédente. Nous allons donc chercher une méthode itérative permettant de déterminer les nouveaux coefficients (avec une mémoire de taille \\(k+1\\)) en fonction des anciens (avec une mémoire de taille \\(k\\)). Pour cela, nous avons besoin de quelques lemmes techniques.\n\nLemma 4.1 Les coefficients de la régression de \\(X_t\\) sur le passé de taille de mémoire \\(k\\) sont les mêmes que ceux de la régression de \\(X_t\\) sur les \\(k\\) prochaines variables du processus : \\[\n\\EL(X_t|\\mathcal H_{t-k}^{t-1})=\\sum_{i=1}^k\\alpha_i(k)X_{t-i}\\implies \\EL(X_t|\\mathcal H_{t+1}^{t+k})=\\sum_{i=1}^k\\alpha_i(k)X_{t+i}\n\\]\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nOn note \\(\\beta_1(k), \\ldots,\\beta_k(k)\\) les coefficients de la régression de \\(X_t\\) sur \\(\\mathcal H_{t+1}^{t+k}\\) (engendré par les \\(k\\) variables futures). Par le même raisonnement, ces coefficients vérifient \\[\n\\left(\n\\begin{array}{c}\n  \\beta_1(k)  \\\\\n  \\vdots  \\\\\n  \\beta_k(k)  \n\\end{array}\n\\right)=R_X(k)^{-1}\\left(\n\\begin{array}{c}\n  \\rho_X(-1) \\\\\n  \\vdots  \\\\\n  \\rho_X(-k)   \n\\end{array}\n\\right),\n\\] puisque la matrice de corrélation ne dépend que des écarts temporels. Par parité de la fonction d’autocorrélation, le vecteur des corrélations à droite n’est autre que le vecteur \\((\\rho_X(1),\\ldots, \\rho_X(k))\\). Ainsi les \\(\\beta_i(k)\\) et les \\(\\alpha_i(k)\\) satisfont la même équation donc \\(\\alpha_i(k)=\\beta_i(k)\\), pour \\(i=1,\\ldots,k\\).\n\n\n\n\nLemma 4.2 On a l’équation récursive suivante exprimant les coefficients pour une mémoire de taille \\(k\\) en fonction de ceux d’une mémoire de taille \\(k-1\\) \\[\n\\alpha_i(k)=\\alpha_i(k-1)-\\alpha_k(k)\\alpha_{k-i}(k-1),\\ \\forall i=1,\\ldots, k-1.\n\\]\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nOn commence par remarquer que \\[\n\\mathcal H_{t-(k-1)}^{t-1} = \\overline{\\text{Vect}}(X_{t-(k-1)},\\ldots,X_{t-1})\n\\subset \\overline{\\text{Vect}}(X_{t-(k-1)},\\ldots,X_{t}) =\\mathcal H_{t-(k-1)}^{t}\n\\]\ndonc en terme de projection orthogonale, on a que\n\\[\nP_{\\H_{t-(k-1)}^{t-1}}(X_t)=P_{\\H_{t-(k-1)}^{t-1}}\\circ P_{\\H_{t-k}^{t-1}} (X_t).\n\\] Ainsi, on a : \\[\\begin{eqnarray*}\n\\EL(X_t|\\H_{t-(k-1)}^{t-1})&=&\nP_{\\H_{t-(k-1)}^{t-1}}\\left(\\alpha_1(k)X_{t-1}+\\cdots+\\alpha_{k-1}(k)X_{t-(k-1)}+\\alpha_k(k) x_{t-k}\\right)\\\\\n&=&\\alpha_1(k)X_{t-1}+\\cdots+\\alpha_{k-1}(k)X_{t-(k-1)}+\\alpha_k(k)\\EL(X_{t-k}|\\H_{t-(k-1)}^{t-1}).\n\\end{eqnarray*}\\] Mais on a aussi que \\[\n\\EL(X_t|\\H_{t-(k-1)}^{t-1})=\\alpha_1(k-1)X_{t-1}+\\cdots+\\alpha_{k-1}(k-1)X_{t-(k-1)}.\n\\] Or d’après le Lemma 4.1, \\[\\begin{eqnarray*}\n\\EL(X_{t-k}|\\H_{t-(k-1)}^{t-1})\n&=& \\EL(X_{t-k}|\\H_{t-k+1}^{(t-k)+(k-1)})\\\\\n&=& \\sum_{i=1}^{k-1} \\alpha_i(k-1) X_{(t-k)+i}\\\\\n&=& \\alpha_1(k-1)X_{t-k+1}+\\alpha_2(k-1)X_{t-k+2}+\\cdots+\\alpha_{k-1}(k-1)X_{t-1}\n\\end{eqnarray*}\\]\nEn rassemblant les expressions, on a \\[\\begin{eqnarray*}\n& &\\alpha_1(k-1)X_{t-1}+\\cdots+\\alpha_{k-1}(k-1)X_{t-(k-1)}\\\\\n&=& \\alpha_1(k)X_{t-1}+\\cdots+\\alpha_{k-1}(k)X_{t-(k-1)}\\\\\n& & +\\alpha_k(k) \\left\\{\\alpha_1(k-1)X_{t-k+1}+\\alpha_2(k-1)X_{t-k+2}+\\cdots+\\alpha_{k-1}(k-1)X_{t-1}\\right\\}\n\\end{eqnarray*}\\] Donc \\[\n\\left\\{\n\\begin{array}{l l l}\n\\alpha_1(k-1) &=& \\alpha_1(k)+\\alpha_k(k)\\alpha_{k-1}(k-1)\\\\\n\\alpha_2(k-1) &=& \\alpha_2(k)+\\alpha_k(k)\\alpha_{k-2}(k-1)\\\\\n&\\vdots&\\\\\n\\alpha_{k-1}(k-1) &=&\\alpha_{k-1}(k)+\\alpha_k(k)\\alpha_{1}(k-1)\n\\end{array}\n\\right.\n\\]\n\n\n\nDu Lemma 4.2, on peut constater que l’on peut obtenir un algorithme récursif pour calculer les coefficients si l’on est capable d’exprimer le dernier terme \\(\\alpha_k(k)\\) en fonction des \\(\\alpha_i(k-1)\\). C’est l’objectif du lemme suivant.\n\nLemma 4.3 On a la relation \\[\n\\alpha_k(k)=\\displaystyle \\frac{\\rho_X(k)-\\underset{i=1}{\\stackrel{k-1}{\\sum}} \\alpha_i(k-1)\\rho_X(k-i)}{1-\\underset{i=1}{\\stackrel{k-1}{\\sum}}\\alpha_i(k-1)\\rho_X(i)}\\ \\forall k\\geq 2\n\\]\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nOn revient à l’équation matricielle : \\[\\begin{eqnarray*}\n\\left(\n\\begin{array}{c}\n  \\rho_X(1) \\\\\n  \\vdots  \\\\\n  \\rho_X(k)   \n\\end{array}\n\\right) &= & R_X(k)\\left(\n\\begin{array}{c}\n  \\alpha_1(k)  \\\\\n  \\vdots  \\\\\n  \\alpha_k(k)  \n\\end{array}\n\\right)\\\\\n&=&\\left(\n\\begin{array}{cccc}\n1   & \\rho_X(1)  & \\cdots & \\rho_X(k-1)  \\\\\n\\rho_X(1)  &1  & \\ddots &\\rho_X(k-2)  \\\\\n\\vdots  &  \\ddots &   \\ddots &\\vdots\\\\\n\\rho_X(k-1) &\\cdots&\\rho_X(1)& 1\n\\end{array}\n\\right)\n\\left(\n\\begin{array}{c}\n  \\alpha_1(k)  \\\\\n  \\vdots  \\\\\n  \\alpha_k(k)  \n\\end{array}\n\\right)\n\\end{eqnarray*}\\]\nLa dernière ligne de ce produit matriciel nous donne l’équation suivante : \\[\n\\rho_X(k)=\\sum_{i=1}^{k-1}\\rho_X(k- i)\\alpha_i(k) + \\alpha_k(k).\n\\] Donc, en utilisant le Lemma 4.2, \\[\\begin{eqnarray*}\n\\alpha_k(k)&=&\\rho_X(k)-\\sum_{i=1}^{k-1}\\rho_X(k- i)\\alpha_i(k)\\\\\n&=&\\rho_X(k)-\\sum_{i=1}^{k-1}\\rho_X(k- i)\\left( \\alpha_i(k-1)-\\alpha_k(k)\\alpha_{k-i}(k-1)  \\right)\\\\\n&=&\\rho_X(k)-\\sum_{i=1}^{k-1}\\rho_X(k- i) \\alpha_i(k-1)+\\alpha_k(k)\\sum_{i=1}^{k-1}\\rho_X(k- i)\\alpha_{k-i}(k-1)\\\\\n&=&\\rho_X(k)-\\sum_{i=1}^{k-1}\\rho_X(k- i) \\alpha_i(k-1)+\\alpha_k(k)\\sum_{u=1}^{k-1}\\rho_X(u)\\alpha_{u}(k-1)\n\\end{eqnarray*}\\] ce qui, en isolant le terme \\(\\alpha_k(k)\\), donne la formule voulue.\n\n\n\nAvec le Lemma 4.2 et le Lemma 4.3, on en déduit l’algorithme de Durbin-Levinson.\n\nProposition 4.13 (Algorithme de Durbin-Levinson)  Les coefficients de la régression linéaire \\(\\EL(X_t|\\mathcal H_{t-k}^{t-1})\\) pour une mémoire de taille \\(k\\) s’obtiennent en fonction de ceux de la régression linéaire \\(\\EL(X_t|\\mathcal H_{t-(k-1)}^{t-1})\\) pour une mémoire de taille \\(k-1\\) grâce aux formules récursives suivantes \\[\n\\left\\{\n\\begin{array}{l l l}\n\\alpha_i(k)&=&\\alpha_i(k-1)-\\alpha_k(k)\\alpha_{k-i}(k-1),\\\\\n\\\\\n\\alpha_k(k)&=&\\displaystyle \\frac{\\rho_X(k)-\\sum_{i=1}^{k-1}\\alpha_i(k-1)\\rho_X(k-i)}{1-\\sum_{i=1}^{k-1}\\alpha_i(k-1)\\rho_X(i)},\\ \\forall k\\geq 2\\\\\n\\\\\n\\alpha_1(1)&=&\\rho_X(1).\n\\end{array}\n\\right.\n\\]"
  },
  {
    "objectID": "chap3.html#autocorrélations-partielles",
    "href": "chap3.html#autocorrélations-partielles",
    "title": "4  Statistique des processus stationnaires du second ordre",
    "section": "4.3 Autocorrélations partielles",
    "text": "4.3 Autocorrélations partielles\nComme nous avons pu le voir dans la section précédente, le coefficient \\(\\alpha_k(k)\\) devant \\(X_{t-k}\\) dans la prévision linéaire optimale de \\(X_t\\) en fonction du passé fini \\(\\mathcal H_{t-k}^{t-1}\\) de la série temporelle \\((X_t)_{t\\in \\Z}\\) joue un rôle particulier. Ces coefficients \\(\\alpha_k(k)\\) sont appelés autocorrélations partielles. Ils vont être au coeur de cette section.\n\nProposition 4.14 Le coefficient \\(\\alpha_k(k)\\) défini dans \\[\n\\EL(X_t|\\mathcal H_{t-k}^{t-1})=\\alpha_1(k)X_{t-1}+\\cdots+\\alpha_k(k)X_{t-k}\n\\] correspond au coefficient de corrélation entre les variables \\(X_t-\\EL(X_t|\\mathcal H_{t-(k-1)}^{t-1})\\) et \\(X_{t-k}-\\EL(X_{t-k}|\\mathcal H_{t-(k-1)}^{t-1})\\).\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nPar définition de la prévision linéaire optimale, on a \\[\n\\EL(X_t|\\H_{t-k}^{t-1})=\\sum_{i=1}^k \\alpha_i(k) X_{t-i}.\n\\] On a vu précédemment au début de la preuve du Lemma 4.2 l’égalité \\[\n\\EL(X_t|\\H_{t-k+1}^{t-1})=\\sum_{i=1}^{k-1} \\alpha_i(k)X_{t-i}+\\alpha_k(k)\\EL(X_{t-k}|\\H_{t-(k-1)}^{t-1}).\n\\] De ces deux équations, on obtient \\[\n\\EL(X_t|\\H_{t-k}^{t-1})-\\EL(X_t|\\H_{t-(k-1)}^{t-1})=\\alpha_k(k)\\left[X_{t-k}-\\EL(X_{t-k}|\\H_{t-(k-1)}^{t-1})\\right].\n\\] On en déduit que \\[\\begin{eqnarray*}\n&&\\C\\left(\\EL(X_t|\\H_{t-k}^{t-1})-\\EL(X_t|\\H_{t-(k-1)}^{t-1}), X_{t-k}-\\EL(X_{t-k}|\\H_{t-(k-1)}^{t-1})\\right)\\\\\n&&=\\alpha_k(k)\\V \\left(X_{t-k}-\\EL(X_{t-k}|\\H_{t-(k-1)}^{t-1}\\right).\n\\end{eqnarray*}\\]\nPar ailleurs, \\[\\begin{eqnarray*}\n& &\\C\\left(\\EL(X_t|\\H_{t-k}^{t-1})-\\EL(X_t|\\H_{t-(k-1)}^{t-1}), X_{t-k}-\\EL(X_{t-k}|\\H_{t-(k-1)}^{t-1})\\right)\\\\\n&=&\\C\\left(X_t-\\EL(X_t|\\H_{t-(k-1)}^{t-1})-\n(X_t- \\EL(X_t|\\H_{t-k}^{t-1})),X_{t-k}-\\EL(X_{t-k}|\\H_{t-(k-1)}^{t-1})\\right)\\\\\n&=&\\C\\left(X_t-\\EL(X_t|\\H_{t-(k-1)}^{t-1}),X_{t-k}-\\EL(X_{t-k}|\\H_{t-(k-1)}^{t-1})\\right)\\\\\n&&-\\C\\left(X_t- \\EL(X_t|\\H_{t-k}^{t-1}),X_{t-k}-\\EL(X_{t-k}|\\H_{t-(k-1)}^{t-1})\\right)\\\\\n&=&\\C\\left(X_t-\\EL(X_t|\\H_{t-(k-1)}^{t-1}),X_{t-k}-\\EL(X_{t-k}|\\H_{t-(k-1)}^{t-1})\\right) -0,\n\\end{eqnarray*}\\] car la seconde covariance est nulle puisque l’on a \\(X_t- \\EL(X_t|\\H_{t-k}^{t-1})\\perp \\H_{t-k}^{t-1}\\) et \\(X_{t-k}-\\EL(X_{t-k}|\\H_{t-k+1}^{t-1}) \\in \\H_{t-k}^{t-1}\\).\nDes deux dernières égalités on en déduit que \\[\\begin{eqnarray*}\n\\alpha_k(k)\n&=&\\frac{\\C\\left(X_t-\\EL(X_t|\\H_{t-(k-1)}^{t-1}),\n                X_{t-k}-\\EL(X_{t-k}|\\H_{t-(k-1)}^{t-1})\\right)}{\\V \\left(X_{t-k}-\\EL(X_{t-k}|\\H_{t-(k-1)}^{t-1}\\right)}\\\\\n&=&\\frac{\\C\\left(X_t-\\EL(X_t|\\H_{t-(k-1)}^{t-1}),\nX_{t-k}-\\EL(X_{t-k}|\\H_{t-k+1}^{t-1})\\right)}\n{\\sqrt{\\V \\left(X_{t}-\\EL(X_{t}|\\H_{t-(k-1)}^{t-1})\\right)}\n\\sqrt{\\V \\left(X_{t-k}-\\EL(X_{t-k}|\\H_{t-(k-1)}^{t-1})\\right)}},\n\\end{eqnarray*}\\] la dernière égalité étant justifiée par l’invariance par translation temporelle des covariances.\n\n\n\n\nDefinition 4.8 Ce coefficient de corrélation \\(\\alpha_k(k)\\) est appelé autocorrélation partielle d’ordre \\(k\\) et est noté \\(r_X(k)\\).\n\nL’autocorrrélation partielle s’interprète donc comme la corrélation entre \\(X_t\\) et \\(X_{t-k}\\) quand on leur a retiré leurs meilleures explications données par les variables intermédiaires. \n\nProposition 4.15 Il est équivalent de connaître le vecteur \\((\\rho_X(1),\\ldots\\rho_X(k))\\) ou le vecteur \\((r_X(1),\\ldots,r_X(k))\\).\n\n\nExample 4.3 On donne ici l’exemple des autocorrélations et autocorrélations partielles empiriques de quelques séries temporelles simulées.\n\nMA1&lt;-arima.sim(n=500,list(ma=c(0.7)))\nautoplot(acf(MA1,plot=FALSE))\nautoplot(pacf(MA1,plot=FALSE))\n\n\n\n\n\n\n\n(a) Autocorrélations empiriques\n\n\n\n\n\n\n\n(b) Autocorrélations partielles empiriques\n\n\n\n\nFigure 4.2: Résultats pour la série MA(1) \\(X_t = \\varepsilon_t -0.7 \\varepsilon_{t-1},\\ (\\varepsilon_t)_{t\\in\\Z}\\sim \\text{WN}(0,1)\\)\n\n\n\n\nAR1&lt;-arima.sim(n=500,list(ar=c(0.7)))\nautoplot(acf(AR1,plot=FALSE))\nautoplot(pacf(AR1,plot=FALSE))\n\n\n\n\n\n\n\n(a) Autocorrélations empiriques\n\n\n\n\n\n\n\n(b) Autocorrélations partielles empiriques\n\n\n\n\nFigure 4.3: Résultats pour la série AR(1) \\(X_t -0.7 X_{t-1}= \\varepsilon_t,\\ (\\varepsilon_t)_{t\\in\\Z}\\sim \\text{WN}(0,1)\\)\n\n\n\n\nB1&lt;-rnorm(1000,0,1)\nautoplot(acf(B1,plot=FALSE))\nautoplot(pacf(B1,plot=FALSE))\n\n\n\n\n\n\n\n(a) Autocorrélations empiriques\n\n\n\n\n\n\n\n(b) Autocorrélations partielles empiriques\n\n\n\n\nFigure 4.4: Résultats pour un bruit blanc gaussien \\(\\mathcal{N}(0,1)\\)"
  },
  {
    "objectID": "chap3.html#tests-de-blancheur-dun-processus",
    "href": "chap3.html#tests-de-blancheur-dun-processus",
    "title": "4  Statistique des processus stationnaires du second ordre",
    "section": "4.4 Tests de blancheur d’un processus",
    "text": "4.4 Tests de blancheur d’un processus\nQuand on va aborder les modèles ARMA, on va chercher à décomposer la partie stationnaire de la série (après avoir estimé ou éliminé tendance et saisonnalité) en une partie exploitable pour la prévision et une partie de bruit blanc. Aussi on a besoin de pouvoir tester la blancheur des résidus.\nOn souhaite donc ici tester l’hypothèse\n\\[\n    \\mathcal H_0 : (X_t)_{t\\in \\Z} \\text{ est un bruit blanc}\n\\]\ncontre\n\\[\n\\mathcal H_1 : (X_t)_{t\\in \\Z} \\text{ n'est pas un bruit blanc}.\n\\]\nPour réaliser ce test, on suppose que l’on observe \\(X_1,\\ldots,X_n\\). Le théorème suivant présente des statistiques de test pour répondre à ce test de blancheur.\n\nTheorem 4.2 \n\nStatistique de Portmanteau \\[\nQ_k=n\\sum_{h=1}^k \\hat \\rho_{X,n}(h)^2 \\underset{n\\to +\\infty}{\\stackrel{\\mathcal L}{\\longrightarrow}} \\chi^2(k)\n\\]\nStatistique de Ljung-Box \\[\nQ^*_k=n(n+2)\\sum_{h=1}^k \\frac{\\hat \\rho_{X,n}(h)^2}{n-h}\\underset{n\\to +\\infty}{\\stackrel{\\mathcal L}{\\longrightarrow}} \\chi^2(k)\n\\]\n\n\n\nSous l’hypothèse \\(\\mathcal H_0\\) (bruit blanc), on a vu que les autocorrélations sont données par \\(\\rho_X(h)=\\mathbb{1}_{h=0},\\ \\forall h\\in\\Z\\). Ainsi les statistiques \\(Q_k\\) et \\(Q^*_k\\) ont tendance à être faibles sous \\(\\mathcal H_0\\) et élevées sous \\(\\mathcal H_1\\). On va donc rejeter \\(\\mathcal H_0\\) si ces statistiques sont élevées.\n\nProposition 4.16 (Procédures de test)  On rejette \\(\\mathcal H_0\\) au risque \\(\\alpha\\) si \\(Q_k\\) (resp. \\(Q_k^*\\)) est supérieure à \\(q_{1-\\alpha,k}\\), le \\((1-\\alpha)\\)-quantile de la loi \\(\\chi^2(k)\\)\nZone de rejet : \\[\n\\mathcal R_\\alpha^{(k)}=\\left\\{Q_k^* &gt; q_{1-\\alpha,k}\\right\\}\n\\textrm{ (resp. }\n\\mathcal R_\\alpha^{(k)}=\\left\\{Q_k &gt; q_{1-\\alpha,k}\\right\\}).\n\\]\n\nRemarquons que l’on a un test pour chaque valeur de \\(k\\) !\n\nExample 4.4 Pour illustrer ces tests, on considère un bruit blanc gaussien. On simule une série de taille 300 et on utilise la fonction Box.test pour tester la blancheur de cette série.\n\nXt&lt;-rnorm(300,0,1)\n# Box.test(Xt,lag=1,type=\"Box-Pierce\")\nBox.test(Xt,lag=10,type=\"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  Xt\nX-squared = 14.214, df = 10, p-value = 0.1634\n\n\nOn ne rejette donc pas \\(\\mathcal H_0\\) pour \\(k=10\\). La Figure 4.5 représente les pvaleurs du même test pour \\(k\\) variant de 1 à 10. Sur la Figure 4.6, on observe que les autocorrélations empiriques sont proche de 0 pour tout \\(|h|&gt;0\\) donc proche du comportement théorique des autocorrélations d’un bruit blanc.\n\nBox.Ljung.Test(Xt)\nggacf(Xt)\n\n\n\n\n\n\nFigure 4.5: pvaleurs du test de Ljung-Box pour plusieurs valeurs de \\(k\\)\n\n\n\n\n\n\n\nFigure 4.6: Autocorrélations empiriques de la série de bruit blanc gaussien\n\n\n\n\n\n\n\n\n\n\n\nAragon, Yves. 2016. Séries Temporelles Avec r. EDP sciences.\n\n\nBrockwell, Peter J, and Richard A Davis. 2002. Introduction to Time Series and Forecasting. Springer.\n\n\n———. 2009. Time Series: Theory and Methods. Springer science & business media.\n\n\nDauxois, Jean-Yves. 2020. “Introduction à l’étude Des Séries Temporelles.” Polycopié cours INSA Toulouse."
  },
  {
    "objectID": "chap4.html#polynomes-et-séries-en-b",
    "href": "chap4.html#polynomes-et-séries-en-b",
    "title": "5  Les modèles ARMA",
    "section": "5.1 Polynomes et séries en B",
    "text": "5.1 Polynomes et séries en B\n\n5.1.1 Définitions\nSoit \\((X_t)_{t\\in \\Z}\\) un processus stationnaire du second ordre. Rappelons que l’opérateur retard \\(B\\) est défini par \\(BX_t=X_{t-1}\\) et l’opérateur avance \\(F\\) par \\(FX_t=X_{t+1}\\). Par composition, on a alors que \\(B^kX_t=X_{t-k}\\) et \\(F^kX_t=X_{t+k}\\). Puis par extension, un polynôme en \\(B\\) (ou \\(F\\)) est défini par \\[\n\\left(\\sum_{i=1}^p a_iB^i\\right)X_t=\\sum_{i=1}^p a_iX_{t-i}.\n\\]\nOn a déjà utilisé ce type d’opérateur pour définir les moyennes mobiles (voir Chapter 2). Nous avons également considéré la notion de série en \\(B\\) quand nous avons introduit la notion de filtrage linéaire (voir Chapter 4). Rappelons que nous avons vu que si \\((X_t)_{t\\in \\Z}\\) est un processus stationnaire, alors le processus \\((Y_t)_{t\\in \\Z}\\) défini par \\[Y_t=\\sum_{i\\in \\Z}a_iX_{t-i}\\] est également stationnaire si \\(\\sum_{i\\in \\Z}|a_i|&lt;+\\infty\\).\n\nDefinition 5.1 Soit \\((a_i)_{i\\in \\Z}\\) une famille absolument sommable de réels \\(\\left(\\sum_{i\\in \\Z}|a_i|&lt;+\\infty\\right)\\).  On appelle série en \\(B\\) de coefficients \\((a_i)_{i\\in \\Z}\\) l’opérateur \\[\nP(B)=\\sum_{i\\in \\Z}a_iB^i\n\\] sur les processus stationnaires qui transforme le processus stationnaire \\((X_t)_{t\\in \\Z}\\) en un processus stationnaire \\((Y_t)_{t\\in \\Z}\\) défini par , pour tout \\(t\\in \\Z\\), par \\[\nY_t=P(B)X_t=\\left(\\sum_{i\\in \\Z}a_iB^i\\right)X_t=\\sum_{i\\in \\Z}a_iX_{t-i},\\ \\forall t\\in\\Z\n\\]\n\nLa manipulation des séries en \\(B\\) se fait comme celle des séries réelles, en particulier pour les combinaisons linéaires et composées de séries.\n\nProposition 5.1 On a les résultats suivants :\n\nLa combinaison linéaire de deux séries en \\(B\\) est une série en \\(B\\).\nLa composée de deux séries en \\(B\\) est une série en \\(B\\).\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nSoient \\(P(B)=\\sum_{i\\in \\Z}a_iB^i\\) et \\(Q(B)=\\sum_{i\\in \\Z}b_iB^i\\) deux séries en \\(B\\), donc telles que \\(\\sum_{i\\in \\Z}|a_i|&lt;+\\infty\\) et \\(\\sum_{i\\in \\Z}|b_i|&lt;+\\infty\\). Considérons l’opérateur \\(\\lambda P(B)+Q(B)\\) et appliquons le à un processus stationnaire \\((X_t)_{t\\in \\Z}\\). On obtient, pour tout \\(t\\in \\Z\\) : \\[\\begin{eqnarray*}\n\\left(\\lambda P(B)+Q(B)\\right)X_t\n&=&\\lambda\nP(B)X_t+Q(B)X_t=\\sum_{i\\in \\Z}\\lambda a_iX_{t-i}+\\sum_{i\\in \\Z}b_iX_{t-i}\\\\\n&=&\\lim_{n\\to +\\infty}\\sum_{i=-n}^{i=+n}\\lambda a_iX_{t-i}+\\lim_{n\\to\n+\\infty}\\sum_{i=-n}^{i=+n}b_iX_{t-i}\\\\\n&=&\\lim_{n\\to\n+\\infty}\\sum_{i=-n}^{i=+n}\\left(\\lambda a_iX_{t-i}+b_iX_{t-i}\\right)=\\sum_{i\\in\n\\Z}(\\lambda a_i+b_i)X_{t-i},\n\\end{eqnarray*}\\] où la dernière égalité est justifiée par l’absolue convergence de la série \\(\\sum(\\lambda a_i+b_i)\\) garantie par l’inégalité \\[\n\\sum_{i\\in \\Z} |\\lambda a_i+b_i|\\leq \\lambda\\sum_{i\\in \\Z}|\na_i|+\\sum_{i\\in \\Z}|b_i|&lt;+\\infty.\n\\] On a bien alors \\[\n\\lambda\\sum_{i\\in \\Z}\na_iB^i+\\sum_{i\\in \\Z} b_iB^i=\\sum_{i\\in \\Z}(\\lambda a_i+b_i)B^i.\n\\] Considérons maintenant la composition des deux séries \\(P(B)\\) et \\(Q(B)\\). On a \\[\nP(B)\\circ Q(B)X_t=P(B)Y_t,\n\\] où \\[\nY_t=\\sum_{j\\in \\Z} b_jX_{t-j}.\n\\] On a donc \\[\\begin{eqnarray*}\nP(B)\\circ Q(B)X_t&=&\\sum_{i\\in \\Z} a_iY_{t-i}=\\lim_{n\\to\n+\\infty}\\sum_{i=-n}^na_iY_{t-i}\\\\ &=&\\lim_{n\\to\n+\\infty}\\sum_{i=-n}^na_i\\left(\\lim_{m\\to +\\infty}\\sum_{j=-m}^mb_j\nX_{t-i-j}\\right)\\\\ &=&\\lim_{n\\to +\\infty}\\lim_{m\\to\n+\\infty}\\sum_{i=-n}^n\\sum_{j=-m}^ma_ib_j X_{t-i-j}\\\\ &=&\\lim_{n\\to\n+\\infty}\\lim_{m\\to\n+\\infty}\\sum_{k=-n-m}^{n+m}\\left(\\sum_{i=\\max(-n,k-m)}^{\\min(n,k+m)}a_ib_{k-i}\\right)X_{t-k},\n\\end{eqnarray*}\\] où la dernière égalité est obtenue en faisant un changement d’indice de \\(j\\) à \\(k=i+j\\). On sait que les séries \\(\\sum a_i\\) et \\(\\sum b_j\\) étant absolument sommables, la série produit de Cauchy (appelé aussi parfois convoluée) \\(\\sum c_k\\) de ces deux suites, où \\[\nc_k=\\sum_{i\\in \\Z}a_ib_{k-i},\n\\] pour \\(k\\in \\Z\\), l’est aussi. On a donc : \\[\nP(B)\\circ Q(B)X_t=\\sum_{k\\in \\Z}c_kB^k,\n\\] ce qui montre que la composition des deux séries en \\(B\\) est bien une série en \\(B\\).\n\n\n\n\n\n5.1.2 Inversion de \\(I-\\lambda B\\)\nDans l’étude des modèles ARMA, on va avoir besoin de savoir quand et comment on peut inverser des polynômes en \\(B\\). On va donc ici commencer par le polynôme de degré 1 en \\(B\\) : \\(I-\\lambda B\\).\nL’opérateur \\(P(B)=I-\\lambda B\\) associe à tout processus stationnaire \\((X_t)_{t\\in \\Z}\\) un nouveau processus stationnaire \\((Y_t)_{t\\in \\Z}\\) tel que \\[\nY_t=(I-\\lambda B)X_t=X_t-\\lambda X_{t-1}.\n\\] Se donnant un processus stationnaire \\((Y_t)_{t\\in \\Z}\\), le problème de l’inversion du polynôme \\(I-\\lambda B\\) revient à déterminer s’il existe un processus stationnaire \\((X_t)_{t\\in \\Z}\\) vérifiant : \\[\nY_t=(I-\\lambda B)X_t=X_t-\\lambda X_{t-1}\n\\] et de donner son expression en fonction du processus \\((Y_t)_{t\\in \\Z}\\).\nOn va montrer dans la suite que ce polynôme \\(P(B)=I-\\lambda B\\) est inversible seulement si \\(|\\lambda|\\neq 1\\) et on obtiendra dans ce cas l’expression de son inverse.\n\n5.1.2.1 1er Cas : \\(|\\lambda|&lt;1\\)\n\nUne solution :\n\nSoit la suite \\((a_i)_{i\\in \\Z}\\) définie par \\(a_i=0\\) pour \\(i\\in\\mathbb Z \\setminus \\mathbb N\\) et \\(a_i=\\lambda^i\\) pour \\(i\\in \\N\\). On a alors \\[\n\\sum_{i\\in\\Z}|a_i|=\\sum_{i\\in \\N}|\\lambda|^i&lt;+\\infty.\n\\]\nLa série en \\(B\\) de coefficients \\((a_i)_{i\\in \\Z}\\) est donc bien définie et on peut écrire \\[\\begin{eqnarray*}\n(I-\\lambda B)\\left(\\sum_{i\\in \\Z}a_iB^i\\right)\n&=&(I-\\lambda B)\\left(\\sum_{i\\in\\N}\\lambda^iB^i\\right)\\\\\n&=&\\sum_{i\\in \\N}\\lambda^iB^i-\\sum_{i\\in \\N}\\lambda^{i+1}B^{i+1}=I.\n\\end{eqnarray*}\\]\nAinsi, pour un processus stationnaire \\((Y_t)_{t\\in \\Z}\\) donné, le processus \\((X_t)_{t\\in \\Z}\\) défini par \\(X_t=\\underset{i=0}{\\stackrel{+\\infty}{\\sum}}\\lambda^iY_{t-i}\\) est solution de l’équation \\[\nY_t=(I-\\lambda B)X_t.\n\\]\n\nNon unicité :\n\n\\((X_t)_{t\\in\\Z}\\) avec \\(X_t=\\sum_{i=0}^{+\\infty}\\lambda^iY_{t-i}\\) n’est pas l’unique solution. En effet, soit \\((X^*_t)_{t\\in \\Z}\\) tel que \\((I-\\lambda B)X_t^*=0\\) et définissons le processus \\((\\tilde X_t)_{t\\in \\Z}\\) par \\(\\tilde X_t=X_t+X_t^*\\). Alors on a que \\(Y_t=(I-\\lambda B)\\tilde X_t\\).\nUn processus \\((X^*_t)_{t\\in \\Z}\\) vérifiant \\((I-\\lambda B)X_t^* = X_t^*-\\lambda X_{t-1}^*=0\\) est de la forme \\(X^*_t=\\lambda^t A\\), pour tout \\(t\\in\\Z\\), où \\(A\\) est une v.a.r. de \\(L^2\\). Ainsi, étant donné un processus \\((Y_t)_{t\\in \\Z}\\), les solutions de l’équation \\(Y_t=(I-\\lambda B)X_t,\\ \\forall t\\in\\Z\\) sont de la forme \\[\n\\tilde X_t=X_t+X_t^*=\\sum_{i=0}^{+\\infty}\\lambda^iY_{t-i}+\\lambda^t A,\\ t\\in\\Z\n\\]\nMais le processus \\((X^*_t)_{t\\in \\Z}\\) n’est pas stationnaire !\n\nUnique solution stationnaire :\n\nEtant donné le processus \\((Y_t)_{t\\in \\Z}\\), la solution stationnaire de l’équation \\(Y_t=(I-\\lambda B)X_t\\) est le processus \\((X_t)_{t\\in \\Z}\\) défini par \\[\nX_t=\\sum_{i=0}^{+\\infty}\\lambda^iY_{t-i}.\n\\]\n\nProposition 5.2 L’opérateur \\((I-\\lambda B)\\) est inversible dans la classe des processus stationnaires d’inverse \\(\\underset{i=0}{\\stackrel{+\\infty}{\\sum}}\\lambda^iB^i\\) quand \\(|\\lambda|&lt;1\\).\n\n\n\n5.1.2.2 2ème Cas : \\(|\\lambda|&gt;1\\)\nOn peut écrire \\[\nI-\\lambda B=-\\lambda B\\left(I-\\frac{1}{\\lambda}F\\right)\n\\] Comme \\(|\\frac 1 \\lambda|&lt;1\\), par le même raisonnement que précédent, le polynôme \\(\\left(I-\\frac{1}{\\lambda}F\\right)\\) est inversible dans la classe des processus stationnaires d’inverse \\(\\sum_{i=0}^{+\\infty}\\lambda^{-i}F^i\\). Le processus \\(-\\lambda B\\) est inversible d’inverse \\((-1/\\lambda)F\\).\n\nProposition 5.3 Quand \\(|\\lambda|&gt;1\\), l’e processus’opérateur \\(I-\\lambda B\\) est inversible dans la classe des processus stationnaires d’inverse \\[\n\\left(\\frac{-1}{\\lambda}F\\right)\\left(\\sum_{i=0}^{+\\infty}\\frac{1}{\\lambda^i}F^i\\right)\n=-\\sum_{i=0}^{+\\infty}\\frac{1}{\\lambda^{i+1}}F^{i+1}=-\\sum_{j=1}^{+\\infty}\\frac{1}{\\lambda^j}F^j\n\\]\nEtant donné un processus stationnaire \\((Y_t)_{t\\in \\Z}\\), l’unique solution stationnaire de l’équation \\(Y_t=(I-\\lambda B)X_t\\) est donc \\((X_t)_{t\\in \\Z}\\) défini par \\[\nX_t=-\\sum_{j=1}^{+\\infty}\\frac{1}{\\lambda^j}Y_{t+j}.\n\\]\n\n\n\n\n\n\n\nRemarque\n\n\n\nDans le cas \\(|\\lambda|&gt;1\\), l’expression de la solution \\((X_t)_{t\\in \\Z}\\) est fonction du futur du processus \\((Y_t)_{t\\in \\Z}\\) et non en fonction de son passé comme dans le cas \\(|\\lambda|&lt;1\\).\n\n\n\n\n5.1.2.3 3ème Cas : \\(|\\lambda|=1\\)\n\nProposition 5.4 Si \\(|\\lambda|=1\\), l’opérateur \\(I-\\lambda B\\) n’est pas inversible dans la classe des processus stationnaires.\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\n\nSoit \\((X_t)_{t\\in \\Z}\\) le processus constant \\(X_t=m,\\ \\forall t\\in\\Z\\). On a alors \\((I- B)X_t= m-m=0\\) ce qui prouve que l’opérateur n’est pas injectif et donc pas bijectif.\nOn peut aussi montrer que \\((I-B)\\) n’est pas surjectif :\n\nsi le processus constant \\((Y_t)_{t\\in\\Z}\\) égal à \\(m\\neq 0\\) avait un antécédent \\((X_t)_{t\\in \\Z}\\) stationnaire, ce dernier serait tel que \\(X_t-X_{t-1}=m,\\ \\forall t\\in\\Z\\). On arrive alors à une contradiction car \\(\\E [X_t-X_{t-1}]=0\\neq m\\).\n\n\n\n\n\n\n5.1.3 Inverse d’un polynôme en \\(B\\)\nOn considère maintenant un polynôme en \\(B\\) de la forme \\[\n    \\Phi(B)=I-\\varphi_1B-\\varphi_2B^2-\\cdots-\\varphi_pB^p.\n\\] et on répondre à la question suivante : \\(\\Phi(B)\\) est-il inversible ? Se donnant un processus \\((Y_t)_{t\\in \\Z}\\), existe-t-il un unique processus stationnaire \\((X_t)_{t\\in \\Z}\\) tel que \\[\n\\Phi(B)X_t=Y_t,\\ \\forall t\\in\\Z.\n\\] Dans les complexes, le polynôme \\(\\Phi(z)\\) a \\(p\\) racines non nécessairement distinctes. Comme il est non constant, ce polynome est scindé. On peut donc alors utiliser les résultats du polynôme \\(I-\\lambda B\\). On a alors la discussion de cas suivante :\n\nSi au moins une des racines est de module 1, alors il n’existe pas de processus stationnaire \\((X_t)_{t\\in \\Z}\\) solution de l’équation \\(\\Phi(B)X_t=Y_t\\).\nSi toutes les racines sont de module différent de 1, alors il existe une série en \\(B\\), notée \\(\\Psi(B)=\\sum \\psi_iB^i\\), telle que :\n\n\\(\\Phi(B)\\Psi(B)=I\\)\n\\((X_t)_{t\\in \\Z}\\), solution de \\(\\Phi(B)X_t=Y_t\\), est stationnaire.\n\nSi toutes les racines sont à l’extérieur du disque unité, alors l’inverse est une série en puissances positives de \\(B\\) uniquement \\[\n\\Psi(B)=\\sum_{i\\in \\N} \\psi_iB^i\n\\]\nSi toutes les racines sont à l’intérieur du disque unité, alors l’inverse est une série en puissances strictement positives de \\(F\\) uniquement \\[\n\\Psi(B)=\\sum_{i\\in \\N^*} \\psi_iF^i\n\\]\n\n\n5.1.3.1 Comment déterminer l’inverse de \\(\\Phi(B)\\) ?\nSi l’inverse du polynôme \\(\\Phi(B)\\) existe, on peut le déterminer par l’une de ces méthodes\n\nIdentification :\n\nPar exemple quand toutes les racines sont à l’extérieur du disque unité, on écrit \\[\n\\left(1-\\varphi_1z-\\varphi_2z^2-\\cdots-\\varphi_pz^p\\right)  \\left(\\sum_{i\\in\\N}\\psi_iz^i\\right)=1\n\\] dont on tire des équations donnant les expressions des coefficients \\(\\psi_i\\) en fonction des \\(\\varphi_i\\).\n\nDécomposition en éléments simples de \\(\\Phi(z)\\) : On écrit \\[\\begin{eqnarray*} \\frac{1}{\\Phi(z)}&=&\\frac{1}{\\prod_{i=1}^p (1-\\lambda_iz)}\\\\\n&=&\\sum_{i=1}^p\\frac{a_i}{1-\\lambda_iz}\\\\ &=&\\sum_{i=1}^pa_i \\sum_{j\\in\n\\Z}\\lambda_i^jz^j\\\\ &=&\\sum_{j\\in \\Z}\\left(\\sum_{i=1}^pa_i\n\\lambda_i^j\\right)z^j=\\Psi(z).\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "chap4.html#sec-chap4AR",
    "href": "chap4.html#sec-chap4AR",
    "title": "5  Les modèles ARMA",
    "section": "5.2 Processus AR",
    "text": "5.2 Processus AR\n\n5.2.1 Définition\nDans cette partie, on se restreint à des processus centrés sans perte de généralités\nDans de nombreuses situations pratiques la valeur en un instant \\(t\\) d’une série temporelle peut s’écrire comme la somme d’une combinaison linéaire des valeurs précédentes de la série et d’un terme de bruit. Un tel modèle est connu sous le nom d’un processus AR (AutoRegressive).\n\nDefinition 5.2 (AR(p))  On dit qu’un processus stationnaire \\((X_t)_{t\\in \\Z}\\) admet une représentation auto-régressive d’ordre \\(p\\) (noté AR(p)) s’il vérifie l’équation récurrente : \\[\nX_t-\\varphi_1X_{t-1}-\\varphi_2X_{t-2}-\\cdots-\\varphi_pX_{t-p}=\\varepsilon_t,\n\\] où \\((\\varepsilon_t)_{t\\in \\Z}\\sim\\text{WN}(0,\\sigma^2)\\) et \\((\\varphi_1,\\ldots,\\varphi_p)\\in\\mathbb R^p\\) avec \\(\\varphi_p\\neq0\\).\nRéécriture :  Un processus AR(p) vérifie l’équation : \\(\\Phi(B)X_t=\\varepsilon_t,\\ \\forall t\\in\\Z\\) avec le polynome en \\(B\\) \\[\n\\Phi(B)=I-\\varphi_1 B-\\varphi_2 B^2-\\cdots-\\varphi_p B^p.\n\\]\nCette représentation est dite canonique si le bruit blanc \\((\\varepsilon_t)_{t\\in \\Z}\\) est tel que \\(\\varepsilon_t\\perp\\mathcal H_{-\\infty}^{t-1}(X)\\) pour tout \\(t\\in\\Z\\).\n\n\n\n\n\n\n\nRemarque\n\n\n\nComme un processus AR doit être stationnaire par déf., les racines du polynôme \\(\\Phi(z)\\) doivent toutes être de module différent de 1 !\nSoit un processus AR(1) vérifiant \\[\nX_t-\\varphi X_{t-1}=\\varepsilon_t \\textrm{ avec }\\varphi=\\pm 1.\n\\] Par itération, on a \\(X_t=\\varphi^tX_0+\\sum_{j=0}^{t-1}\\varphi^j\\varepsilon_{t-j}\\). \\[\n\\V(X_t)+\\V(\\varphi^tX_0)-2\\varphi^t\\C(X_t,X_0)=\\sum_{j=0}^{t-1}\\varphi^{2j}\\V(\\varepsilon_{t-j}).\n\\] S’il existe un processus stationnaire \\((X_t)_{t\\in \\Z}\\) vérifiant cette équation, on doit donc avoir  \\(2- 2 \\varphi^t\\rho_X(t)=\\frac{t\\sigma^2}{\\gamma_X(0)}.\\) On aboutit à une contradiction en faisant tendre \\(t\\) vers \\(+\\infty\\). \n\n\n\n\n5.2.2 Un exemple instructif : le processus AR(1)\nOn considère \\((X_t)_{t\\in \\Z}\\) un processus AR(1), vérifiant donc l’équation de récurrence \\[\nX_t-\\varphi X_{t-1}=\\varepsilon_t,\\ \\forall t\\in\\Z\n\\] où \\((\\varepsilon_t)_{t\\in \\Z}\\) est un bruit blanc \\(\\text{WN}(0,\\sigma^2)\\) et \\(\\varphi\\neq 0\\).\nD’après l’étude précédente sur l’inversibilité de \\(I-\\varphi B\\), trois cas se présentent suivant les valeurs du paramètre \\(\\varphi\\).\n\n5.2.2.1 Cas \\(|\\varphi|=1\\)\nOn a vu précédemment que dans ce cas, il n’existe pas de processus stationnaire \\((X_t)_{t\\in \\Z}\\) vérifiant l’équation \\((I-\\varphi B)X_t=\\varepsilon_t\\). Il n’existe donc pas de processus AR(1) de paramètre 1 ou -1.\n\n\n5.2.2.2 Cas \\(|\\varphi|&lt;1\\)\n\nEcriture de \\(X_t\\) en fonction du bruit blanc :\n\nPour \\(|\\varphi|&lt;1\\), l’inversibilité de \\(I-\\varphi B\\) est assurée dans la classe des processus stationnaires et \\[\n(I-\\varphi B)^{-1}=\\sum_{i=0}^{+\\infty}\\varphi^iB^i.\n\\]\nAinsi le processus AR(1) pour \\(|\\varphi|&lt;1\\) s’écrit, en fonction du bruit blanc \\((\\varepsilon_t)_{t\\in \\Z}\\), sous la forme : \\[\nX_t=\\sum_{i=0}^{+\\infty}\\varphi^i\\varepsilon_{t-i},\\ \\forall t\\in\\Z.\n\\]\nDe cette écriture, on en déduit que \\(X_t\\in\\mathcal H_{-\\infty}^t(\\varepsilon)\\) et donc \\(\\mathcal H_{-\\infty}^t(X)\\subset \\mathcal H_{-\\infty}^t(\\varepsilon)\\) pour tout \\(t\\) dans \\(\\Z\\). De plus, par propriété des bruits blancs \\(\\varepsilon_t\\perp \\mathcal H_{-\\infty}^{t-1}(\\varepsilon)\\). Ainsi \\(\\varepsilon_t \\perp \\mathcal H_{-\\infty}^{t-1}(X)\\), donc cette représentation est canonique.\n\nPrévision linéaire optimale :\n\nAyant observé \\(X_1,\\ldots,X_n\\), on prédit la valeur suivante de la série \\(X_{n+1}\\) par \\[\n\\hat X_n(1)=\\EL(X_{n+1}|\\mathcal\nH_{-\\infty}^n(X))=\\EL(\\varphi X_n +\\varepsilon_{n+1}|\\mathcal\nH_{-\\infty}^n(X))=\\varphi X_n\n\\] car \\(\\varepsilon_{n+1}\\perp\\mathcal H_{-\\infty}^n(X)\\).\n\nProcessus des innovations :\n\nDans ce cas, \\[\nX_{t}-\\hat X_{t-1}(1)=X_{t}-\\varphi X_{t-1} = \\varepsilon_{t},\\ \\forall t\\in\\Z\n\\] C’est donc le bruit blanc \\((\\varepsilon_t)_{t\\in \\Z}\\) dans la représentation AR(1). On verra que ce résultat reste valable pour tout modèle AR de représentation canonique.\n\n\n5.2.2.3 Cas \\(|\\varphi|&gt;1\\)\n\nEcriture (non canonique) de \\(X_t\\) selon le bruit blanc :\n\nD’après l’étude précédente, le polynôme \\(I-\\varphi B\\) est inversible d’inverse \\[\n    (I-\\varphi B)^{-1}=-\\sum_{i=1}^{+\\infty}\\frac{1}{\\varphi^i}F^i \\textrm{  d'où  }\n    X_t=-\\sum_{i=1}^{+\\infty}\\frac{1}{\\varphi^i}\\varepsilon_{t+i}.\n\\]\nCette représentation du processus AR(1) n’est pas canonique car\n\\[\\begin{eqnarray*}\n\\C(\\varepsilon_t,X_{t-1})&=&\\C\\left(\\varepsilon_t,-\\sum_{i=1}^{+\\infty}\\frac{1}{\\varphi^i}\\varepsilon_{t-1+i}\\right)\\\\\n&=& -\\frac{1}{\\varphi}\\V(\\varepsilon_t)= -\\frac{\\sigma^2}{\\varphi}\\neq 0.\n\\end{eqnarray*}\\]\n\nPrévision linéaire optimale :\n\nAyant observé \\(X_1,\\ldots,X_n\\), on prédit la valeur suivante de la série \\(X_{n+1}\\) par \\[\\begin{eqnarray*}\n\\hat X_n(1)&=&\\EL(X_{n+1}|\\mathcal\nH_{-\\infty}^n(X))\\\\ &=&\\EL(\\varphi X_n +\\varepsilon_{n+1}|\\mathcal\nH_{-\\infty}^n(X))\\\\ &=&\\varphi X_n+\\EL(\\varepsilon_{n+1}|\\mathcal\nH_{-\\infty}^n(X))\n\\end{eqnarray*}\\] le dernier terme n’ayant aucune raison d’être nul, puisque la représentation n’est pas canonique.\n\nProcessus des innovations : \\[\\begin{eqnarray*}\nX_{t+1}-\\hat X_t(1)\n&=&\\varphi X_t+\\varepsilon_{t+1}-\\varphi X_t-\\EL(\\varepsilon_{t+1}|\\mathcal H_{-\\infty}^t(X))\\\\ &=&\\varepsilon_{t+1}-\\EL(\\varepsilon_{t+1}|\\mathcal H_{-\\infty}^t(X))\n\\end{eqnarray*}\\] donc non confondus avec le bruit blanc utilisé dans la représentation (non canonique) du processus AR(1).\nVers une représentation canonique :\n\nOn va ici montrer que le processus AR(1) pour \\(|\\varphi|&gt;1\\) possède un autre représentation, qui est canonique. Pour cela, on utilise la formule reliant les densités spectrales de deux processus stationnaires dont l’un est le filtrage linéaire d’un autre : partant de \\((I-\\varphi B)X_t=\\varepsilon_t\\), on a \\[\nf_X(\\omega)|1-\\varphi e^{-i\\omega}|^2=f_\\varepsilon(\\omega)=\\frac{\\sigma^2}{2\\pi} \\Longleftrightarrow\nf_X(\\omega)=\\frac{\\sigma^2}{2\\pi}\\frac{1}{|1-\\varphi e^{-i\\omega}|^2}.\n\\]\nSoit le processus \\((\\eta_t)_{t\\in \\Z}\\) défini par \\(\\eta_t=X_t-\\frac{1}{\\varphi}X_{t-1},\\ \\forall t \\in \\Z\\). Alors \\[\\begin{eqnarray*}\n    f_\\eta(\\omega)&=&\\textcolor{magenta}{f_X(\\omega)} \\left|1-\\frac{1}{\\varphi}\n    e^{-i\\omega}\\right|^2\n    =\\textcolor{magenta}{\\frac{\\sigma^2}{2\\pi}}\\frac{|1-\\frac{1}{\\varphi}\n    e^{-i\\omega}|^2}{\\textcolor{magenta}{|1-\\varphi e^{-i\\omega}|^2}}\\\\\n    &=&\\frac{\\sigma^2}{2\\pi\\varphi^2}\\frac{|1-\\frac{1}{\\varphi}\n    e^{-i\\omega}|^2}{|\\frac{1}{\\varphi}- e^{-i\\omega}|^2} =\n    \\frac{\\sigma^2}{2\\pi\\varphi^2} \\frac{|1-\\frac{1}{\\varphi}\n    e^{-i\\omega}|^2}{|\\frac{1}{\\varphi}e^{i\\omega}-1|^2}\n    =\\frac{\\sigma^2}{2\\pi\\varphi^2}.\n    \\end{eqnarray*}\\]\nOn a donc une nouvelle représentation AR(1) pour le processus \\((X_t)_{t\\in \\Z}\\) : \\[\n(I-\\frac{1}{\\varphi} B)X_t=\\eta_t \\textrm{ où } (\\eta_t)_{t\\in \\Z}\\sim\n\\text{WN}(0,\\sigma^2/\\varphi^2)\n\\] et la représentation est canonique car \\(|1/\\varphi|&lt;1\\). On a aussi que le processus \\((\\eta_t)_{t\\in \\Z}\\) correspond au processus des innovations.\nCette étude du cas particulier du processus AR(1) nous a donc permis de voir que la représentation AR(1) n’est pas unique. En choisissant celle avec le coefficient \\(\\varphi\\) ou \\(1/\\varphi\\) de module inférieur à 1, on a la représentation canonique. Ce résultat se généralise aux processus AR d’ordre quelconque.\n\n\n\n5.2.3 Propriétés\nLa proposition suivante généralise les résultats vus sur la surjectivité et l’inversibilité du polynôme en \\(I-\\lambda B\\) et décrit le processus AR en fonction du bruit blanc.\n\nProposition 5.5 Soit \\((\\varepsilon_t)_{t\\in \\Z}\\) un bruit blanc et \\(\\Phi(B)\\) un polynôme en \\(B\\).\n\nIl existe une infinité de processus du second ordre \\((X_t)_{t\\in \\Z}\\) vérifiant : \\[\n\\Phi(B)X_t=\\varepsilon_t, \\text{ pour tout }t\\in \\Z.\n\\]\nSi les racines du polynôme \\(\\Phi(B)\\) sont toutes de module différent de 1, il existe une unique solution stationnaire \\((X_t)_{t\\in \\Z}\\). Elle s’écrit alors sous la forme d’un filtrage linéaire du bruit blanc \\((\\varepsilon_t)_{t\\in \\Z}\\) : \\[\nX_t=\\Psi(B)\\varepsilon_t=\\sum_{i\\in\\Z}\\psi_i\\varepsilon_{t-i},\\ \\forall t\\in \\Z.\n\\]\nSi les racines du polynômes sont toutes à l’extérieur du disque unité, alors l’écriture moyenne mobile du processus AR(p) ne considère que les valeurs passées du bruit blanc : \\[\nX_t=\\Psi(B)\\varepsilon_t=\\sum_{i\\in\\textcolor{red}{\\N}}\\psi_i\\varepsilon_{t-i},\\ \\forall t\\in \\Z.\n\\]\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nPour s’en convaincre, il suffit de factoriser le polynôme \\(\\Phi(B)\\) de degré \\(p\\) en fonction de ses \\(p\\) racines et d’appliquer \\(p\\) fois le résultat d’inversibilité vu pour le polynôme \\(I-\\lambda B\\).\n\n\n\nComme vu dans le cas du processus AR(1), il n’existe pas qu’une seule représentation d’un processus AR(\\(p\\)). En revanche une seule de ces représentations est canonique.\n\nProposition 5.6 Soit \\((\\varepsilon_t)_{t\\in \\Z}\\) un bruit blanc et \\(\\Phi(B)\\) un polynôme en \\(B\\) de degré \\(p\\) exactement dont toutes les racines \\((z_i = 1/ \\lambda_i)_{i=1,\\ldots,p}\\) (non néc. distinctes) sont de module différent de 1.\nSoit \\((X_t)_{t\\in \\Z}\\) l’unique processus stationnaire AR(p) vérifiant \\[\n\\Phi(B)X_t=\\varepsilon_t, \\text{ pour tout }t\\in \\Z.\n\\]\n\nLe processus \\((X_t)_{t\\in \\Z}\\) satisfait plusieurs représentations AR(p) différentes : soit \\(\\tilde \\Phi(B)\\) un nouveau polynôme en \\(B\\) ayant pour racines celles de \\(\\Phi(B)\\) ou leurs inverses, le processus \\((X_t)_{t\\in \\Z}\\) admet aussi la représentation : \\[ \\tilde \\Phi(B)X_t=\\eta_t,\n\\text{ pour tout }t\\in \\Z, \\textrm{ où }(\\eta_t)_{t\\in \\Z} \\textrm{un bruit blanc}.\n\\]\nToutes les représentations ont même ordre.\nUne seule représentation est canonique : \\[\n\\tilde \\Phi(B)X_t=\\eta_t,\\ \\forall t\\in \\Z,\n\\] où \\[\n\\tilde\\Phi(B)=\\prod_{i:|z_i|&gt;1}\\left(I-\\lambda_iB\\right)\\prod_{i:|z_i|&lt;1}\\left(I-\\frac{1}{\\lambda_i}B\\right).\n\\]\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nOn montre seulement le dernier point.  La démonstration est assez proche de ce que l’on a fait pour le processus AR(1) et utilise donc les densités spectrales. De l’équation \\[\n\\Phi(B)X_t=\\varepsilon_t, \\text{ pour tout }t\\in \\Z,\n\\] on tire \\[\nf_X(\\omega)\\left| \\Phi(e^{-i\\omega}) \\right|^2=f_\\varepsilon(\\omega).\n\\] L’équation \\[\n\\tilde \\Phi(B)X_t=\\eta_t, \\text{ pour tout }t\\in \\Z,\n\\] donne quant à elle \\[\nf_\\eta(\\omega)=f_X(\\omega)\\left| \\tilde\\Phi(e^{-i\\omega})\n\\right|^2.\n\\] En supposant toutes les racines de \\(\\Phi(z)\\) réelles, on peut refaire le même type de raisonnement que celui fait pour le AR(1) et obtenir \\[\\begin{eqnarray*} f_\\eta(\\omega)&=&\\frac{\\left| \\tilde\\Phi(e^{-i\\omega})\n\\right|^2}{\\left| \\Phi(e^{-i\\omega})\n\\right|^2}f_\\varepsilon(\\omega)=\\frac{\\sigma^2}{2\\pi}\\frac{\\left|\n\\prod_{j:|z_j|&gt;1}\\left(1-\\lambda_je^{-i\\omega}\\right)\\prod_{j:|z_j|&lt;1}\\left(1-\\frac{1}{\\lambda_j}e^{-i\\omega}\\right)\n\\right|^2}{\\left|    \\prod_{j=1}^p\\left(1-\\lambda_je^{-i\\omega}\\right)\n\\right|^2}\\\\ &=&\\frac{\\sigma^2}{2\\pi}\\frac{\\left|\n\\prod_{j:|z_j|&lt;1}\\left(1-\\frac{1}{\\lambda_j}e^{-i\\omega}\\right)\n\\right|^2}{\\left|    \\prod_{j:|z_j|&lt;1}\\left(1-\\lambda_je^{-i\\omega}\\right)\n\\right|^2}=\\frac{\\sigma^2}{2\\pi}\\frac{1}{\\prod_{j:|z_j|&lt;1} |\\lambda_j|^2}.\n\\end{eqnarray*}\\] Que se passe-t-il si une racine \\(z=1/\\lambda\\) n’est pas réelle ? Le polynôme \\(\\Phi(z)\\) étant à coefficient réel, son conjugué \\(\\bar z\\) est également racine. On peut alors écrire en ne considérant dans le ratio précédent que les termes correspondants à ces deux racines conjuguées : \\[\\begin{eqnarray*} \\frac{\\left|\n\\left(1-\\frac{1}{\\lambda}e^{-i\\omega}\\right)   \\left(1-\\frac{1}{\\bar\n\\lambda}e^{-i\\omega}\\right)  \\right|^2}{\\left|    \\left(1-\\lambda\ne^{-i\\omega}\\right)\\left(1-\\bar \\lambda e^{-i\\omega}\\right)\n\\right|^2}&=&\\frac{\\left| \\left( -\\frac{1}{\\lambda } e^{-i\\omega} \\right)\n\\left(-\\frac{1}{\\bar \\lambda}e^{-i\\omega}\\right)  \\left(1-\\lambda\ne^{i\\omega}\\right) \\left(1-\\bar \\lambda e^{i\\omega}\\right)  \\right|^2}{\\left|\n\\left(1-\\lambda e^{-i\\omega}\\right)\\left(1-\\bar \\lambda e^{-i\\omega}\\right)\n\\right|^2}\\\\ &=&\\frac{1}{|\\lambda|^4}\\text{ (puisque }\\overline{1-\\lambda\ne^{i\\omega}}=1-\\bar \\lambda e^{-i\\omega}) \\end{eqnarray*}\\] et on obtient le même résultat que dans le cas de racines réelles. On a donc prouvé que l’on avait, avec ce nouveau polynôme \\(\\tilde \\Phi(B)\\) obtenu en remplaçant racines à l’intérieur du disque unité par leur inverse, une nouvelle représentation AR(\\(p\\)) du processus \\((X_t)_{t\\in \\Z}\\) puisque \\((\\eta_t)_{t\\in \\Z}\\) est un bruit blanc. Il reste à prouver que la représentation obtenue est canonique. Le raisonnement suit de près ce que l’on a fait pour le cas particulier du processus \\(AR(1)\\). D’une part on montre l’égalité des histoires des processus \\((X_t)_{t\\in \\Z}\\) et \\((\\eta_t)_{t\\in \\Z}\\), i.e. \\[ \\H_{-\\infty}^t(X)=\n\\H_{-\\infty}^t(\\eta). \\] On dit dans ce cas que le processus est . C’est bien le cas ici puisque l’équation \\[ \\tilde \\Phi(B)X_t=\\eta_t, \\text{\npour tout }t\\in \\Z., \\] où \\(\\tilde \\Phi(B)\\) est un polynôme avec puissance positives de \\(B\\), assure la première inclusion~: \\(\\H_{-\\infty}^t(\\eta)\\subset \\H_{-\\infty}^t(X)\\). Mais comme \\(\\tilde \\Phi(B)\\) possède par construction toutes ses racines hors du disque unité, il est inversible d’inverse ne faisant intervenir que les puissances positives de \\(B\\) : \\[ X_t=\\sum_{i\\in \\N} \\tilde\n\\psi_i\\eta_{t-i}. \\] On en déduit l’inclusion inverse : \\(\\H_{-\\infty}^t(X)\\subset \\H_{-\\infty}^t(\\eta)\\) et donc l’égalité. La représentation canonique est alors bien acquise puisque, en tant que bruit blanc, le processus \\((\\eta_t)_{t\\in \\Z}\\) est évidemment orthongonal à son passé, donc ici également orthogonal au passé de \\((X_t)_{t\\in \\Z}\\).\n\n\n\n\nProposition 5.7 Si la représentation du processus AR(p) \\[\n\\Phi(B)X_t=\\varepsilon_t, \\text{ pour tout }t\\in \\Z,\n\\] est canonique, le bruit blanc \\((\\varepsilon_t)_{t\\in \\Z}\\) utilisé dans sa représentation est aussi le processus des innovations.\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nPar définition le processus des innovations est donné par \\[\\begin{eqnarray*}\nX_{t+1}-\\hat X_t(1)&=&X_{t+1}-\\EL\\left(\n\\varphi_1X_t+\\cdots+\\varphi_pX_{t+1-p}+\\varepsilon_{t+1}|\\H_{-\\infty}^t(X)\\right)\\\\\n&=&X_{t+1}-\\varphi_1X_t-\\cdots-\\varphi_pX_{t+1-p}+0 \\text{ (puisque }\n\\varepsilon_{t+1}\\perp \\H_{-\\infty}^t(X))\\\\ &=& \\varepsilon_{t+1},\n\\end{eqnarray*}\\] ce qui est bien le résultat annoncé.\n\n\n\n\nProposition 5.8 Tout processus AR(p) de représentation canonique \\[ \\Phi(B)X_t=\\varepsilon_t, \\text{ pour tout }t\\in \\Z, \\] admet la représentation MA(\\(\\infty\\)) : \\[\nX_t=\\varepsilon_t +\\sum_{i=1}^{+\\infty}\n\\psi_i\\varepsilon_{t-i}.\n\\]\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nNous avons déjà obtenu les éléments de cette proposition. La seule nouveauté, que l’on aurait pu noter plus tôt, est l’égalité à 1 du premier coefficient \\(\\psi_0\\) dans l’inverse du polynôme \\(\\Phi (z)\\). Ceci se voit facilement par identification du terme constant.\n\n\n\n\n\n5.2.4 Liaisons temporelles\nSoit un processus \\((X_t)_{t\\in \\Z}\\) de représentation AR(\\(p\\)) canonique : \\[\n\\Phi(B)X_t=\\varepsilon_t,\\ \\forall t\\in \\Z.\n\\]\n\nVariance du processus \\((X_t)_{t\\in \\Z}\\) :\n\\[\\begin{eqnarray*}\n\\gamma_X(0)&=&\\V(X_t)=\\C\\left(\\sum_{i=1}^p\\varphi_iX_{t-i}+\\varepsilon_t,X_t\\right)\\\\\n&=&\\sum_{i=1}^p\\varphi_i \\C(X_{t-i},X_t)+\\C(\\varepsilon_t,X_t)\\\\\n&=&\\sum_{i=1}^p\\varphi_i\n\\gamma_X(i)+\\C\\left(\\varepsilon_t,\\sum_{i=1}^p\\varphi_iX_{t-i}+\\varepsilon_t\\right)\\\\\n&=&\\sum_{i=1}^p \\varphi_i\n\\underbrace{\\gamma_X(i)}_{\\gamma_X(0)\\rho_X(i)}+\\V(\\varepsilon_t) \\textrm{ (car\ncanonique)}.\n\\end{eqnarray*}\\] où la dernière égalité est obtenue par orthogonalité du bruit blanc par rapport à l’histoire du processus AR(\\(p\\)) dans sa représentation canonique. On a alors \\[\n\\gamma_X(0)=\\frac{\\sigma^2}{1-\\sum_{i=1}^p\\varphi_i\\rho_X(i)}.\n\\]\nAutocovariances : pour \\(h&gt;0\\), \\[\\begin{eqnarray*}\n\\gamma_X(h)=\\C(X_t,X_{t+h})&=&\\C\\left(X_t,\\sum_{i=1}^p\\varphi_iX_{t+h-i}+\\varepsilon_{t+h}\\right)\\\\\n&=&\\sum_{i=1}^p\\varphi_i \\C(X_tX_{t+h-i})+\\C(X_t,\\varepsilon_{t+h})\\\\\n&=&\\sum_{i=1}^p\\varphi_i \\gamma_X(h-i) \\text{ car  }\\varepsilon_{t+h}\\perp\n\\mathcal H_{-\\infty}^{t+h-1}(X).\n\\end{eqnarray*}\\]\nAutocorrélations : elles satisfont l’équation linéaire récurrente d’ordre \\(p\\) \\[\n\\rho_X(h)=\\sum_{i=1}^p\\varphi_i \\rho_X(h-i),\\ \\forall h\\in \\N^*\n\\tag{5.1}\\]\n\nLes autocorrélations vérifient donc l’équation matricielle dite de :\n\\[\\begin{eqnarray*}\n\\left( \\begin{array}{c} \\rho_X(1) \\\\ \\vdots  \\\\\n\\rho_X(p) \\end{array} \\right)&=& \\underbrace{\\left( \\begin{array}{cccc} 1   &\n\\rho_X(1)  & \\cdots & \\rho_X(p-1)  \\\\ \\rho_X(1)  &1  & \\ddots &\\rho_X(p-2)  \\\\\n\\vdots  &  \\ddots &   \\ddots &\\vdots\\\\ \\rho_X(p-1) &\\cdots&\\rho_X(1)& 1\n\\end{array} \\right)}_{R_X(p)} \\left( \\begin{array}{c} \\varphi_1 \\\\ \\vdots  \\\\\n\\varphi_p \\end{array} \\right)\\\\\n\\\\\n\\text{ et }\\\\\n\\gamma_X(0)&=&\\frac{\\sigma^2}{1-\\underset{i=1}{\\stackrel{p}{\\sum}}\\varphi_i\\rho_X(i)}.\n\\end{eqnarray*}\\]\n\nProposition 5.9 (Autocorrélations simples d’un AR(p))  Les autocorrélations simples d’un processus AR(p) décroissent, de manière exponentielle ou sinusoïdale amortie, vers 0.\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nL’équation Equation 5.1 vérifiée par les autocorrélations d’un AR(\\(p\\)) est une équation de récurrence linéaire de polynôme caractéristique \\[ r^p-\\varphi_1r^{p-1}-\\ldots\n-\\varphi_{p-1}r-\\varphi_p=r^p\\Phi\\left(\\frac{1}{r}\\right). \\] Les racines (non nulles car \\(\\varphi_p\\) est non nul) de ce polynôme sont les inverses de celles du polynôme \\(\\Phi(z)\\) définissant l’AR(\\(p\\)). La solution de l’équation Equation 5.1 dans \\(\\mathbb C\\) est donc une combinaison linéaire de solutions de la forme \\[\n(a_{i,0}+a_{i,1}h+\\cdots+a_{i,k-1}h^{k-1})\\lambda_i^h,\n\\] où \\(k\\) est l’ordre de multiplicité de la racine \\(z_i=1/\\lambda_i\\) de \\(\\Phi(z)\\). Or, puisqu’on considère la représentation canonique, les racines \\(z_i\\) de \\(\\Phi(z)\\) sont de module à l’extérieur du disque unité. Ainsi, si toutes les solutions sont réelles alors les autocorrélations décroissent effectivement de manière exponentielle vers 0. Si une des racines \\(z_i=1/\\lambda_i\\) de \\(\\Phi(z)\\) n’est pas réelle, on sait alors que son conjugué l’est aussi et que les solutions réelles correspondant à ces deux racines conjuguées s’écrivent sous la forme \\[\n\\begin{array}{l l}\n|\\lambda_i|^h & \\left[(a_{i,0}+a_{i,1}h+\\cdots+a_{i,k-1}h^{k-1})\\cos(\\omega_i h) \\right.\\\\\n& \\left. +(b_{i,0}+b_{i,1}h+\\cdots+b_{i,k-1}h^{k-1})\\sin(\\omega_i h)\\right],\n\\end{array}\n\\] où \\(\\omega_i\\) est le module de \\(\\lambda_i\\). Elles sont donc bien simusoïdales amorties.\n\n\n\n\nExample 5.1 Pour un AR(1) d’équation canonique \\(X_t=\\varphi X_{t-1}+\\varepsilon_t,\\) on a \\(\\rho_X(h)=\\varphi^{|h|}.\\) On a bien la décroissance exponentielle des autocorrelations et on constate que le coefficient \\(\\varphi\\) d’un AR(1) est en fait la première autocovariance.\n\n\nExample 5.2 Soit le processus AR(2) défini par \\(X_t = 0.7 X_{t-1}-0.1 X_{t-2}+\\varepsilon_t\\). Les racines du polynome \\(\\Phi(z)=0.1 z^2 -0.7 z +1\\) sont \\(2\\) et \\(5\\). La Figure 5.1 montre la décroissante exponentielle des autocorrélations empiriques de ce processus.\n\n\n\n\n\nFigure 5.1: ACF empirique du processus AR(2) \\(X_t = 0.7 X_{t-1}-0.1 X_{t-2}+\\varepsilon_t\\)\n\n\n\n\nSoit le processus AR(2) défini par \\(X_t = \\frac{3}{4} X_{t-1} - \\frac{3}{16} X_{t-2}+\\varepsilon_t\\). Les racines du polynome \\(\\Phi(z)=\\frac{3}{16} z^2 - \\frac 3 4 z +1\\) sont \\(2\\pm i 2\\sqrt{3}/3\\). La Figure 5.2 montre les autocorrélations empiriques de ce processus.\n\n\n\n\n\nFigure 5.2: ACF empirique du processus AR(2) \\(X_t = \\frac{3}{4} X_{t-1} - \\frac{3}{16} X_{t-2}+\\varepsilon_t\\)\n\n\n\n\n\n\n\nProposition 5.10 Autocorrélations partielles d’un AR(p)\nLes autocorrélations partielles d’un processus AR(p) sont nulles à partir du rang \\(p+1\\) :\n\n\\(r_X(p)=\\varphi_p\\neq 0\\)\n\\(r_X(k)=0,\\ \\forall k&gt;p\\)\n\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nIl suffit d’écrire la définition de l’autocorrélation partielle. On sait que le coefficient \\(r_X(k)\\) est donné par le coefficient \\(\\alpha_k(k)\\) dans la projection de \\(X_t\\) sur \\(\\H_{t-k}^{t-1}\\), i.e. : \\[\n\\EL(X_t|\\H_{t-k}^{t-1})=\\alpha_1(k)X_{t-1}+\\cdots+\\alpha_k(k)X_{t-k}.\n\\] Ainsi, pour \\(k=p\\), on peut écrire \\[\\begin{eqnarray*}\n\\EL(X_t|\\H_{t-p}^{t-1})&=&\\EL(\\varphi_1X_{t-1}+\\varphi_2X_{t-2}+\\cdots+\\varphi_pX_{t-p}+\\varepsilon_t,|\\H_{t-p}^{t-1})\\\\\n&=&\\varphi_1X_{t-1}+\\varphi_2X_{t-2}+\\cdots+\\varphi_pX_{t-p}, \\end{eqnarray*}\\] dont on tire \\(r_X(p)=\\varphi_p\\neq0\\). Pour \\(k&gt;p\\), on a : \\[\\begin{eqnarray*}\n\\EL(X_t|\\H_{t-k}^{t-1})&=&\\EL(\\varphi_1X_{t-1}+\\varphi_2X_{t-2}+\\cdots+\\varphi_pX_{t-p}+\\varepsilon_t,|\\H_{t-k}^{t-1})\\\\\n&=&\\varphi_1X_{t-1}+\\varphi_2X_{t-2}+\\cdots+\\varphi_pX_{t-p}\\\\\n&=&\\varphi_1X_{t-1}+\\varphi_2X_{t-2}+\\cdots+\\varphi_pX_{t-p}\\\\\n& &+0\\times X_{t-p-1}+\\cdots+ 0\\times X_{t-k}\n\\end{eqnarray*}\\] et donc \\(r_X(k)=0\\).\n\n\n\n\n\n\n\n\n\nRemarques\n\n\n\n\nLa propriété \\(r_X(p)=\\varphi_p\\) n’est vraie que dans le cas d’une représentation canonique.\nOn peut montrer que la propriété d’autocorrélations partielles nulles à partir d’un certain rang \\(p+1\\) est caractéristique d’un AR(\\(p\\)).\n\n\n\n\nExample 5.3  Pour illustrer la propriété que les autocorrélations partielles d’un processus \\(AR(p)\\) sont nulles à partir du rang p+1, les autocorrélations partielles empiriques sont tracées en Figure 5.3 pour le processus \\(X_t = 0.7 X_{t-1}-0.1 X_{t-2}+\\varepsilon_t\\) (à gauche - AR(2)) et le processus \\(X_t = 0.7 X_{t-1}-0.1 X_{t-2}+0.2 X_{t-3}+\\varepsilon_t\\) (à droite - AR(3)).\n\n\n\n\n\n\n\n(a) le processus \\(X_t = 0.7 X_{t-1}-0.1 X_{t-2}+\\varepsilon_t\\)\n\n\n\n\n\n\n\n(b) le processus \\(X_t = 0.7 X_{t-1}-0.1 X_{t-2}+0.2 X_{t-3}+\\varepsilon_t\\)\n\n\n\n\nFigure 5.3: Autocorrélations partielles empiriques pour …"
  },
  {
    "objectID": "chap4.html#sec-chap4MA",
    "href": "chap4.html#sec-chap4MA",
    "title": "5  Les modèles ARMA",
    "section": "5.3 Processus MA",
    "text": "5.3 Processus MA\n\n5.3.1 Définition\n\nDefinition 5.3 On dit qu’un processus stationnaire \\((X_t)_{t\\in \\Z}\\) admet une représentation en moyenne mobile d’ordre \\(q\\) (noté MA(q)) s’il vérifie l’équation \\[\nX_t=\\varepsilon_t+\\theta_1\\varepsilon_{t-1}+\\theta_2\\varepsilon_{t-2}+\\cdots+\\theta_q\\varepsilon_{t-q}\n\\] où \\((\\varepsilon_t)_{t\\in \\Z}\\sim\\text{WN}(0,\\sigma^2)\\) et \\((\\theta_1,\\ldots,\\theta_q)\\in\\R^q\\), \\(\\theta_q\\neq0\\).\nRéécriture :  \\(X_t=\\Theta(B)\\varepsilon\\_t,\\ \\forall t\\in\\Z\\) avec le polynôme en \\(B\\) \\(\\Theta(B)=I+\\theta_1B+\\theta_2B^2+\\cdots+\\theta_qB^q.\\)\nCette représentation est dite canonique si les racines du polynôme \\(\\Theta(z)\\) sont toutes à l’extérieur du disque unité.\n\n\n\n\n\n\n\nRemarque\n\n\n\nContrairement à un AR, un MA est entièrement spécifié. A bruit blanc et paramètres \\(\\theta_1,\\ldots,\\theta_q\\) fixés, il ne correspond qu’un seul processus MA(\\(q\\)) qui est le filtrage linéaire de \\((\\varepsilon_t)_{t\\in \\Z}\\) par le filtre \\[\n\\Theta(B)=I+\\theta_1B+\\theta_2B^2+\\cdots+\\theta_qB^q.\n\\]\n\n\n\n\n5.3.2 Propriétés\n\nProposition 5.11 Soit \\((\\varepsilon_t)_{t\\in \\Z}\\) un bruit blanc et \\(\\Theta(B)\\) un polynôme en \\(B\\) de degré \\(q\\) exactement dont toutes les racines (non néc. distinctes) sont de module différent de 1.\nSoit \\((X_t)_{t\\in \\Z}\\) un processus \\(\\text{MA}(q)\\) vérifiant \\(X_t=\\Theta(B)\\varepsilon_t, \\forall t\\in \\Z.\\)\n\nLe processus \\((X_t)_{t\\in \\Z}\\) satisfait plusieurs représentations MA(q) différentes : soit \\(\\tilde \\Theta(B)\\) un nouveau polynôme en \\(B\\) ayant pour racines celles de \\(\\Theta(B)\\) ou leurs inverses, le processus \\((X_t)_{t\\in \\Z}\\) admet aussi la représentation : \\[\nX_t=\\tilde \\Theta(B)\\eta_t,\\ \\forall t\\in \\Z, \\textrm{ où }(\\eta_t)_{t\\in \\Z} \\textrm{un bruit blanc}.\n\\]\nUne seule représentation est canonique. Elle est obtenue en prenant le polynome avec toutes les racines à l’extérieur du disque unité.\nUn processus \\(\\text{MA}\\) est stationnaire quelque soit sa représentation car filtrage linéaire d’un bruit blanc.\nUn processus \\(\\text{MA}\\) est toujours centré.\n\n\n\nProposition 5.12 Soit \\((X_t)_{t\\in \\Z}\\) un processus MA(q) de représentation canonique \\[\nX_t=\\Theta(B)\\varepsilon_t,\\ \\forall t\\in\\Z\n\\textrm{ avec } (\\varepsilon_t)_{t\\in \\Z}\\sim\\text{WN}(0,\\sigma^2)\n\\]\nAlors le processus \\((X_t)_{t\\in \\Z}\\) possède une représentation AR(\\(\\infty\\)) : \\[\n\\varepsilon_t=\\sum_{i=0}^{+\\infty}\\pi_iX_{t-i} \\textrm{ avec }\\pi_0=1\n\\]\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nPar hypothèse la représentation est canonique donc les racines du polynôme \\(\\Theta(z)\\) sont toutes situées à l’extérieur du disque unité. Ainsi l’opérateur \\(\\Theta(B)\\) est inversible d’inverse \\[\n\\Theta^{-1}(B)=\\sum_{i=0}^{+\\infty}\\pi_iB^i=I+\\sum_{i=1}^{+\\infty}\\pi_iB^i,\n\\] la dernière égalité étant obtenue par identification des termes constants. On a donc bien \\[\n\\varepsilon_t=X_t+\\sum_{i=1}^{+\\infty}\\pi_iX_{t-i}.\n\\]\n\n\n\n\nProposition 5.13 Soit \\((X_t)_{t\\in \\Z}\\) un processus MA(q) de représentation canonique \\[\nX_t=\\Theta(B)\\varepsilon_t,\\ \\forall t\\in\\Z\n\\textrm{ avec } (\\varepsilon_t)_{t\\in \\Z}\\sim\\text{WN}(0,\\sigma^2)\n\\] Alors le processus des innovations correspond au bruit blanc \\((\\varepsilon_t)_{t\\in \\Z}\\) de sa représentation canonique.\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nA partir de la Proposition 5.12, on a que \\[\n\\varepsilon_t=X_t+\\sum_{i=1}^{+\\infty}\\pi_iX_{t-i}.\n\\]\nLes écritures MA(\\(q\\)) et AR(\\(\\infty\\)) du processus \\((X_t)_{t\\in \\Z}\\) montrent que \\(\\H_{-\\infty}^t(X)= \\H_{-\\infty}^t(\\varepsilon)\\). On peut ainsi écrire \\[\\begin{eqnarray*}\n\\EL(X_t|\\H_{-\\infty}^{t-1}(X))&=&\\EL(X_t|\\H_{-\\infty}^{t-1}(\\varepsilon))\\\\\n&=&\\EL(\n\\varepsilon_t+\\theta_1\\varepsilon_{t-1}+\\theta_2\\varepsilon_{t-2}+\\cdots+\\theta_q\\varepsilon_{t-q}|\\H_{-\\infty}^{t-1}(\\varepsilon))\\\\\n&=&\\theta_1\\varepsilon_{t-1}+\\theta_2\\varepsilon_{t-2}+\\cdots+\\theta_q\\varepsilon_{t-q},\n\\end{eqnarray*}\\] par orthoganilité du bruit blanc par rapport à son passé. On en déduit pour l’innovation \\[\nX_t-\\EL(X_t|\\H_{-\\infty}^{t-1}(X))=X_t-\\left(\\theta_1\\varepsilon_{t-1}+\\theta_2\\varepsilon_{t-2}+\\cdots+\\theta_q\\varepsilon_{t-q}\\right)=\\varepsilon_t,\n\\] ce qui achève la démonstration.\n\n\n\n\n\n5.3.3 Liaisons temporelles\nOn considère un processus \\(MA(q)\\) \\[X_t=\\Theta(B)\\varepsilon_t,\\ \\forall t\\in\\Z\\] avec \\(\\Theta(B)=I+\\theta_1B+\\theta_2B^2+\\cdots+\\theta_qB^q\\) et \\((\\varepsilon_t)_{t\\in\\Z}\\sim\\text{WN}(0,\\sigma^2)\\).\n\nVariance du processus \\((X_t)_{t\\in\\Z}\\) : \nPar la propriété d’orthogonalité d’un bruit blanc, on a\n\n\\[\\begin{eqnarray*}\n\\V(X_t) &=&\\V\\left( \\varepsilon_t +\n\\sum_{j=1}^q\\theta_j\\varepsilon_{t-j}\\right)\\\\\n&=&\\sigma^2\\left(1+\\sum_{j=1}^q\\theta_j^2\\right).\n\\end{eqnarray*}\\]\n\nFonction d’autocovariance de \\((X_t)_{t\\in\\Z}\\) :\n\n\\[\\begin{eqnarray*}\n\\gamma_X(h)&=&\\C(X_t,X_{t+h})\\\\\n&=&\\C\\left(\\varepsilon_t +\n\\sum_{j=1}^q\\theta_j\\varepsilon_{t-j},\\varepsilon_{t+h} +\n\\sum_{i=1}^q\\theta_i\\varepsilon_{t+h-i}\\right).\n\\end{eqnarray*}\\]\nA partir de cette expression, on peut immédiatement remarquer que \\(\\gamma_X(h)=0\\) pour \\(h&gt;q\\).\nPour \\(h=1\\), \\(\\C(X_t,X_{t+1}) = \\E\\left[(\\varepsilon_t +  \\sum_{j=1}^q\\theta_j\\varepsilon_{t-j})(\\varepsilon_{t+1} +  \\sum_{i=1}^q\\theta_i\\varepsilon_{t+1-i})\\right]  =\\sigma^2(\\theta_1+\\theta_1\\theta_2+\\cdots+\\theta_{q-1}\\theta_q)\\).\nPour \\(h=2\\), \\(\\C(X_t,X_{t+2}) =\\E\\left[(\\varepsilon_t +  \\sum_{j=1}^q\\theta_j\\varepsilon_{t-j})(\\varepsilon_{t+2} +  \\sum_{j=1}^q\\theta_j\\varepsilon_{t+2-j})\\right] =\\sigma^2(\\theta_2+\\theta_1\\theta_3+\\cdots+\\theta_{q-2}\\theta_q)\\).\nOn en déduit pour tout \\(h\\) dans \\(\\N^*\\) : \\[\\begin{eqnarray*}\n\\gamma_X(h)&=&\\sigma^2(\\theta_h+\\theta_1\\theta_{1+h}+\\cdots+\\theta_{q-h}\\theta_q)\n=\\sigma^2\\left(\\theta_h+\\sum_{i=1}^{q-h}\\theta_i\\theta_{i+h}\\right).\n\\end{eqnarray*}\\]\n\nFonction d’autocorrélation de \\((X_t)_{t\\in\\Z}\\) :\n\n\nProposition 5.14 (Autocorrélations simples d’un MA(q))  \n\nLes autocorrélations simples d’un processus MA(q) sont nulles à partir du rang \\(q+1\\) : \\[\\rho_X(q)\\neq 0 \\textrm{ et }\\rho_X(h)=0,\\ \\forall h&gt;q.\\]\nLa propriété d’autocorrélations simples nulles à partir d’un certain rang \\(q+1\\) est caractéristique d’un MA(\\(q\\))\n\n\n\nOn considère le processus MA(2) défini par \\(X_t = \\varepsilon_t - 0.7 \\varepsilon_{t-1}+0.1 \\varepsilon_{t-2}\\). La Figure 5.4 représente les autocorrélations empiriques de ce processus. On a bien des autocorrélations proche de 0 à partir de \\(h&gt;2\\).\n\n\n\n\n\nFigure 5.4: ACF empirique du processus MA(2) \\(X_t = \\varepsilon_t - 0.7 \\varepsilon_{t-1}+0.1 \\varepsilon_{t-2}\\)\n\n\n\n\nLa Figure 5.5 représente les autocorrélations empiriques du processus défini par \\(X_t = \\varepsilon_t - 0.7 \\varepsilon_{t-1}+0.1 \\varepsilon_{t-2} -0.2 \\varepsilon_{t-3}\\). Les autocorrélations empiriques sont bien proche de 0 à partir de \\(h&gt;3\\).\n\n\n\n\n\nFigure 5.5: ACF empirique du processus MA(3) \\(X_t = \\varepsilon_t - 0.7 \\varepsilon_{t-1}+0.1 \\varepsilon_{t-2} -0.2 \\varepsilon_{t-3}\\)\n\n\n\n\n\n\n\nProposition 5.15 (Autocorrélations partielles d’un MA(q))  Les autocorrélations partielles d’un processus MA(q) sont solutions d’une équation linéaire récurrente d’ordre \\(q\\). Elles décroissent, de manière exponentielle ou sinusoïdale amortie, vers 0.\n\n\nExample 5.4  On reprend le processus MA(2) défini par \\(X_t = \\varepsilon_t - 0.7 \\varepsilon_{t-1}+0.1 \\varepsilon_{t-2}\\). Les racines du polynome \\(\\Theta(z)\\) associé sont 2 et 5. La Figure 5.6 représente les autocorrélations partielles empiriques.\n\n\n\n\n\nFigure 5.6: pACF empirique du processus MA(2) \\(X_t = \\varepsilon_t - 0.7 \\varepsilon_{t-1}+0.1 \\varepsilon_{t-2}\\)\n\n\n\n\nLa Figure 5.7 représente les autocorrélations partielles empirique du processus MA(2) défini par MA(2) \\(X_t = \\varepsilon_t + 0.4 \\varepsilon_{t-1} - 0.45 \\varepsilon_{t-2}\\), où les racines du polynome \\(\\Theta(z)\\) associé sont \\(2\\pm i 2\\sqrt{3}/3\\).\n\n\n\n\n\nFigure 5.7: pACF empirique du processus MA(2) \\(X_t = \\varepsilon_t + 0.4 \\varepsilon_{t-1} - 0.45 \\varepsilon_{t-2}\\)"
  },
  {
    "objectID": "chap4.html#processus-arma",
    "href": "chap4.html#processus-arma",
    "title": "5  Les modèles ARMA",
    "section": "5.4 Processus ARMA",
    "text": "5.4 Processus ARMA\nNous allons maintenant étudier des processus stationnaires avec une partie AR et une partie MA. On les appelle processus ARMA pour AutoRegressive Moving Average. Ils sont très importants en pratique car on peut montrer que tout processus stationnaire peut être approché par un processus ARMA.\n\n5.4.1 Définition\n\nDefinition 5.4 On dit qu’un processus stationnaire \\((X_t)_{t\\in \\Z}\\) admet une représentation ARMA\\((p,q)\\) s’il vérifie l’équation \\[\nX_t=\\textcolor{red}{\\underset{j=1}{\\stackrel{p}{\\sum}} \\varphi_j\nX_{t-j}} + \\textcolor{blue}{\\varepsilon_t +\\underset{i=1}{\\stackrel{q}{\\sum}}\n\\theta_i\\varepsilon_{t-i}}\n\\] avec\n\n\\((\\varepsilon_t)_{t\\in \\Z}\\sim\\text{WN}(0,\\sigma^2)\\)\n\\((\\varphi_1,\\ldots, \\varphi_p)\\in\\R^p,\\ \\varphi_p\\neq0\\)\n\\((\\theta_1,\\ldots,\\theta_q)\\in\\R^{q},\\ \\theta_q\\neq 0\\)\n\nRéécriture :  En utilisant les polynômes en \\(B\\) \\[\\begin{eqnarray*} \\Phi(B)&=&I-\\varphi_1B-\\varphi_2B^2-\\cdots-\\varphi_pB^p\\\\\n\\text{ et }\\Theta(B)&=&I+\\theta_1B+\\theta_2B^2+\\cdots+\\theta_qB^q,\n\\end{eqnarray*}\\] un processus ARMA(p,q) vérifie l’équation : \\[\n\\textcolor{red}{\\Phi(B)X_t}=\\textcolor{blue}{\\Theta(B)\\varepsilon_t},\\ \\forall t\\in\\Z.\n\\]\n\n\nDefinition 5.5 La représentation d’un processus ARMA\\((p,q)\\) \\(\\Phi(B)X_t=\\Theta(B)\\varepsilon_t\\) est dite\n\nminimale si les polynômes \\(\\Phi(z)\\) et \\(\\Theta(z)\\) n’ont pas de racine commune\ncausale si le polynôme \\(\\Phi(z)\\) a toutes ses racines à l’extérieur du disque unité\ninversible si le polynôme \\(\\Theta(z)\\) a toutes ses racines à l’extérieur du disque unité\ncanonique si elle est causale et inversible.\n\n\n\n\n\n\n\n\nRemarques\n\n\n\nSi la représentation n’est pas minimale (\\(\\Phi(z)\\) et \\(\\Theta(z)\\) ont une ou des racine(s) commune(s)), alors\n\nSoit aucune de ces racines communes n’est sur le cercle unité. Dans ce cas, le processus \\((X_t)_{t\\in \\Z}\\) admet aussi la représentation \\[\n\\tilde\\Phi(B)X_t=\\tilde \\Theta(B)\\varepsilon_t,\\ \\forall t\\in\\Z\n\\] où les nouveaux polynômes sont obtenus à partir des précédents en enlevant les racines communes.\nSi au moins une des racines communes est sur le cercle unité, alors il peut y avoir plus d’un unique processus stationnaire vérifiant l’équation. Dans la suite, on ne considèrera que des représentations minimales.\n\n\n\n\n\n5.4.2 Ecritures MA(\\(\\infty\\)) et AR(\\(\\infty\\))\n\nProposition 5.16 (Ecriture MA(\\(\\infty\\))) Soit \\((X_t)_{t\\in \\Z}\\) un processus ARMA(p,q) de représentation minimale et causale. Il admet alors la représentation MA(\\(\\infty\\)) \\[\nX_t=\\Phi^{-1}(B)\\Theta(B)\\varepsilon_t=\\varepsilon_t+\\sum_{i=1}^{+\\infty}\\psi_i\\varepsilon_{t-i},\n\\] où les coefficients \\((\\psi_i)_{i\\in N}\\) forment une famille absolument sommable et vérifient l’équation de récurrence linéaire : \\[\n\\psi_i-\\sum_{j=1}^p\\varphi_j\\psi_{i-j}=\\theta_i,\\ \\forall i\\in \\N,\n\\] avec \\(\\psi_i=0\\) pour \\(i&lt;0\\), \\(\\psi_0=1\\), \\(\\theta_0=1\\) et \\(\\theta_i=0\\) pour \\(i&gt;q\\).\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nLa représentation étant causale, les racines du polynôme \\(\\Phi(z)\\) sont toutes à l’extérieur du disque unité. L’inverse de \\(\\Phi(z)\\) s’écrit alors comme une série de puissances positives de \\(B\\). Multipliée par \\(\\Theta(B)\\), on garde une série en puissances positives de \\(B\\) et on a donc l’écriture : \\[\nX_t=\\Phi^{-1}(B)\\Theta(B)\\varepsilon_t=\\varepsilon_t+\\sum_{i=1}^{+\\infty}\\psi_i\\varepsilon_{t-i},\n\\text{ pour tout }t\\in \\Z.\n\\] Grâce à cette écriture MA(\\(\\infty\\)) de \\((X_t)_{t\\in \\Z}\\), on peut réécrire l’équation \\[\nX_t-\\varphi_1X_{t-1}-\\varphi_2X_{t-2}-\\cdots-\\varphi_pX_{t-p}=\\varepsilon_t\n+\\theta_1\\varepsilon_{t-1}+\\theta_2\\varepsilon_{t-2}+\\cdots+\\theta_q\\varepsilon_{t-q}\n\\] sous la forme \\[\\begin{eqnarray*}\n&\\left(\n\\varepsilon_t+\\sum_{i=1}^{+\\infty}\\psi_i\\varepsilon_{t-i}\\right)-\\varphi_1\\left(\\varepsilon_{t-1}+\\sum_{i=1}^{+\\infty}\\psi_i\\varepsilon_{t-1-i}\\right)-\\cdots\n-\\varphi_p\n\\left(\\varepsilon_{t-p}+\\sum_{i=1}^{+\\infty}\\psi_i\\varepsilon_{t-p-i}\\right)\\\\\n&=\\varepsilon_t\n+\\theta_1\\varepsilon_{t-1}+\\theta_2\\varepsilon_{t-2}+\\cdots+\\theta_q\\varepsilon_{t-q}.\n\\end{eqnarray*}\\] En identifiant les coefficients devant chaque \\(\\varepsilon_t\\), on obtient bien la formule annoncée.\n\n\n\n\nProposition 5.17 (Ecriture AR(\\(\\infty\\))) Soit \\((X_t)_{t\\in \\Z}\\) un processus ARMA(p,q) de représentation minimale et inversible. Il admet alors la représentation AR(\\(\\infty\\)) \\[\n\\varepsilon_t=\\Theta^{-1}(B)\\Phi(B)X_t=X_t+\\sum_{i=1}^{+\\infty}\\pi_iX_{t-i},\n\\] où les coefficients \\((\\pi_i)_{i\\in N}\\) forment une famille absolument sommable et vérifient l’équation de récurrence linéaire : \\[\n\\pi_i+\\sum_{j=1}^q\\theta_j\\pi_{i-j}=-\\varphi_i,\\ \\forall i\\in \\N,\n\\] avec \\(\\pi_i=0\\) pour \\(i&lt;0\\), \\(\\varphi_0=-1\\) et \\(\\varphi_i=0\\) pour \\(i&gt;p\\).\n\n\n\n\n\n\n\nPreuve\n\n\n\n\n\nLa représentation étant inversible, les racines du polynôme \\(\\Theta(z)\\) sont toutes à l’extérieur du disque unité. L’inverse de \\(\\Theta(z)\\) s’écrit alors comme une série de puissances positives de \\(B\\). Multipliée par \\(\\Phi(B)\\), on garde une série en puissances positives de \\(B\\) et on a donc l’écriture : \\[\n\\varepsilon_t=\\Theta^{-1}(B)\\Phi(B)X_t=X_t+\\sum_{i=1}^{+\\infty}\\pi_iX_{t-i},\n\\text{ pour tout }t\\in \\Z.\n\\] Grâce à cette écriture AR(\\(\\infty\\)), on peut réécrire l’équation \\[\nX_t-\\varphi_1X_{t-1}-\\varphi_2X_{t-2}-\\cdots-\\varphi_pX_{t-p}=\\varepsilon_t\n+\\theta_1\\varepsilon_{t-1}+\\theta_2\\varepsilon_{t-2}+\\cdots+\\theta_q\\varepsilon_{t-q}\n\\] sous la forme \\[\\begin{eqnarray*}\n& & X_t-\\varphi_1 X_{t-1}-\\varphi_2X_{t-2}-\\cdots-\\varphi_pX_{t-p} \\\\\n&=& \\left(X_t+\\sum_{i=1}^{+\\infty}\\pi_iX_{t-i}\\right)\n+\\theta_1 \\left(X_{t-1}+\\sum_{i=1}^{+\\infty}\\pi_iX_{t-i-1}\\right)+\\cdots+\\theta_q \\left(X_{t-q}+\\sum_{i=1}^{+\\infty}\\pi_iX_{t-i-q}\\right)\n\\end{eqnarray*}\\] En identifiant les coefficients devant chaque \\(X_t\\), on obtient bien la formule annoncée.\n\n\n\n\n\n5.4.3 Liaisons temporelles\nOn considère un processus \\((X_t)_{t\\in\\Z}\\) qui est un ARMA(\\(p,q\\)) d’équation \\[X_{t}-\\underset{j=1}{\\stackrel{p}{\\sum}}\\varphi_jX_{t-j}=\\varepsilon_{t} +\\underset{i=1}{\\stackrel{q}{\\sum}}\\theta_i\\varepsilon_{t-i}.\\]\n\nFonction d’autocorrélation à partir de l’écriture ARMA(\\(p,q\\)) :  On considère \\(X_{t+h}-\\underset{j=1}{\\stackrel{p}{\\sum}}\\varphi_jX_{t+h-j}=\\varepsilon_{t+h} +\\underset{i=1}{\\stackrel{q}{\\sum}}\\theta_i\\varepsilon_{t+h-i}\\) et on prend la covariance avec \\(X_t\\) ce qui donne \\[\\begin{eqnarray*}\n  \\gamma_X(h)-\\underset{j=1}{\\stackrel{p}{\\sum}}\\varphi_j\\gamma_X(h-j) &=&\n  \\C\\left(\\varepsilon_{t+h}\n  +\\underset{i=1}{\\stackrel{q}{\\sum}}\\theta_i\\varepsilon_{t+h-i},X_t\\right)\\\\ &=&\n  \\C\\left(\\varepsilon_{t+h}\n  +\\underset{i=1}{\\stackrel{q}{\\sum}}\\theta_i\\varepsilon_{t+h-i},\\varepsilon_t+\\sum_{i=1}^{+\\infty}\\psi_i\\varepsilon_{t-i}\\right).\n  \\end{eqnarray*}\\]\n\nOn a ausi que pour \\(h&gt;q\\), \\(\\gamma_X(h)-\\underset{j=1}{\\stackrel{p}{\\sum}}\\varphi_j\\gamma_X(h-j)=0.\\) Pour \\(0\\leq h\\leq q\\), on obtient \\[\n\\gamma_X(h)-\\underset{j=1}{\\stackrel{p}{\\sum}}\\varphi_j\\gamma_X(h-j)=\\sigma^2\\left(\\sum_{i=0}^{q-h}\\theta_{h+i}\\psi_i{}\\right)\\textrm{ avec } \\psi_0=1\n\\]\n\nFonction d’autocorrélation avec l’écriture MA(\\(\\infty\\)) :\n\nEn utilisant l’écriture MA(\\(\\infty\\)) du processus \\((X_t)_{t\\in \\Z}\\) on obtient cette fois-ci \\[\n\\gamma_X(h)=\\C\\left(\n\\varepsilon_t+\\sum_{i=1}^{+\\infty}\\psi_i\\varepsilon_{t-i},\\varepsilon_{t+h}+\\sum_{i=1}^{+\\infty}\\psi_i\\varepsilon_{t+h-i}\\right)=\\sigma^2\\sum_{i=0}^{+\\infty}\n\\psi_i\\psi_{i+h}.\n\\]\n\n\n\n\n\n\nRemarques\n\n\n\n\nLa première expression obtenue sous forme de relation de récurrence, permet de montrer que les autocorrélations simples décroissent de manière exponentielle ou sinusoïdale amortie vers 0 avec \\(h\\). On peut montrer le même genre de résultat pour les autocorrélations partielles.\nOn constate, qu’à la différence des cas particuliers des processus AR(\\(p\\)) ou MA(\\(q\\)), il n’existe pas de caractérisation aisée pour les modèles ARMA(\\(p,q\\)). Les autocorrélations simples ou partielles ne s’annulent pas à partir d’un certain rang."
  },
  {
    "objectID": "chap4.html#vers-les-processus-arima-sarima",
    "href": "chap4.html#vers-les-processus-arima-sarima",
    "title": "5  Les modèles ARMA",
    "section": "5.5 Vers les processus ARIMA/ SARIMA",
    "text": "5.5 Vers les processus ARIMA/ SARIMA\n\n5.5.1 ARIMA\nDans la pratique, les processus sont rarement stationnaires. Les modèles ARIMA (AutoRegressive Integrated Moving Average) sont une extension des processus ARMA aux processus non stationnaires. Ils sont basés sur l’idée générale suivante essentiellement conçue pour les processus non stationnaires à tendance polynomiale : on peut différencier suffisamment le processus initial afin d’obtenir un processus sans tendance et appliquer un modèle ARMA sur le processus différencié.\n\nDefinition 5.6 On dit qu’un processus \\((X_t)_{t\\in \\Z}\\) admet une représentation [ARIMA\\((p,d,q)\\)]]{style=“color:blue;”} (\\(p,d,q\\in\\mathbb N\\)) s’il vérifie l’équation : \\[\n\\Phi(B)(I-B)^dX_t= c + \\Theta(B)\\varepsilon_t,\\ \\forall t\\in\\Z\n\\] où \\[\\begin{eqnarray*}\n& & c\\in\\mathbb{R}\\\\\n& & \\Phi(B)=I-\\varphi_1B-\\varphi_2B^2-\\cdots-\\varphi_pB^p,\\ (\\varphi_1,\\ldots, \\varphi_p)\\in\\R^p,\\ \\varphi_p\\neq0\\\\\n& & \\Theta(B)= I+\\theta_1B+\\theta_2B^2+\\cdots+\\theta_qB^q,\\ (\\theta_1,\\ldots,\\theta_q)\\in\\R^q,\\ \\ \\ \\theta_q\\neq 0\\\\\n& & (\\varepsilon_t)_{t\\in \\Z}\\sim\\text{WN}(0,\\sigma^2)\n\\end{eqnarray*}\\]\n\n\n\n\n\n\n\nRemarques\n\n\n\n\nOn peut montrer que, dans le cas d’un processus ARIMA\\((p,d,q)\\), le processus \\((I-B)^dX_t\\) est asympotiquement (au sens quand \\(t\\to +\\infty\\)) un processus ARMA(p,q).\nOn peut aussi établir des représentations AR(\\(\\infty\\)) et MA(\\(\\infty\\)) pour les processus ARIMA.\n\n\n\nPar rapport aux caractéristiques évoquées précédemment (voir ?sec-liaisonsAR et ?sec-liaisonsMA) sur les autocorrélations et autocorrélations partielles pour les processus AR et MA (voir ?sec-liaisonsAR et ?sec-liaisonsMA), on a le résultat suivant.\n\nProposition 5.18  \n\nSi la série temporelle suit un ARIMA(p,d,0) alors l’ACF et le PACF sur les données différenciées vérifient\n\nl’ACF décroit exponentiellement ou sinusoidal\nUn pic significatif au lag \\(p\\) sur le PACF et pas après le lag \\(p\\)\n\nSi la série temporelle suit un ARIMA(0, d, q) alors l’ ACF et le PACF sur les données différenciées vérifient :\n\nle PACF est décroissant exponentiellement ou sinusoidal\nUn pic significatif au lag \\(q\\) pour l’ACF et pas après le lag \\(q\\)\n\n\n\n\n\nExample 5.5 Pour illustrer la Proposition 5.18, la Figure 5.8 montre les autocorrélations et autocorrélations partielles empiriques pour les mesures différenciées une fois du processus \\[\n(I-0.1 B -0.2 B^2 - 0.6 B^3)(I-B) X_t = \\varepsilon_t,\\ (\\varepsilon_t)_{t\\in\\Z}\\sim \\text{WN}(0,1)\n\\]\n\n\n\n\n\n\n\n(a) Autocorrélations empiriques\n\n\n\n\n\n\n\n(b) Autocorrélations partielles empiriques\n\n\n\n\nFigure 5.8: Résultats pour le processus ARIMA(3,1,0) \\((I-0.1 B -0.2 B^2 - 0.6 B^3)(I-B) X_t = \\varepsilon_t,\\ (\\varepsilon_t)_{t\\in\\Z}\\sim \\text{WN}(0,1)\\)\n\n\nPour illustrer la Proposition 5.18, la Figure 5.9 montre les autocorrélations et autocorrélations partielles empiriques pour les mesures différenciées une fois du processus \\[\n(I-B) X_t = (I-0.6 B -0.6 B^2 - 0.2 B^3)\\varepsilon_t,\\ (\\varepsilon_t)_{t\\in\\Z}\\sim \\text{WN}(0,1)\n\\]\n\n\n\n\n\n\n\n(a) Autocorrélations empiriques\n\n\n\n\n\n\n\n(b) Autocorrélations partielles empiriques\n\n\n\n\nFigure 5.9: Résultats pour le processus ARIMA(0,1,3) \\((I-B) X_t = (I-0.6 B -0.6 B^2 - 0.2 B^3)\\varepsilon_t,\\ (\\varepsilon_t)_{t\\in\\Z}\\sim \\text{WN}(0,1)\\)\n\n\n\n\n\n5.5.2 Sélection de modèle\nQuand on étudie une série temporelle, on peut être amené à hésiter entre plusieurs modèles pour la modéliser au mieux. Supposons que l’on considère une collection de modèles ARIMA \\(\\mathcal M = \\left\\{\\texttt{mod}_1,\\ldots,\\texttt{mod}_K\\right\\}\\). On est dans le cadre classique d’un problème de sélection de modèles. On traite donc ce problème par les étapes suivantes :\n\nAjustement de chaque modèle \\(\\texttt{mod}_k\\): Pour chaque modèle fixé, on estime les paramètres (par maximum de vraisemblance le plus souvent)\nOn minimise un critère pénalisé pour choisir le meilleur modèle. Par exemple,\n\nle critère AIC : \\(\\mbox{AIC}(\\texttt{mod}_k) = -2 \\mbox{ log. vrais.} + 2 (p+q+1+\\mathbb{1}_{c\\neq 0})\\)\nle critère AIC corrigé : \\(\\mbox{AICc}(\\texttt{mod}_k) = \\mbox{AIC}(\\texttt{mod}_k) + \\frac{2 (p+q+1+\\mathbb{1}_{c\\neq 0})(p+q+2+\\mathbb{1}_{c\\neq 0})}{n-p-q-2-\\mathbb{1}_{c\\neq 0}}\\)\nle critère BIC : \\(\\mbox{BIC}(\\texttt{mod}_k) = -2 \\mbox{ log. vrais.} + (p+q+1+\\mathbb{1}_{c\\neq 0}) \\log(n)\\)\n\n\n\nExample 5.6 Pour illustrer la sélection de modèle, on a ici simulées une série temporelle selon le modèle ARIMA(1,1,2) suivant\n\\[\n(I-0.8 B)(I-B) X_t = (I-0.3B+0.6 B^2)\\varepsilon_t,\\ \\ (\\varepsilon_t)_t\\sim \\text{WN}(0,1.5)\n\\]\nEn Figure 5.10, on trace la série temporelle étudiée \\((X_t)_{t\\in\\Z}\\) ainsi que les autocorrélations et autocorrélations partielles empiriques de \\(((I-B)X_t)_{t\\in\\Z}\\).\n\n\n\n\n\n\n\n(a) Série temporelle observée sur les 200 premiers temps\n\n\n\n\n\n\n\n(b) Autocorrélations empiriques\n\n\n\n\n\n\n\n\n\n(c) Autocorrélations partielles empiriques\n\n\n\n\nFigure 5.10: Résultats pour une série temporelle simulées selon un modèle ARIMA(1,1,2).\n\n\nOn considère la collection de modèles suivante \\(\\mathcal M = \\{ARIMA(p,1,q),\\ p\\in\\{1,2,3,4\\},\\ q\\in\\{0,1,2\\}\\}\\). On calcule les différents critères évoqués précédemment. Au vu des résultats ci-dessous, les trois critères sélectionnent un modèle ARIMA(1,1,2).\n\n\n            mod    AIC   AICc    BIC\n1  ARIMA(1,1,0) 677.50 677.56 684.09\n2  ARIMA(2,1,0) 662.93 663.05 672.83\n3  ARIMA(3,1,0) 654.65 654.85 667.84\n4  ARIMA(4,1,0) 626.42 626.73 642.91\n5  ARIMA(1,1,1) 670.44 670.57 680.34\n6  ARIMA(2,1,1) 662.15 662.36 675.34\n7  ARIMA(3,1,1) 641.34 641.64 657.83\n8  ARIMA(4,1,1) 627.87 628.30 647.66\n9  ARIMA(1,1,2) 628.95 629.16 642.15\n10 ARIMA(2,1,2) 630.34 630.65 646.83\n11 ARIMA(3,1,2) 632.20 632.63 651.99\n12 ARIMA(4,1,2) 628.39 628.97 651.48\n\n\n\n\n\n5.5.3 SARIMA\nLes modèles SARIMA, pour Seasonal ARIMA, sont une extension des modèles ARIMA à des séries temporelles présentant une saisonnalité.\n\nDefinition 5.7 On dit qu’un processus \\((X_t)_{t\\in \\Z}\\) admet une représentation SARIMA\\((p,d,q)(P,D,Q)[s]\\) s’il vérifie l’équation : \\[\n\\textcolor{red}{(I-B^s)^D\\Phi_P(B^s)}\\textcolor{blue}{\\Phi_p(B)(I-B)^d}\\ X_t=c + \\textcolor{blue}{\\Theta_q(B)}\\textcolor{red}{\\Theta_Q(B^s)}\\varepsilon_t,\\ \\forall t\\in\\Z\n\\] où \\[\\begin{eqnarray*}\n& & c\\in\\mathbb{R}\\\\\n& & \\Phi_p(B)=I-\\varphi_1B-\\varphi_2B^2-\\cdots-\\varphi_pB^p,\\ (\\varphi_1,\\ldots, \\varphi_p)\\in\\R^p,\\ \\varphi_p\\neq0\\\\\n& & \\Phi_P(B^s)=I-\\phi_1B^s-\\phi_2(B^s)^2-\\cdots-\\phi_P(B^s)^P,\\ (\\phi_1,\\ldots, \\phi_P)\\in\\R^P,\\ \\phi_P\\neq 0\\\\\n& & \\Theta_q(B)= I+\\theta_1B+\\theta_2B^2+\\cdots+\\theta_qB^q,\\ (\\theta_1,\\ldots,\\theta_q)\\in\\R^q,\\ \\ \\ \\theta_q\\neq 0\\\\\n& & \\Theta_Q(B^s)= I+\\vartheta_1(B^s)+\\vartheta_2(B^s)^2+\\cdots+\\vartheta_Q(B^s)^Q,\\ (\\vartheta_1,\\ldots,\\vartheta_Q)\\in\\R^Q,\\ \\ \\ \\vartheta_Q\\neq 0\\\\\n& & (\\varepsilon_t)_{t\\in \\Z}\\sim\\text{WN}(0,\\sigma^2)\n\\end{eqnarray*}\\]\n\nEn pratique, voici comment on peut procéder pour “identifier” les différents ordres :\n\nChoix de \\(d\\), \\(D\\) et \\(s\\) :\n\nAllure de la série pour détecter la non-stationnarité et la saisonnalité \\(s\\)\nDifférencier la série successivement jusqu’à stationnarité (\\(d=1,2\\) et \\(D=0,1\\) suffisent souvent) \n\nChoix de \\(p\\), \\(q\\) (\\(P\\) et \\(Q\\) idem sur les lags \\(k\\times s\\)) : A partir de ACF et pACF sur la série différenciée\n\nCassure de l’ACF au lag \\(q\\) -&gt; MA(q)\nCassure pACF au lag \\(p\\) -&gt; AR(p)\nAucune cassure -&gt; ARMA\nDécroissance très lente ou inexistante, il faut peut être encore différencier (mais avec parcimonie)\n\n\n\nExample 5.7 Dans cet exemple, on simule une série temporelle selon un modèle SARIMA(0,1,1)(0,1,1)[12] \\[\n(I-B^{12})(I-B) X_t = (I+0.8 B)(I+0.4 B^{12})\\varepsilon_t\n\\]\nLa série est représentée en Figure 5.11.\n\n\n\n\n\nFigure 5.11: Série temporelle simulée selon un modèle SARIMA(0,1,1)(0,1,1)[12]\n\n\n\n\nLa Figure 5.12 permet de partir sur une saisonnalité par mois pour la série temporelle \\((s=12)\\).\n\n\n\n\n\n\n\n(a) Tracé de la série par mois\n\n\n\n\n\n\n\n(b) Tracé des points \\((X_t,X_{t+h})\\) pour \\(h\\in\\{1,\\ldots,12\\}\\)\n\n\n\n\nFigure 5.12: Pour l’étude de la saisonnalité.\n\n\nOn trace ensuite les autocorrélations et autocorrélations empiriques pour la série \\((X_t)\\) (voir Figure 5.13)).\n\n\n\n\n\n\n\n(a) Autocorrélations empiriques\n\n\n\n\n\n\n\n(b) Autocorrelations partielles empiriques\n\n\n\n\nFigure 5.13: Autocorrélations et autocorrélations partielles de la série \\((X_t)_{t\\in\\Z}\\).\n\n\nLa Figure 5.14 (resp. Figure 5.15) représente la série temporelle différentiée \\((I-B)X_t\\) (resp. \\((I-B^{12})X_t\\))\n\n\n\n\n\nFigure 5.14: Tracé de la série temporelle différenciée \\(((I-B)X_t)_{t\\in\\Z}\\).\n\n\n\n\n\n\n\n\n\nFigure 5.15: Tracé de la série temporelle différenciée \\(((I-B^{12})X_t)_{t\\in\\Z}\\).\n\n\n\n\nOn trace ensuite en Figure 5.16, les autocorrélations et autocorrélations partielles empiriques de la série \\[\nY_t = (I-B^{12})(I-B) X_t\n\\]\n\n\n\n\n\n\n\n(a) Autocorrélations empiriques\n\n\n\n\n\n\n\n(b) Autocorrelations partielles empiriques\n\n\n\n\nFigure 5.16: Autocorrélations et autocorrélations partielles de la série \\(Y_t = (I-B^{12})(I-B) X_t\\).\n\n\nOn décide alors d’ajuster sur la série \\((X_t)_{t\\in\\Z}\\) un modèle SARIMA(0,1,1)(0,1,1)[12]. L’estimation des paramètres est donnée ici à l’aide de la fonction arima() et on teste ensuite la blancheur des résidus avec la fonction Box.test() :\n\nres&lt;-arima(Xt,order=c(0,1,1),seasonal=list(order=c(0,1,1),period=12))\nres\n\n\nCall:\narima(x = Xt, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 12))\n\nCoefficients:\n         ma1    sma1\n      0.8236  0.3584\ns.e.  0.0220  0.0391\n\nsigma^2 estimated as 0.9851:  log likelihood = -829.86,  aic = 1665.72\n\nBox.test(res$residuals)\n\n\n    Box-Pierce test\n\ndata:  res$residuals\nX-squared = 0.67745, df = 1, p-value = 0.4105\n\n\n\nPour finir ce chapitre, on étude un jeu de données réelles concernant la pollution en oxyde d’azote à Londre.\n\nExample 5.8 (Pollution en oxyde d’azote à Londron) La Figure 5.17 représente la mesure de pollution en oxyde d’azote à Londre par heure entre le 1/01/98 et le 21/01/98. On trace les autocorrélations empiriques (voir en Figure 5.18) et les autocorrélations partielles empiriques (voir en Figure 5.19) de cette série temporelle \\((X_t)\\).\n\n\n\n\n\nFigure 5.17: Mesure de l’oxyde d’azote dans l’air à Londre par heure entre le 1/01/98 et le 21/01/98.\n\n\n\n\n\n\n\n\n\nFigure 5.18: Autocorrélations de la série \\(X_t\\)\n\n\n\n\n\n\n\n\n\nFigure 5.19: Autocorrélations partielles de la série \\(X_t\\)\n\n\n\n\nOn analyse ensuite les autocorrélations et autocorrélations partielles empiriques de la série différenciée \\(Y_t = (I-B^{24}) X_t\\) (voir Figure 5.20). On peut alors considérer des modèles SARIMA avec \\(0\\leq q\\leq 12\\), \\(0\\leq p \\leq 2\\), \\(0\\leq Q \\leq 2\\), \\(0\\leq P \\leq 1\\).\n\n\n\n\n\n\n\n(a) Autocorrélations empiriques\n\n\n\n\n\n\n\n(b) Autocorrelations partielles empiriques\n\n\n\n\nFigure 5.20: Autocorrélations et autocorrélations partielles de la série \\(Y_t = (I-B^{24})X_t\\).\n\n\nOn met donc en place une sélection de modèles avec le critère AIC à l’aide de la fonction auto.arima(). On retient un modèle SARIMA\\((0,1,1)(0,0,1)[24]\\) et on contrôle la blancheur des résidus obtenus.\n\nset.seed(1234)\nres&lt;-auto.arima(tta,ic=c(\"aic\"),\n                max.p=2,max.q=12,\n                max.P=1,max.Q=2,seasonal=TRUE)\nres\n\nSeries: tta \nARIMA(0,1,1)(0,0,1)[24] with drift \n\nCoefficients:\n         ma1    sma1    drift\n      0.0784  0.1071  -0.1602\ns.e.  0.0549  0.0463   3.0371\n\nsigma^2 = 3305:  log likelihood = -2750.34\nAIC=5508.68   AICc=5508.76   BIC=5525.56\n\nBox.test(res$residuals)\n\n\n    Box-Pierce test\n\ndata:  res$residuals\nX-squared = 0.10787, df = 1, p-value = 0.7426\n\n\nOn représente en Figure 5.21 la série temporelle ajustée par ce modèle par rapport à la série temporelle observée.\n\n\n\n\n\nFigure 5.21: Série temporelle estimée avec un modèle SARIMA(0,1,1)(0,0,1)(24) par rapport à la série temporelle observée.\n\n\n\n\n\n\n\n\n\nAragon, Yves. 2016. Séries Temporelles Avec r. EDP sciences.\n\n\nBrockwell, Peter J, and Richard A Davis. 2002. Introduction to Time Series and Forecasting. Springer.\n\n\n———. 2009. Time Series: Theory and Methods. Springer science & business media.\n\n\nDauxois, Jean-Yves. 2020. “Introduction à l’étude Des Séries Temporelles.” Polycopié cours INSA Toulouse."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aragon, Yves. 2016. Séries Temporelles Avec r. EDP\nsciences.\n\n\nBrockwell, Peter J, and Richard A Davis. 2002. Introduction to Time\nSeries and Forecasting. Springer.\n\n\n———. 2009. Time Series: Theory and Methods. Springer science\n& business media.\n\n\nDauxois, Jean-Yves. 2020. “Introduction à l’étude Des Séries\nTemporelles.” Polycopié cours INSA Toulouse."
  }
]